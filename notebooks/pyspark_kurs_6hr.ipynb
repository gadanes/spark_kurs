{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "727b74ea-8ec1-42e6-943a-764ecc1abfc1",
   "metadata": {},
   "source": [
    "# Spark Session and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a460217-eef5-475c-b3d9-240fc357778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(\"hello\",), (\"world\",)], [\"word\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0ea962-34b4-42b3-9ad0-79257ad31c35",
   "metadata": {},
   "source": [
    "# Loading env file that contain connection string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2b1d08-96fa-44da-a060-5334852530f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load variables from .env into os.environ\n",
    "load_dotenv(\"env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f3fb0-de06-4dcc-bc5b-a666803f2b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_name = os.environ.get(\"SERVERNAME\")\n",
    "database_name = os.environ.get(\"DATABASENAME\")\n",
    "username = os.environ.get(\"USERNAME\")\n",
    "password = os.environ.get(\"PASSWORD\")\n",
    "\n",
    "# Standard JDBC URL format for SQL Server\n",
    "jdbc_url = f\"jdbc:sqlserver://{server_name}:1433;databaseName={database_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40fa1fa-f568-4d74-8442-147f104bed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#table name to read\n",
    "table_name = \"dbo.AP18_CUSTOMER_MASTER\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de57f861-e8f3-4399-beca-7edc58d58dcc",
   "metadata": {},
   "source": [
    "# Read from our Azure Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a809f152-0543-4527-9b03-7bfea8848568",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    jdbcDF = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"user\", username) \\\n",
    "        .option(\"password\", password) \\\n",
    "        .load()\n",
    "\n",
    "    # Display schema and a few rows if successful\n",
    "    jdbcDF.printSchema()\n",
    "    jdbcDF.show(5)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"JDBC read failed!\")\n",
    "    print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eb86dc-4f03-433f-bdac-df466af12ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc6b479-9c66-4c24-98d1-c18fce78dd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acb259a-cd83-41f0-a856-41839afa6f6e",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cbce6c-6f5c-4104-aac7-9e4d1a90feb1",
   "metadata": {},
   "source": [
    "## ðŸ§  1. Was ist Spark? Warum PySpark?\n",
    "\n",
    "> \"Alle reden Ã¼ber Spark â€” aber was lÃ¶st es eigentlich?\"\n",
    "\n",
    "- **Apache Spark** ist eine verteilte Datenverarbeitungs-Engine.\n",
    "  - *Ãœbersetzt*: Anstatt dass ein Computer alles alleine macht (wie dein Laptop mit Python), verteilt Spark die Arbeit auf *viele* Rechner oder CPU-Kerne.\n",
    "  - *Einsatzgebiet*: Riesige Datenmengen und Machine Learning auf Skalenniveau.\n",
    "\n",
    "- **Warum nicht einfach Pandas oder reines Python?**\n",
    "  - *Kleine Daten passen in den Arbeitsspeicher â†’ Pandas reicht locker.*\n",
    "  - *Daten grÃ¶ÃŸer als RAM â†’ Pandas stirbt.*\n",
    "  - *Man braucht echte Parallelisierung â†’ Spark.*\n",
    "\n",
    "- **Warum PySpark?**\n",
    "  - Spark ist in Scala und Java geschrieben â€” aber wer will fÃ¼r normale Datenverarbeitung bitte in Java rumfrickeln?\n",
    "  - **PySpark** erlaubt dir, Spark mit Python zu steuern. Fast alle Vorteile, aber einfacher zu schreiben.\n",
    "\n",
    "**Ehrliche EinschÃ¤tzung**:  \n",
    "> Nutze Spark nur, wenn du *musst* (Daten zu groÃŸ, Performance wird kritisch).  \n",
    "> Ansonsten: **Pandas > PySpark** bei kleinen bis mittleren Daten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b26ccbf-d965-48a4-a67c-fe33ef45105d",
   "metadata": {},
   "source": [
    "## âš™ï¸ 2. Cluster Mode vs Local Mode\n",
    "\n",
    "Stell dir vor, du hast eine schwere Datenverarbeitung.\n",
    "\n",
    "| Modus | Was passiert | Wann verwenden | Wichtigster Punkt |\n",
    "|:----|:------------|:----------------|:-----------|\n",
    "| **Local Mode** | Spark lÃ¤uft auf deinem Rechner, nutzt mehrere CPU-Kerne. | Entwicklung, Tests, kleine Datenmengen. | *Trainingsmodus.* |\n",
    "| **Cluster Mode** | Spark lÃ¤uft auf mehreren Servern (Nodes). | Produktion, Big Data. | *Echte Skalierung.* |\n",
    "\n",
    "**Skizze:**\n",
    "\n",
    "```plaintext\n",
    "Local Mode:\n",
    "+-----------------+\n",
    "| Laptop          |\n",
    "| [Core1][Core2]  |\n",
    "| [Core3][Core4]  |\n",
    "+-----------------+\n",
    "\n",
    "Cluster Mode:\n",
    "+--------------------+     +-------------------+     +-------------------+\n",
    "| Worker Node 1      |     | Worker Node 2      |     | Worker Node 3      |\n",
    "| [Task1][Task2]     |     | [Task3][Task4]     |     | [Task5][Task6]     |\n",
    "+--------------------+     +-------------------+     +-------------------+\n",
    "          \\                    |                    /\n",
    "           \\                   |                   /\n",
    "                +----------------------------------+\n",
    "                |        Spark Driver Program     |\n",
    "                +----------------------------------+\n",
    "```\n",
    "\n",
    "> **Wichtig**: *Im Cluster Mode steuert ein Programm viele Maschinen.*  \n",
    "> **Local Mode simuliert nur ein Mini-Cluster auf deinem Laptop.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0562472-92d4-47a1-a47d-0c04ec2cbd33",
   "metadata": {},
   "source": [
    "## âš™ï¸ 3. Spark Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1d5537-11bc-4025-b7b7-d61f38bb8a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(url=\"https://spark.apache.org/docs/latest/img/cluster-overview.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ff1737-7bc6-4145-bb15-1406af065e70",
   "metadata": {},
   "source": [
    "### Spark Architektur: Cluster Overview)\r\n",
    "\r\n",
    "Dieses Diagramm zeigt **wie Spark verteilt arbeitet**:\r\n",
    "\r\n",
    "| Komponente | Beschreibung |\r\n",
    "|:---|:---|\r\n",
    "| **Driver Program** | Dein Hauptprogramm. Es steuert alles: Job-Aufteilung, Kommunikation, Fehlerbehandlung. |\r\n",
    "| **SparkContext** | Die zentrale Verbindung vom Driver zu Spark. Du programmierst damit die Aktionen und Transformationen. |\r\n",
    "| **Cluster Manager** | Verwaltet die Ressourcen im Cluster (CPU, RAM). Beispiele: YARN, Kubernetes, Standalone. |\r\n",
    "| **Worker Nodes** | Die Maschinen, die die eigentliche Datenverarbeitung Ã¼bernehmen. |\r\n",
    "| **Executor** | Ein Prozess auf einem Worker, der Tasks ausfÃ¼hrt und Daten im Speicher hÃ¤lt. |\r\n",
    "| **Task** | Die kleinste Recheneinheit. Ein Spark-Job wird in viele Tasks aufgeteilt. |\r\n",
    "| **Cache** | Zwischenspeicher (RAM) fÃ¼r Daten, die mehrfach gebraucht werden, damit sie nicht immer neu berechnet werden mÃ¼ssen. |\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### ðŸ”¥ Wichtig zu verstehen:\r\n",
    "\r\n",
    "- **Driver** â†’ Teilt Jobs auf und schickt sie Ã¼ber den **Cluster Manager** an die **Worker Nodes**.\r\n",
    "- Jeder **Worker** hat mindestens einen **Executor**, der wiederum mehrere **Tasks** parallel ausfÃ¼hrt.\r\n",
    "- **Caches** sparen Zeit, indem sie Daten im RAM statt auf der Platte halten.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### ðŸ§  Skeptische Beobachtungen:\r\n",
    "\r\n",
    "- **Single Point of Failure**: Wenn der Driver abstÃ¼rzt, ist der ganze Job verloren (auÃŸer du hast High Availability konfiguriert).\r\n",
    "- **Ressourcenverschwendung mÃ¶glich**: Executors brauchen RAM â€“ schlecht abgestimmt â†’ viele Out-of-Memory-Fehler.\r\n",
    "- **Netzwerk Bottleneck**: Schweres Shuffling (viel Datentausch zwischen Workern) kann Spark-Jonehmer nicht einfach abschalten.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1f5fd2-3eb5-4897-a91b-e1c338530cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abc35db4-f9e8-4475-b1fd-fa125d321ade",
   "metadata": {},
   "source": [
    "# âœ¨ 4.`SparkSession` als Einstiegspunkt\n",
    "\n",
    "- In Spark 2.0 und neuer ist **`SparkSession`** der *offizielle Einstiegspunkt* fÃ¼r jede Spark-Anwendung.\n",
    "- FrÃ¼her musste man `SparkContext`, `SQLContext`, `HiveContext` usw. separat erstellen â€” heute bÃ¼ndelt `SparkSession` alles in einem Objekt.\n",
    "\n",
    "**Merksatz**:  \n",
    "> **Ohne SparkSession â†’ kein Zugriff auf Spark.**\n",
    "\n",
    "**Beispiel:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "621d7b10-96ea-44d6-b5d9-244a72743d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Localspark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99335c5c-2926-4eef-a2f4-416e53f865a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3949ec0-20d0-420b-91f5-546fd6319156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fd4217-8fad-43d8-a177-89c26c948176",
   "metadata": {},
   "source": [
    "## ðŸ†š 5. DataFrame vs RDD (nur die Basics)\n",
    "\n",
    "| Feature | DataFrame | RDD |\n",
    "|:--------|:----------|:----|\n",
    "| **Definition** | Tabelle mit Spalten und Datentypen (Ã¤hnlich SQL oder Pandas) | Rohes, unstrukturiertes Datenset (verteilte Liste von Objekten) |\n",
    "| **Benutzerfreundlichkeit** | Hoch â€“ SQL-Ã¤hnliche Operationen | Niedrig â€“ eigene Map/Reduce-Logik schreiben |\n",
    "| **Performance** | Optimiert durch Catalyst & Tungsten Engine | Weniger optimiert (du musst dich selbst um Performance kÃ¼mmern) |\n",
    "| **Anwendungsfall** | Klassische Datenverarbeitung, Analytics, Machine Learning Pipelines | Low-Level Transformationen, wenn extreme FlexibilitÃ¤t gebraucht wird |\n",
    "\n",
    "**Skeptische Wahrheit**:  \n",
    "> Wer heute noch direkt mit RDDs arbeitet, hat meist ein sehr spezielles Problem â€” oder kein Vertrauen in Spark-Optimierungen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd0d8d7-1239-45f6-b1d0-2318e5693fbf",
   "metadata": {},
   "source": [
    "## ðŸ”„ 6. Lebenszyklus eines DataFrames in Spark\n",
    "\n",
    "**Was passiert intern?**\n",
    "\n",
    "1. **Definition**:  \n",
    "   Du schreibst Transformationen (`select`, `filter`, `join`), aber Spark fÃ¼hrt *noch nichts* aus.\n",
    "\n",
    "2. **Logischer Plan**:  \n",
    "   Spark erstellt einen logischen Ablaufplan (nur eine Beschreibung, noch keine AusfÃ¼hrung).\n",
    "\n",
    "3. **Physikalischer Plan**:  \n",
    "   Spark optimiert den logischen Plan zu einem ausfÃ¼hrbaren Programm (z.â€¯B. Broadcast Joins, Predicate Pushdown).\n",
    "\n",
    "4. **AusfÃ¼hrung (Action!)**:  \n",
    "   Erst wenn eine **Action** kommt (`show()`, `collect()`, `write()`) wird der Plan tatsÃ¤chlich ausgefÃ¼hrt â†’ *Lazy Execution Prinzip.*\n",
    "\n",
    "**Grafisch:**\n",
    "\n",
    "```plaintext\n",
    "Transformationen -> Logischer Plan -> Optimierter physikalischer Plan -> Task-AusfÃ¼hrung\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797f89f0-98d6-49b9-9141-dace2f7e627a",
   "metadata": {},
   "source": [
    "## ðŸ“Š 7. Daten erkunden und transformieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f91438-e013-4483-90ab-e886fb10edc4",
   "metadata": {},
   "source": [
    "### CSV Lesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61d153b7-4ac3-4061-b843-5291d2ea2e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+--------------------+-------------------+------+\n",
      "|   name|age|      city|               email|          job_title|salary|\n",
      "+-------+---+----------+--------------------+-------------------+------+\n",
      "|   Anna| 28|    Berlin|anna.mueller@exam...|       Data Analyst| 52000|\n",
      "|    Ben| 35|   Hamburg|ben.schmidt@examp...|  Software Engineer| 74000|\n",
      "|  Clara| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|\n",
      "|  David| 40|   Cologne|david.schneider@e...|    Project Manager| 83000|\n",
      "|    Eva| 31| Stuttgart|eva.huber@example...|     Data Scientist| 68000|\n",
      "|  Felix| 29|    Berlin|felix.wagner@exam...|    DevOps Engineer|  NULL|\n",
      "|   Gina| 45|   Hamburg|gina.fischer@exam...|         HR Manager| 69000|\n",
      "| Hannah| 26| Frankfurt|hannah.koch@examp...|        UI Designer| 54000|\n",
      "|   Igor| 38|    Munich|igor.keller@examp...|      Sales Manager| 75000|\n",
      "|  Julia| 24|DÃ¼sseldorf|julia.schmitt@exa...|    Content Creator| 41000|\n",
      "|   Karl| 50|    Berlin|karl.bauer@exampl...|                CTO|120000|\n",
      "|   Lina| 33| Stuttgart|lina.maier@exampl...|         Accountant| 58000|\n",
      "|    Max| 41|   Cologne|max.frank@example...|   Network Engineer| 71000|\n",
      "|   Nina| 30|   Hamburg|nina.lehmann@exam...|      Product Owner|  NULL|\n",
      "| Oliver| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|\n",
      "|  Paula| 36|    Munich|paula.hartmann@ex...|   Product Designer| 65000|\n",
      "|Quentin| 43| Frankfurt|quentin.schulz@ex...|         Consultant| 80000|\n",
      "|   Rita| 25|DÃ¼sseldorf|rita.lang@example...|   Junior Developer| 47000|\n",
      "| Stefan| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|\n",
      "|   Tina| 32|   Cologne|tina.kraus@exampl...|      Data Engineer| 73000|\n",
      "+-------+---+----------+--------------------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Korrekte RAW-URL\n",
    "url = \"https://raw.githubusercontent.com/gadanes/spark_kurs/main/notebooks/data.csv\"\n",
    "\n",
    "# Lade die CSV-Datei herunter\n",
    "response = requests.get(url)\n",
    "with open(\"/tmp/data.csv\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Lese die CSV-Datei mit Spark ein\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"/tmp/data.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817713d1-ed22-4581-862f-11242502f14d",
   "metadata": {},
   "source": [
    "### Spalten auswÃ¤hlen (`select`)\n",
    "\n",
    "Mit `.select()` kannst du gezielt bestimmte Spalten aus dem DataFrame auswÃ¤hlen.\n",
    "\n",
    "**Merke:**  \n",
    "> `.select()` **verÃ¤ndert** das ursprÃ¼ngliche DataFrame **nicht**.  \n",
    "> Es erzeugt eine **neue** Version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af63e456-558b-4fc4-a1a0-2770f586e7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------------+\n",
      "|   name|      city|          job_title|\n",
      "+-------+----------+-------------------+\n",
      "|   Anna|    Berlin|       Data Analyst|\n",
      "|    Ben|   Hamburg|  Software Engineer|\n",
      "|  Clara|    Munich|Marketing Assistant|\n",
      "|  David|   Cologne|    Project Manager|\n",
      "|    Eva| Stuttgart|     Data Scientist|\n",
      "|  Felix|    Berlin|    DevOps Engineer|\n",
      "|   Gina|   Hamburg|         HR Manager|\n",
      "| Hannah| Frankfurt|        UI Designer|\n",
      "|   Igor|    Munich|      Sales Manager|\n",
      "|  Julia|DÃ¼sseldorf|    Content Creator|\n",
      "|   Karl|    Berlin|                CTO|\n",
      "|   Lina| Stuttgart|         Accountant|\n",
      "|    Max|   Cologne|   Network Engineer|\n",
      "|   Nina|   Hamburg|      Product Owner|\n",
      "| Oliver|    Berlin|  Backend Developer|\n",
      "|  Paula|    Munich|   Product Designer|\n",
      "|Quentin| Frankfurt|         Consultant|\n",
      "|   Rita|DÃ¼sseldorf|   Junior Developer|\n",
      "| Stefan| Stuttgart|         IT Support|\n",
      "|   Tina|   Cologne|      Data Engineer|\n",
      "+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\", \"city\", \"job_title\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf1b4e3-ba14-4f38-95e3-8a855a1da02b",
   "metadata": {},
   "source": [
    "### Zeilen filtern (`filter`, `where`)\n",
    "\n",
    "Mit `.filter()` oder `.where()` kannst du Zeilen nach Bedingungen auswÃ¤hlen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d5a474d-0d16-4a79-8f59-1e924e8223f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+--------------------+-----------------+------+\n",
      "|   name|age|     city|               email|        job_title|salary|\n",
      "+-------+---+---------+--------------------+-----------------+------+\n",
      "|    Ben| 35|  Hamburg|ben.schmidt@examp...|Software Engineer| 74000|\n",
      "|  David| 40|  Cologne|david.schneider@e...|  Project Manager| 83000|\n",
      "|    Eva| 31|Stuttgart|eva.huber@example...|   Data Scientist| 68000|\n",
      "|   Gina| 45|  Hamburg|gina.fischer@exam...|       HR Manager| 69000|\n",
      "|   Igor| 38|   Munich|igor.keller@examp...|    Sales Manager| 75000|\n",
      "|   Karl| 50|   Berlin|karl.bauer@exampl...|              CTO|120000|\n",
      "|   Lina| 33|Stuttgart|lina.maier@exampl...|       Accountant| 58000|\n",
      "|    Max| 41|  Cologne|max.frank@example...| Network Engineer| 71000|\n",
      "|  Paula| 36|   Munich|paula.hartmann@ex...| Product Designer| 65000|\n",
      "|Quentin| 43|Frankfurt|quentin.schulz@ex...|       Consultant| 80000|\n",
      "| Stefan| 39|Stuttgart|stefan.becker@exa...|       IT Support| 56000|\n",
      "|   Tina| 32|  Cologne|tina.kraus@exampl...|    Data Engineer| 73000|\n",
      "+-------+---+---------+--------------------+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.age > 30).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38213a20-b01d-4c6b-ba69-0f6f37cebab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+--------------------+---------------+------+\n",
      "| name|age|   city|               email|      job_title|salary|\n",
      "+-----+---+-------+--------------------+---------------+------+\n",
      "|David| 40|Cologne|david.schneider@e...|Project Manager| 83000|\n",
      "| Karl| 50| Berlin|karl.bauer@exampl...|            CTO|120000|\n",
      "+-----+---+-------+--------------------+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.salary > 80000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d50acc1-d00d-453f-b7ae-90a0ca015ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+--------------------+-----------------+------+\n",
      "|   name|age|     city|               email|        job_title|salary|\n",
      "+-------+---+---------+--------------------+-----------------+------+\n",
      "|    Ben| 35|  Hamburg|ben.schmidt@examp...|Software Engineer| 74000|\n",
      "|  David| 40|  Cologne|david.schneider@e...|  Project Manager| 83000|\n",
      "|    Eva| 31|Stuttgart|eva.huber@example...|   Data Scientist| 68000|\n",
      "|   Gina| 45|  Hamburg|gina.fischer@exam...|       HR Manager| 69000|\n",
      "|   Igor| 38|   Munich|igor.keller@examp...|    Sales Manager| 75000|\n",
      "|   Karl| 50|   Berlin|karl.bauer@exampl...|              CTO|120000|\n",
      "|   Lina| 33|Stuttgart|lina.maier@exampl...|       Accountant| 58000|\n",
      "|    Max| 41|  Cologne|max.frank@example...| Network Engineer| 71000|\n",
      "|  Paula| 36|   Munich|paula.hartmann@ex...| Product Designer| 65000|\n",
      "|Quentin| 43|Frankfurt|quentin.schulz@ex...|       Consultant| 80000|\n",
      "| Stefan| 39|Stuttgart|stefan.becker@exa...|       IT Support| 56000|\n",
      "|   Tina| 32|  Cologne|tina.kraus@exampl...|    Data Engineer| 73000|\n",
      "+-------+---+---------+--------------------+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.age > 30).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09904709-5ee5-4fe2-aefd-5ec1a7349d02",
   "metadata": {},
   "source": [
    "**Hinweis:**  \n",
    "> **`filter`** und **`where`** sind **identisch** â€“ es ist reine Geschmackssache."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0259e2-e095-461e-ad9a-a8e85242e731",
   "metadata": {},
   "source": [
    "### Neue Spalten erstellen (`withColumn`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c7e63c4-d344-473b-86d2-2f63c34be827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+--------------------+-------------------+------+----------+\n",
      "|   name|age|      city|               email|          job_title|salary|age_plus_5|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+\n",
      "|   Anna| 28|    Berlin|anna.mueller@exam...|       Data Analyst| 52000|      33.0|\n",
      "|    Ben| 35|   Hamburg|ben.schmidt@examp...|  Software Engineer| 74000|      40.0|\n",
      "|  Clara| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|      27.0|\n",
      "|  David| 40|   Cologne|david.schneider@e...|    Project Manager| 83000|      45.0|\n",
      "|    Eva| 31| Stuttgart|eva.huber@example...|     Data Scientist| 68000|      36.0|\n",
      "|  Felix| 29|    Berlin|felix.wagner@exam...|    DevOps Engineer|  NULL|      34.0|\n",
      "|   Gina| 45|   Hamburg|gina.fischer@exam...|         HR Manager| 69000|      50.0|\n",
      "| Hannah| 26| Frankfurt|hannah.koch@examp...|        UI Designer| 54000|      31.0|\n",
      "|   Igor| 38|    Munich|igor.keller@examp...|      Sales Manager| 75000|      43.0|\n",
      "|  Julia| 24|DÃ¼sseldorf|julia.schmitt@exa...|    Content Creator| 41000|      29.0|\n",
      "|   Karl| 50|    Berlin|karl.bauer@exampl...|                CTO|120000|      55.0|\n",
      "|   Lina| 33| Stuttgart|lina.maier@exampl...|         Accountant| 58000|      38.0|\n",
      "|    Max| 41|   Cologne|max.frank@example...|   Network Engineer| 71000|      46.0|\n",
      "|   Nina| 30|   Hamburg|nina.lehmann@exam...|      Product Owner|  NULL|      35.0|\n",
      "| Oliver| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|      32.0|\n",
      "|  Paula| 36|    Munich|paula.hartmann@ex...|   Product Designer| 65000|      41.0|\n",
      "|Quentin| 43| Frankfurt|quentin.schulz@ex...|         Consultant| 80000|      48.0|\n",
      "|   Rita| 25|DÃ¼sseldorf|rita.lang@example...|   Junior Developer| 47000|      30.0|\n",
      "| Stefan| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|      44.0|\n",
      "|   Tina| 32|   Cologne|tina.kraus@exampl...|      Data Engineer| 73000|      37.0|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"age_plus_5\", col(\"age\") + 5)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0529f6cd-049d-49ac-b7c6-5cfbcd3bc5a5",
   "metadata": {},
   "source": [
    "**Wichtig:**  \n",
    "> Jede `.withColumn()`-Operation erstellt **intern ein neues DataFrame** â€” Spark verÃ¤ndert nie das Originalobjekt direkt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d5ce0-73c3-41f4-894f-649d02c3ccd1",
   "metadata": {},
   "source": [
    "### Bedingte Logik (`when`, `otherwise`)\n",
    "\n",
    "Mit `when` und `otherwise` kannst du **Bedingungen** einbauen, Ã¤hnlich wie `if-else`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81e150e6-f8cf-4d1f-a02f-3320d4626eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "|   name|age|      city|               email|          job_title|salary|age_plus_5|income_category|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "|   Anna| 28|    Berlin|anna.mueller@exam...|       Data Analyst| 52000|      33.0|Normalverdiener|\n",
      "|    Ben| 35|   Hamburg|ben.schmidt@examp...|  Software Engineer| 74000|      40.0|Normalverdiener|\n",
      "|  Clara| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|      27.0|Geringverdiener|\n",
      "|  David| 40|   Cologne|david.schneider@e...|    Project Manager| 83000|      45.0|  Hochverdiener|\n",
      "|    Eva| 31| Stuttgart|eva.huber@example...|     Data Scientist| 68000|      36.0|Normalverdiener|\n",
      "|  Felix| 29|    Berlin|felix.wagner@exam...|    DevOps Engineer|  NULL|      34.0|Geringverdiener|\n",
      "|   Gina| 45|   Hamburg|gina.fischer@exam...|         HR Manager| 69000|      50.0|Normalverdiener|\n",
      "| Hannah| 26| Frankfurt|hannah.koch@examp...|        UI Designer| 54000|      31.0|Normalverdiener|\n",
      "|   Igor| 38|    Munich|igor.keller@examp...|      Sales Manager| 75000|      43.0|Normalverdiener|\n",
      "|  Julia| 24|DÃ¼sseldorf|julia.schmitt@exa...|    Content Creator| 41000|      29.0|Geringverdiener|\n",
      "|   Karl| 50|    Berlin|karl.bauer@exampl...|                CTO|120000|      55.0|  Hochverdiener|\n",
      "|   Lina| 33| Stuttgart|lina.maier@exampl...|         Accountant| 58000|      38.0|Normalverdiener|\n",
      "|    Max| 41|   Cologne|max.frank@example...|   Network Engineer| 71000|      46.0|Normalverdiener|\n",
      "|   Nina| 30|   Hamburg|nina.lehmann@exam...|      Product Owner|  NULL|      35.0|Geringverdiener|\n",
      "| Oliver| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|      32.0|Normalverdiener|\n",
      "|  Paula| 36|    Munich|paula.hartmann@ex...|   Product Designer| 65000|      41.0|Normalverdiener|\n",
      "|Quentin| 43| Frankfurt|quentin.schulz@ex...|         Consultant| 80000|      48.0|  Hochverdiener|\n",
      "|   Rita| 25|DÃ¼sseldorf|rita.lang@example...|   Junior Developer| 47000|      30.0|Normalverdiener|\n",
      "| Stefan| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|      44.0|Normalverdiener|\n",
      "|   Tina| 32|   Cologne|tina.kraus@exampl...|      Data Engineer| 73000|      37.0|Normalverdiener|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"income_category\",\n",
    "    when(col(\"salary\") >= 80000, \"Hochverdiener\")\n",
    "    .when((col(\"salary\") >= 43000) & (col(\"salary\") < 80000), \"Normalverdiener\")\n",
    "    .otherwise(\"Geringverdiener\")\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af09c43-3bca-46ca-b6bd-bdfa3903bffc",
   "metadata": {},
   "source": [
    "**Merke:**  \n",
    "> Viele verschachtelte `when`-Bedingungen kÃ¶nnen unÃ¼bersichtlich werden â†’ sauber strukturieren!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2954d6-368d-436f-bd07-8a7530190d8f",
   "metadata": {},
   "source": [
    "### Umgang mit fehlenden Werten (`fillna`, `dropna`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58cd589f-cb93-4841-b3d2-25994c39567b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "|   name|age|      city|               email|          job_title|salary|age_plus_5|income_category|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "|   Anna| 28|    Berlin|anna.mueller@exam...|       Data Analyst| 52000|      33.0|Normalverdiener|\n",
      "|    Ben| 35|   Hamburg|ben.schmidt@examp...|  Software Engineer| 74000|      40.0|Normalverdiener|\n",
      "|  Clara| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|      27.0|Geringverdiener|\n",
      "|  David| 40|   Cologne|david.schneider@e...|    Project Manager| 83000|      45.0|  Hochverdiener|\n",
      "|    Eva| 31| Stuttgart|eva.huber@example...|     Data Scientist| 68000|      36.0|Normalverdiener|\n",
      "|  Felix| 29|    Berlin|felix.wagner@exam...|    DevOps Engineer| 50000|      34.0|Geringverdiener|\n",
      "|   Gina| 45|   Hamburg|gina.fischer@exam...|         HR Manager| 69000|      50.0|Normalverdiener|\n",
      "| Hannah| 26| Frankfurt|hannah.koch@examp...|        UI Designer| 54000|      31.0|Normalverdiener|\n",
      "|   Igor| 38|    Munich|igor.keller@examp...|      Sales Manager| 75000|      43.0|Normalverdiener|\n",
      "|  Julia| 24|DÃ¼sseldorf|julia.schmitt@exa...|    Content Creator| 41000|      29.0|Geringverdiener|\n",
      "|   Karl| 50|    Berlin|karl.bauer@exampl...|                CTO|120000|      55.0|  Hochverdiener|\n",
      "|   Lina| 33| Stuttgart|lina.maier@exampl...|         Accountant| 58000|      38.0|Normalverdiener|\n",
      "|    Max| 41|   Cologne|max.frank@example...|   Network Engineer| 71000|      46.0|Normalverdiener|\n",
      "|   Nina| 30|   Hamburg|nina.lehmann@exam...|      Product Owner| 50000|      35.0|Geringverdiener|\n",
      "| Oliver| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|      32.0|Normalverdiener|\n",
      "|  Paula| 36|    Munich|paula.hartmann@ex...|   Product Designer| 65000|      41.0|Normalverdiener|\n",
      "|Quentin| 43| Frankfurt|quentin.schulz@ex...|         Consultant| 80000|      48.0|  Hochverdiener|\n",
      "|   Rita| 25|DÃ¼sseldorf|rita.lang@example...|   Junior Developer| 47000|      30.0|Geringverdiener|\n",
      "| Stefan| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|      44.0|Normalverdiener|\n",
      "|   Tina| 32|   Cologne|tina.kraus@exampl...|      Data Engineer| 73000|      37.0|Normalverdiener|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fehlende Werte fÃ¼llen (`fillna`):\n",
    "# Ersetzt fehlende GehÃ¤lter durch 50000\n",
    "df_filled = df.fillna({\"salary\": 50000})\n",
    "df_filled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b47df62-00cc-4dac-9afc-d5da14cf63d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "|   name|age|      city|               email|          job_title|salary|age_plus_5|income_category|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "|   Anna| 28|    Berlin|anna.mueller@exam...|       Data Analyst| 52000|      33.0|Normalverdiener|\n",
      "|    Ben| 35|   Hamburg|ben.schmidt@examp...|  Software Engineer| 74000|      40.0|Normalverdiener|\n",
      "|  Clara| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|      27.0|Geringverdiener|\n",
      "|  David| 40|   Cologne|david.schneider@e...|    Project Manager| 83000|      45.0|  Hochverdiener|\n",
      "|    Eva| 31| Stuttgart|eva.huber@example...|     Data Scientist| 68000|      36.0|Normalverdiener|\n",
      "|   Gina| 45|   Hamburg|gina.fischer@exam...|         HR Manager| 69000|      50.0|Normalverdiener|\n",
      "| Hannah| 26| Frankfurt|hannah.koch@examp...|        UI Designer| 54000|      31.0|Normalverdiener|\n",
      "|   Igor| 38|    Munich|igor.keller@examp...|      Sales Manager| 75000|      43.0|Normalverdiener|\n",
      "|  Julia| 24|DÃ¼sseldorf|julia.schmitt@exa...|    Content Creator| 41000|      29.0|Geringverdiener|\n",
      "|   Karl| 50|    Berlin|karl.bauer@exampl...|                CTO|120000|      55.0|  Hochverdiener|\n",
      "|   Lina| 33| Stuttgart|lina.maier@exampl...|         Accountant| 58000|      38.0|Normalverdiener|\n",
      "|    Max| 41|   Cologne|max.frank@example...|   Network Engineer| 71000|      46.0|Normalverdiener|\n",
      "| Oliver| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|      32.0|Normalverdiener|\n",
      "|  Paula| 36|    Munich|paula.hartmann@ex...|   Product Designer| 65000|      41.0|Normalverdiener|\n",
      "|Quentin| 43| Frankfurt|quentin.schulz@ex...|         Consultant| 80000|      48.0|  Hochverdiener|\n",
      "|   Rita| 25|DÃ¼sseldorf|rita.lang@example...|   Junior Developer| 47000|      30.0|Geringverdiener|\n",
      "| Stefan| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|      44.0|Normalverdiener|\n",
      "|   Tina| 32|   Cologne|tina.kraus@exampl...|      Data Engineer| 73000|      37.0|Normalverdiener|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zeilen mit fehlenden Werten lÃ¶schen (`dropna`):\n",
    "# Entfernt alle Zeilen, die mindestens einen `null`-Wert enthalten.\n",
    "df_clean = df.dropna()\n",
    "df_clean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e5eaa6-75f7-4694-8a6a-340bf1cd39ba",
   "metadata": {},
   "source": [
    "### Gruppieren und Aggregieren (`groupBy` + `agg`)\n",
    "\n",
    "Mit `.groupBy()` kannst du dein DataFrame nach einer oder mehreren Spalten **gruppieren**.  \n",
    "Mit `.agg()` kannst du dann **Aggregationfunktionen** auf jede Gruppe anwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c443ed8a-8e61-4b41-b701-2d70fb240264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      city|       avg(salary)|\n",
      "+----------+------------------+\n",
      "| Frankfurt|           67000.0|\n",
      "|    Berlin| 79666.66666666667|\n",
      "|DÃ¼sseldorf|           44000.0|\n",
      "|   Hamburg|           71500.0|\n",
      "| Stuttgart|60666.666666666664|\n",
      "|   Cologne| 75666.66666666667|\n",
      "|    Munich|60666.666666666664|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "#Durchschnitt berechnen\n",
    "df.groupBy(\"city\").agg(F.avg(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "934f27be-0407-4e6c-b991-d142d6f36b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      city|count|\n",
      "+----------+-----+\n",
      "| Frankfurt|    2|\n",
      "|    Berlin|    4|\n",
      "|DÃ¼sseldorf|    2|\n",
      "|   Hamburg|    3|\n",
      "| Stuttgart|    3|\n",
      "|   Cologne|    3|\n",
      "|    Munich|    3|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ZÃ¤hlt die Anzahl der EintrÃ¤ge\n",
    "df.groupBy(\"city\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "496db719-ab31-4d74-803e-df98241edeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|      city|sum(salary)|\n",
      "+----------+-----------+\n",
      "| Frankfurt|   134000.0|\n",
      "|    Berlin|   239000.0|\n",
      "|DÃ¼sseldorf|    88000.0|\n",
      "|   Hamburg|   143000.0|\n",
      "| Stuttgart|   182000.0|\n",
      "|   Cologne|   227000.0|\n",
      "|    Munich|   182000.0|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Summiert Werte einer Spalte\n",
    "df.groupBy(\"city\").agg(F.sum(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434926aa-1dc1-486d-8e0b-bfd06df133aa",
   "metadata": {},
   "source": [
    "**Wichtig zu wissen:**  \n",
    "> `.groupBy()` alleine macht noch nichts. Erst `.agg()`, `.count()`, `.sum()` oder `.avg()` lÃ¶sen die echte Berechnung aus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f41c4d-506e-47fc-b1c9-f3246af5c8f9",
   "metadata": {},
   "source": [
    "### Sortieren und Reihenfolge Ã¤ndern (`orderBy`, `sort`)\n",
    "\n",
    "Mit `.orderBy()` oder `.sort()` kannst du dein DataFrame sortieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a905c80-ee74-4d72-90a3-8861ec215020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "|   name|age|      city|               email|          job_title|salary|age_plus_5|income_category|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "|  David| 40|   Cologne|david.schneider@e...|    Project Manager| 83000|      45.0|  Hochverdiener|\n",
      "|Quentin| 43| Frankfurt|quentin.schulz@ex...|         Consultant| 80000|      48.0|  Hochverdiener|\n",
      "|   Igor| 38|    Munich|igor.keller@examp...|      Sales Manager| 75000|      43.0|Normalverdiener|\n",
      "|    Ben| 35|   Hamburg|ben.schmidt@examp...|  Software Engineer| 74000|      40.0|Normalverdiener|\n",
      "|   Tina| 32|   Cologne|tina.kraus@exampl...|      Data Engineer| 73000|      37.0|Normalverdiener|\n",
      "|    Max| 41|   Cologne|max.frank@example...|   Network Engineer| 71000|      46.0|Normalverdiener|\n",
      "|   Gina| 45|   Hamburg|gina.fischer@exam...|         HR Manager| 69000|      50.0|Normalverdiener|\n",
      "|    Eva| 31| Stuttgart|eva.huber@example...|     Data Scientist| 68000|      36.0|Normalverdiener|\n",
      "| Oliver| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|      32.0|Normalverdiener|\n",
      "|  Paula| 36|    Munich|paula.hartmann@ex...|   Product Designer| 65000|      41.0|Normalverdiener|\n",
      "|   Lina| 33| Stuttgart|lina.maier@exampl...|         Accountant| 58000|      38.0|Normalverdiener|\n",
      "| Stefan| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|      44.0|Normalverdiener|\n",
      "| Hannah| 26| Frankfurt|hannah.koch@examp...|        UI Designer| 54000|      31.0|Normalverdiener|\n",
      "|   Anna| 28|    Berlin|anna.mueller@exam...|       Data Analyst| 52000|      33.0|Normalverdiener|\n",
      "|   Rita| 25|DÃ¼sseldorf|rita.lang@example...|   Junior Developer| 47000|      30.0|Normalverdiener|\n",
      "|  Clara| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|      27.0|Geringverdiener|\n",
      "|  Julia| 24|DÃ¼sseldorf|julia.schmitt@exa...|    Content Creator| 41000|      29.0|Geringverdiener|\n",
      "|   Karl| 50|    Berlin|karl.bauer@exampl...|                CTO|120000|      55.0|  Hochverdiener|\n",
      "|  Felix| 29|    Berlin|felix.wagner@exam...|    DevOps Engineer|  NULL|      34.0|Geringverdiener|\n",
      "|   Nina| 30|   Hamburg|nina.lehmann@exam...|      Product Owner|  NULL|      35.0|Geringverdiener|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Nach Gehalt absteigend sortieren\n",
    "df.orderBy(F.desc(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25eb8450-e829-4e4e-b3fa-abf6e34b0f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "|   name|age|      city|               email|          job_title|salary|age_plus_5|income_category|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "|  Clara| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|      27.0|Geringverdiener|\n",
      "|  Julia| 24|DÃ¼sseldorf|julia.schmitt@exa...|    Content Creator| 41000|      29.0|Geringverdiener|\n",
      "|   Rita| 25|DÃ¼sseldorf|rita.lang@example...|   Junior Developer| 47000|      30.0|Normalverdiener|\n",
      "| Hannah| 26| Frankfurt|hannah.koch@examp...|        UI Designer| 54000|      31.0|Normalverdiener|\n",
      "| Oliver| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|      32.0|Normalverdiener|\n",
      "|   Anna| 28|    Berlin|anna.mueller@exam...|       Data Analyst| 52000|      33.0|Normalverdiener|\n",
      "|  Felix| 29|    Berlin|felix.wagner@exam...|    DevOps Engineer|  NULL|      34.0|Geringverdiener|\n",
      "|   Nina| 30|   Hamburg|nina.lehmann@exam...|      Product Owner|  NULL|      35.0|Geringverdiener|\n",
      "|    Eva| 31| Stuttgart|eva.huber@example...|     Data Scientist| 68000|      36.0|Normalverdiener|\n",
      "|   Tina| 32|   Cologne|tina.kraus@exampl...|      Data Engineer| 73000|      37.0|Normalverdiener|\n",
      "|   Lina| 33| Stuttgart|lina.maier@exampl...|         Accountant| 58000|      38.0|Normalverdiener|\n",
      "|    Ben| 35|   Hamburg|ben.schmidt@examp...|  Software Engineer| 74000|      40.0|Normalverdiener|\n",
      "|  Paula| 36|    Munich|paula.hartmann@ex...|   Product Designer| 65000|      41.0|Normalverdiener|\n",
      "|   Igor| 38|    Munich|igor.keller@examp...|      Sales Manager| 75000|      43.0|Normalverdiener|\n",
      "| Stefan| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|      44.0|Normalverdiener|\n",
      "|  David| 40|   Cologne|david.schneider@e...|    Project Manager| 83000|      45.0|  Hochverdiener|\n",
      "|    Max| 41|   Cologne|max.frank@example...|   Network Engineer| 71000|      46.0|Normalverdiener|\n",
      "|Quentin| 43| Frankfurt|quentin.schulz@ex...|         Consultant| 80000|      48.0|  Hochverdiener|\n",
      "|   Gina| 45|   Hamburg|gina.fischer@exam...|         HR Manager| 69000|      50.0|Normalverdiener|\n",
      "|   Karl| 50|    Berlin|karl.bauer@exampl...|                CTO|120000|      55.0|  Hochverdiener|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Oder aufsteigend (Standard)\n",
    "df.orderBy(\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c46d30d-767e-4114-8e25-3f576ee60894",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ðŸ”ƒ Sortieren und Reihenfolge Ã¤ndern (`orderBy`, `sort`)\n",
    "\n",
    "Mit `.orderBy()` oder `.sort()` kannst du dein DataFrame sortieren.\n",
    "\n",
    "**Beispiel: Nach Gehalt absteigend sortieren:**\n",
    "\n",
    "```python\n",
    "df.orderBy(F.desc(\"salary\")).show()\n",
    "```\n",
    "\n",
    "**Oder aufsteigend (Standard):**\n",
    "\n",
    "```python\n",
    "df.orderBy(\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23c4293-1fc5-4d04-9474-60b5d87686c3",
   "metadata": {},
   "source": [
    "**Hinweis:**  \n",
    "> `orderBy` und `sort` sind **gleichwertig**.  \n",
    "> Bei riesigen DataFrames kann Sortieren teuer werden â†’ vorsichtig einsetzen!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9529da-148a-47b3-ab8b-a207b585e4e4",
   "metadata": {},
   "source": [
    "### Window Functions\n",
    "\n",
    "Window Functions erlauben dir, Ã¼ber eine **Teilmenge** deiner Daten zu arbeiten, **ohne** sie vollstÃ¤ndig zu aggregieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7344af77-fded-48b8-a1a0-56a1222e94f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+------------+\n",
      "|   name|age|      city|               email|          job_title|salary|age_plus_5|income_category|rank_in_city|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+------------+\n",
      "| Oliver| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|      32.0|Normalverdiener|           1|\n",
      "|   Anna| 28|    Berlin|anna.mueller@exam...|       Data Analyst| 52000|      33.0|Normalverdiener|           2|\n",
      "|   Karl| 50|    Berlin|karl.bauer@exampl...|                CTO|120000|      55.0|  Hochverdiener|           3|\n",
      "|  Felix| 29|    Berlin|felix.wagner@exam...|    DevOps Engineer|  NULL|      34.0|Geringverdiener|           4|\n",
      "|  David| 40|   Cologne|david.schneider@e...|    Project Manager| 83000|      45.0|  Hochverdiener|           1|\n",
      "|   Tina| 32|   Cologne|tina.kraus@exampl...|      Data Engineer| 73000|      37.0|Normalverdiener|           2|\n",
      "|    Max| 41|   Cologne|max.frank@example...|   Network Engineer| 71000|      46.0|Normalverdiener|           3|\n",
      "|   Rita| 25|DÃ¼sseldorf|rita.lang@example...|   Junior Developer| 47000|      30.0|Normalverdiener|           1|\n",
      "|  Julia| 24|DÃ¼sseldorf|julia.schmitt@exa...|    Content Creator| 41000|      29.0|Geringverdiener|           2|\n",
      "|Quentin| 43| Frankfurt|quentin.schulz@ex...|         Consultant| 80000|      48.0|  Hochverdiener|           1|\n",
      "| Hannah| 26| Frankfurt|hannah.koch@examp...|        UI Designer| 54000|      31.0|Normalverdiener|           2|\n",
      "|    Ben| 35|   Hamburg|ben.schmidt@examp...|  Software Engineer| 74000|      40.0|Normalverdiener|           1|\n",
      "|   Gina| 45|   Hamburg|gina.fischer@exam...|         HR Manager| 69000|      50.0|Normalverdiener|           2|\n",
      "|   Nina| 30|   Hamburg|nina.lehmann@exam...|      Product Owner|  NULL|      35.0|Geringverdiener|           3|\n",
      "|   Igor| 38|    Munich|igor.keller@examp...|      Sales Manager| 75000|      43.0|Normalverdiener|           1|\n",
      "|  Paula| 36|    Munich|paula.hartmann@ex...|   Product Designer| 65000|      41.0|Normalverdiener|           2|\n",
      "|  Clara| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|      27.0|Geringverdiener|           3|\n",
      "|    Eva| 31| Stuttgart|eva.huber@example...|     Data Scientist| 68000|      36.0|Normalverdiener|           1|\n",
      "|   Lina| 33| Stuttgart|lina.maier@exampl...|         Accountant| 58000|      38.0|Normalverdiener|           2|\n",
      "| Stefan| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|      44.0|Normalverdiener|           3|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/29 08:00:42 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@baf44cee0c18:38801\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "25/04/29 08:00:51 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@baf44cee0c18:38801\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"city\").orderBy(F.desc(\"salary\"))\n",
    "\n",
    "df = df.withColumn(\"rank_in_city\", F.rank().over(window_spec))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4faa86-2ba9-4336-ba12-0ece074eb1ee",
   "metadata": {},
   "source": [
    "### Lesen von Azure Datenbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1808200-74c9-4ef7-8acf-8ce882caffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Localspark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\",\n",
    "        \"com.microsoft.sqlserver:mssql-jdbc:12.6.1.jre11\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0348963-f55f-4a87-8fb8-737c634b3ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d8551f5",
   "metadata": {},
   "source": [
    "# Hour 1: Introduction to PySpark\n",
    "\n",
    "- What is Apache Spark?\n",
    "- Differences between PySpark and Pandas\n",
    "- Spark architecture overview (driver, executor, cluster manager)\n",
    "- Setting up Spark locally using Docker or pip\n",
    "- Creating a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64ef118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already included, reinforce\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"IntroSession\").getOrCreate()\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19e2784",
   "metadata": {},
   "source": [
    "# Hour 2: DataFrame Operations\n",
    "\n",
    "- Creating DataFrames\n",
    "- Selecting, filtering, adding columns\n",
    "- Aggregation and grouping\n",
    "- Hands-on: Word count and basic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d79519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Create a DataFrame and perform groupBy aggregation\n",
    "data = [(\"apple\", 2), (\"banana\", 3), (\"apple\", 1)]\n",
    "df = spark.createDataFrame(data, [\"fruit\", \"count\"])\n",
    "df.groupBy(\"fruit\").sum(\"count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7451c3ee",
   "metadata": {},
   "source": [
    "# Hour 3: Working with External Data\n",
    "\n",
    "- Reading CSV, JSON, Parquet\n",
    "- Writing data to storage\n",
    "- Connecting to databases using JDBC (already introduced)\n",
    "- Data cleaning and null handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb5c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Read a CSV file\n",
    "csv_df = spark.read.option(\"header\", True).csv(\"sample_data.csv\")\n",
    "csv_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddc149c",
   "metadata": {},
   "source": [
    "# Hour 4: Advanced Transformations & Spark SQL\n",
    "\n",
    "- Using UDFs\n",
    "- Explode, arrays, maps\n",
    "- Date/timestamp operations\n",
    "- Writing and running SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150abafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create temp view and use SQL\n",
    "csv_df.createOrReplaceTempView(\"data\")\n",
    "sql_result = spark.sql(\"SELECT fruit, COUNT(*) FROM data GROUP BY fruit\")\n",
    "sql_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ff64ba",
   "metadata": {},
   "source": [
    "# Hour 5: Optimization and Performance\n",
    "\n",
    "- Spark execution plan with `explain()`\n",
    "- Caching and persistence\n",
    "- Partitioning strategies\n",
    "- Join types and broadcast joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb4fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Show query plan\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc315246",
   "metadata": {},
   "source": [
    "# Hour 6: Mini Project + Recap\n",
    "\n",
    "- Load a dataset\n",
    "- Perform transformations\n",
    "- Apply SQL or aggregations\n",
    "- Write results to file or database\n",
    "- Wrap-up discussion and review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0657ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for project code\n",
    "data = [(\"2023-01-01\", 100), (\"2023-01-02\", 120)]\n",
    "df = spark.createDataFrame(data, [\"date\", \"sales\"])\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
