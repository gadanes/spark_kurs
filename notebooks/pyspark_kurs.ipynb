{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17cbce6c-6f5c-4104-aac7-9e4d1a90feb1",
   "metadata": {},
   "source": [
    "## üß† 1. Was ist Spark? Warum PySpark?\n",
    "\n",
    "> \"Alle reden √ºber Spark ‚Äî aber was l√∂st es eigentlich?\"\n",
    "\n",
    "- **Apache Spark** ist eine verteilte Datenverarbeitungs-Engine.\n",
    "  - Anstatt dass ein Computer alles alleine macht (wie dein Laptop mit Python), verteilt Spark die Arbeit auf *viele* Rechner oder CPU-Kerne.\n",
    "  - *Einsatzgebiet*: Riesige Datenmengen und Machine Learning auf Skalenniveau.\n",
    "\n",
    "- **Warum nicht einfach Pandas oder reines Python?**\n",
    "  - *Wenn die Daten bequem in den Arbeitsspeicher passen ‚Üí Pandas ist v√∂llig ausreichend.*\n",
    "  - *Sind die Daten gr√∂√üer als der RAM ‚Üí Pandas bricht zusammen.*\n",
    "  - *Wenn echte Parallelverarbeitung gefragt ist ‚Üí Spark ist die richtige Wahl.*\n",
    "\n",
    "- **Warum PySpark?**\n",
    "  - Spark ist in Scala und Java geschrieben ‚Äî aber wer will f√ºr normale Datenverarbeitung in Java rumfrickeln?\n",
    "  - **PySpark** erlaubt dir, Spark mit Python zu steuern. Fast alle Vorteile, aber einfacher zu schreiben.\n",
    " \n",
    "- **Wann Scala/Java doch besser ist?**\n",
    "    - Wenn du maximale Performance brauchst (z. B. komplexe UDFs).\n",
    "    - Bei extrem gro√üen Datasets (Python-Overhead kann sp√ºrbar werden).\n",
    "    - F√ºr Spark-Interna-Entwicklung (z. B. eigene Spark-Erweiterungen).\n",
    "\n",
    "**Ehrliche Einsch√§tzung**:  \n",
    "> Nutze Spark nur, wenn du *musst* (Daten zu gro√ü, Performance wird kritisch).  \n",
    "> Ansonsten: **Pandas > PySpark** bei kleinen bis mittleren Daten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b26ccbf-d965-48a4-a67c-fe33ef45105d",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 2. Cluster Mode vs Local Mode\n",
    "\n",
    "Stell dir vor, du hast eine schwere Datenverarbeitung.\n",
    "\n",
    "| Modus | Was passiert | Wann verwenden | Wichtigster Punkt |\n",
    "|:----|:------------|:----------------|:-----------|\n",
    "| **Local Mode** | Spark l√§uft auf deinem Rechner, nutzt mehrere CPU-Kerne. | Entwicklung, Tests, kleine Datenmengen. | *Trainingsmodus.* |\n",
    "| **Cluster Mode** | Spark l√§uft auf mehreren Servern (Nodes). | Produktion, Big Data. | *Echte Skalierung.* |\n",
    "\n",
    "**Skizze:**\n",
    "\n",
    "```plaintext\n",
    "Local Mode:\n",
    "+-----------------+\n",
    "| Laptop          |\n",
    "| [Core1][Core2]  |\n",
    "| [Core3][Core4]  |\n",
    "+-----------------+\n",
    "\n",
    "Cluster Mode:\n",
    "+--------------------+     +-------------------+     +-------------------+\n",
    "| Worker Node 1      |     | Worker Node 2      |     | Worker Node 3      |\n",
    "| [Task1][Task2]     |     | [Task3][Task4]     |     | [Task5][Task6]     |\n",
    "+--------------------+     +-------------------+     +-------------------+\n",
    "          \\                    |                    /\n",
    "           \\                   |                   /\n",
    "                +----------------------------------+\n",
    "                |        Spark Driver Program     |\n",
    "                +----------------------------------+\n",
    "```\n",
    "\n",
    "> **Wichtig**: *Im Cluster Mode steuert ein Programm viele Maschinen.*  \n",
    "> **Local Mode simuliert nur ein Mini-Cluster auf deinem Laptop.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0562472-92d4-47a1-a47d-0c04ec2cbd33",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 3. Spark Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be1d5537-11bc-4025-b7b7-d61f38bb8a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://spark.apache.org/docs/latest/img/cluster-overview.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(url=\"https://spark.apache.org/docs/latest/img/cluster-overview.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ff1737-7bc6-4145-bb15-1406af065e70",
   "metadata": {},
   "source": [
    "### Spark Architektur: Cluster Overview)\r\n",
    "\r\n",
    "Dieses Diagramm zeigt **wie Spark verteilt arbeitet**:\r\n",
    "\r\n",
    "| Komponente | Beschreibung |\r\n",
    "|:---|:---|\r\n",
    "| **Driver Program** | Dein Hauptprogramm. Es steuert alles: Job-Aufteilung, Kommunikation, Fehlerbehandlung. |\r\n",
    "| **SparkContext** | Die zentrale Verbindung vom Driver zu Spark. Du programmierst damit die Aktionen und Transformationen. |\r\n",
    "| **Cluster Manager** | Verwaltet die Ressourcen im Cluster (CPU, RAM). Beispiele: YARN, Kubernetes, Standalone. |\r\n",
    "| **Worker Nodes** | Die Maschinen, die die eigentliche Datenverarbeitung √ºbernehmen. |\r\n",
    "| **Executor** | Ein Prozess auf einem Worker, der Tasks ausf√ºhrt und Daten im Speicher h√§lt. |\r\n",
    "| **Task** | Die kleinste Recheneinheit. Ein Spark-Job wird in viele Tasks aufgeteilt. |\r\n",
    "| **Cache** | Zwischenspeicher (RAM) f√ºr Daten, die mehrfach gebraucht werden, damit sie nicht immer neu berechnet werden m√ºssen. |\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### üî• Wichtig zu verstehen:\r\n",
    "\r\n",
    "- **Driver** ‚Üí Teilt Jobs auf und schickt sie √ºber den **Cluster Manager** an die **Worker Nodes**.\r\n",
    "- Jeder **Worker** hat mindestens einen **Executor**, der wiederum mehrere **Tasks** parallel ausf√ºhrt.\r\n",
    "- **Caches** sparen Zeit, indem sie Daten im RAM statt auf der Platte halten.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### üß† Skeptische Beobachtungen:\r\n",
    "\r\n",
    "- **Single Point of Failure**: Wenn der Driver abst√ºrzt, ist der ganze Job verloren (au√üer du hast High Availability konfiguriert).\r\n",
    "- **Ressourcenverschwendung m√∂glich**: Executors brauchen RAM ‚Äì schlecht abgestimmt ‚Üí viele Out-of-Memory-Fehler.\r\n",
    "- **Netzwerk Bottleneck**: Schweres Shuffling (viel Datentausch zwischen Workern) kann Spark-Jonehmer nicht einfach abschalten.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc35db4-f9e8-4475-b1fd-fa125d321ade",
   "metadata": {},
   "source": [
    "# ‚ú® 4.`SparkSession` als Einstiegspunkt\n",
    "\n",
    "- In Spark 2.0 und neuer ist **`SparkSession`** der *offizielle Einstiegspunkt* f√ºr jede Spark-Anwendung.\n",
    "- Fr√ºher musste man `SparkContext`, `SQLContext`, `HiveContext` usw. separat erstellen ‚Äî heute b√ºndelt `SparkSession` alles in einem Objekt.\n",
    "\n",
    "**Merksatz**:  \n",
    "> **Ohne SparkSession ‚Üí kein Zugriff auf Spark.**\n",
    "\n",
    "**Beispiel:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "621d7b10-96ea-44d6-b5d9-244a72743d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Localspark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ecc8f28-fcb4-48e5-9cb9-4dc489fe6245",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(\"Alice\", \"Engineering\", 65000, 28),\n",
    "    (\"Bob\", \"Marketing\", 58000, 32),\n",
    "    (\"Carol\", \"Engineering\", 72000, 35)], \n",
    "                           [\"name\", \"department\", \"salary\", \"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2882bafc-1d61-4128-accb-c0028513da45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+------+---+\n",
      "| name| department|salary|age|\n",
      "+-----+-----------+------+---+\n",
      "|Alice|Engineering| 65000| 28|\n",
      "|  Bob|  Marketing| 58000| 32|\n",
      "|Carol|Engineering| 72000| 35|\n",
      "+-----+-----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99335c5c-2926-4eef-a2f4-416e53f865a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3949ec0-20d0-420b-91f5-546fd6319156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fd4217-8fad-43d8-a177-89c26c948176",
   "metadata": {},
   "source": [
    "## üÜö 5. DataFrame vs RDD (nur die Basics)\n",
    "\n",
    "| Feature | DataFrame | RDD |\n",
    "|:--------|:----------|:----|\n",
    "| **Definition** | Tabelle mit Spalten und Datentypen (√§hnlich SQL oder Pandas) | Rohes, unstrukturiertes Datenset (verteilte Liste von Objekten) |\n",
    "| **Benutzerfreundlichkeit** | Hoch ‚Äì SQL-√§hnliche Operationen | Niedrig ‚Äì eigene Map/Reduce-Logik schreiben |\n",
    "| **Performance** | Optimiert durch Catalyst & Tungsten Engine | Weniger optimiert (du musst dich selbst um Performance k√ºmmern) |\n",
    "| **Anwendungsfall** | Klassische Datenverarbeitung, Analytics, Machine Learning Pipelines | Low-Level Transformationen, wenn extreme Flexibilit√§t gebraucht wird |\n",
    "\n",
    "**Skeptische Wahrheit**:  \n",
    "> Wer heute noch direkt mit RDDs arbeitet, hat meist ein sehr spezielles Problem ‚Äî oder kein Vertrauen in Spark-Optimierungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "134ca400-0e6c-4a4c-8c67-61e36451d278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('M√ºller', 35, 'Berlin'), ('Schmidt', 28, 'M√ºnchen')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# RDD aus Liste erstellen (verteilte Rohdaten)\n",
    "rdd = sc.parallelize([\n",
    "    (\"M√ºller\", 35, \"Berlin\"),\n",
    "    (\"Schmidt\", 28, \"M√ºnchen\")\n",
    "])\n",
    "\n",
    "data = rdd.collect()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f318be5a-3aef-42e3-b64d-722f2e00fc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------+\n",
      "|   Name|Alter|  Stadt|\n",
      "+-------+-----+-------+\n",
      "| M√ºller|   35| Berlin|\n",
      "|Schmidt|   28|M√ºnchen|\n",
      "+-------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# DataFrame mit Schema erstellen\n",
    "df = spark.createDataFrame(\n",
    "    [(\"M√ºller\", 35, \"Berlin\"), (\"Schmidt\", 28, \"M√ºnchen\")],\n",
    "    [\"Name\", \"Alter\", \"Stadt\"]\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d5a64cc-644c-45ba-adee-c6fe01949b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('M√ºller', 70), ('Schmidt', 56)]\n"
     ]
    }
   ],
   "source": [
    "# Map-Operation (manuelle Transformation)\n",
    "rdd_mapped = rdd.map(lambda x: (x[0], x[1] * 2))  # Verdopple das Alter\n",
    "print(rdd_mapped.collect()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4fcc058-741b-4744-84f8-493ad452bebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "|   Name|Doppeltes_Alter|\n",
      "+-------+---------------+\n",
      "| M√ºller|             70|\n",
      "|Schmidt|             56|\n",
      "+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL-√§hnliche Operation\n",
    "df_transformed = df.select(\"Name\", (df.Alter * 2).alias(\"Doppeltes_Alter\"))\n",
    "df_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d513907b-b574-4bb5-97bc-c91b8278ad2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63, 2)\n",
      "Durchschnittsalter: 31.5\n"
     ]
    }
   ],
   "source": [
    "# Durchschnittsalter in einem Schritt berechnen\n",
    "durchschnittsalter = rdd.map(lambda x: (x[1], 1)) \\\n",
    "                       .reduce(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "\n",
    "print(durchschnittsalter)\n",
    "durchschnittsalter = durchschnittsalter[0] / durchschnittsalter[1]\n",
    "print(\"Durchschnittsalter:\", durchschnittsalter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879380b-bad4-4756-b64e-c7b2c0923782",
   "metadata": {},
   "source": [
    "Was passiert Schritt f√ºr Schritt:\n",
    "\n",
    "    1. Vorbereitung:\n",
    "\n",
    "        - Ihr RDD enth√§lt nach der map-Operation Paare von (Alter, 1)\n",
    "\n",
    "        - Beispiel: [(35, 1), (28, 1)]\n",
    "\n",
    "    2. Erster Reduzierungsschritt:\n",
    "\n",
    "        - a = (35, 1), b = (28, 1)\n",
    "\n",
    "        - Die Lambda-Funktion berechnet:\n",
    "\n",
    "            a[0] + b[0] ‚Üí 35 + 28 ‚Üí 63 (Summe der Alter)\n",
    "\n",
    "            a[1] + b[1] ‚Üí 1 + 1 ‚Üí 2 (Anzahl der Personen)\n",
    "\n",
    "        - Ergebnis: (63, 2)\n",
    "\n",
    "    3. Endergebnis:\n",
    "\n",
    "        - Da nur zwei Elemente im RDD sind, ist dies das finale Ergebnis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0a1312f-65cf-4b0a-a555-ea33e3c56d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|Durchschnittsalter|\n",
      "+------------------+\n",
      "|              31.5|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.agg(F.avg(\"Alter\").alias(\"Durchschnittsalter\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0d808f-55a4-43dd-9429-4bb9c93ec85a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb38a4a7-7960-4a89-b08a-76e14430c754",
   "metadata": {},
   "source": [
    "## üîÑ 6. Lebenszyklus eines DataFrames in Spark\n",
    "\n",
    "| Phase                  | Was passiert?                                                                                                  | Warum es wichtig ist                                         |\n",
    "| ---------------------- | -------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------ |\n",
    "| **1 Transformation**   | Du **schreibst** Befehle (`select`, `filter`, `join`). Spark merkt sie sich ‚Äì aber rechnet noch **gar nicht**. | Kein Warten, du kannst erst mal ‚ÄúPipeline basteln‚Äù.          |\n",
    "| **2 Logischer Plan**   | Spark malt sich eine **To-do-Liste**: ‚ÄûWelche Spalten brauche ich? Welche Tabelle wird womit verkn√ºpft?‚Äú       | Tippfehler wie ‚ÄûSpalte existiert nicht‚Äú fliegen hier auf.    |\n",
    "| **3 Optimierter Plan** | Spark **sortiert** die To-do-Liste, damit es schneller geht (z. B. zuerst filtern ‚Üí dann join).                | Der Optimierer (Catalyst-Optimierer + Adaptive Query Execution (AQE)): gut sortiert = viel weniger Daten bewegen.        |\n",
    "| **4 Physischer Plan**  | Spark sagt jetzt: ‚ÄûSo verteile ich die Arbeit auf die Kerne / Rechner ‚Äì los geht‚Äôs!‚Äú                           | Hier entstehen Kosten wie Shuffle oder Sortieren.            |\n",
    "| **5 Action**           | Ein Befehl wie `show()`, `count()` oder `write()` **startet** die Ausf√ºhrung.                                  | Erst jetzt siehst du Tasks & Stages im Spark-UI (Port 4040). |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed931f2e-2492-47db-924b-358926fd3edb",
   "metadata": {},
   "source": [
    "### Job vs Stage vs Task"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08b7d6db-8cea-4ded-a6d5-e70532f1f682",
   "metadata": {},
   "source": [
    "| Ebene     | Was ist das? (auf den Punkt)                                                                                                                              | Wann entsteht sie?                                                                                   | Parallelit√§t                                  |\n",
    "| --------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- | --------------------------------------------- |\n",
    "| **Job**   | **Ein Action-Aufruf** wie `show()`, `count()`, `write()` startet einen Job.                                                                               | Immer genau dann, wenn du so eine Action ausf√ºhrst.                                                  | Besteht aus mehreren Stages.                  |\n",
    "| **Stage** | **Ein Arbeitsschritt, den Spark ohne neues Verteilen der Daten durchrechnen kann.**<br>(√úber das ‚ÄûDaten-Verteilen‚Äú ‚Äì Shuffle ‚Äì sprechen wir gleich noch.) | Spark setzt eine neue Stage, sobald Daten neu sortiert / √ºber das Netzwerk geschickt werden m√ºssten. | Viele Tasks laufen parallel in einer Stage.   |\n",
    "| **Task**  | **Die kleinste Aufgabe:** ‚ÄûVerarbeite Partition X‚Äú                                                                                                        | Spark erzeugt 1 Task pro Partition in der Stage.                                                     | 1 Task benutzt 1 CPU-Kern auf einem Executor. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29e2181f-e207-41a5-8361-76a52eec7553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [avg(salary)#114 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(avg(salary)#114 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=334]\n",
      "      +- HashAggregate(keys=[department#105], functions=[avg(salary#106L)])\n",
      "         +- HashAggregate(keys=[department#105], functions=[partial_avg(salary#106L)])\n",
      "            +- Exchange hashpartitioning(department#105, 200), REPARTITION_BY_COL, [plan_id=329]\n",
      "               +- Project [department#105, salary#106L]\n",
      "                  +- Scan ExistingRDD[name#104,department#105,salary#106L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1Ô∏è‚É£ Session starten (lokal mit 4 Kernen = 4 parallele Tasks pro Stage)\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[4]\")\n",
    "    .appName(\"jobs-stages-tasks-demo\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 2Ô∏è‚É£ Mini-DataFrame im Code erzeugen\n",
    "data = [\n",
    "    (\"Alice\", \"Engineering\", 65000),\n",
    "    (\"Bob\",   \"Marketing\",  58000),\n",
    "    (\"Carol\", \"Engineering\", 72000),\n",
    "    (\"Dave\",  \"Marketing\",  55000),\n",
    "    (\"Eve\",   \"Engineering\", 90000),\n",
    "]\n",
    "cols = [\"name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data, cols)\n",
    "\n",
    "# 3Ô∏è‚É£ Spark zwingen, mindestens einen Shuffle zu bauen\n",
    "#spark.conf.set(\"spark.sql.shuffle.partitions\", 4)  # ‚Üí 4 Tasks in der Reduce-Stage\n",
    "\n",
    "result = (\n",
    "    df.repartition(\"department\")            # l√∂st einen Shuffle aus (Stage 0)\n",
    "      .groupBy(\"department\")                # Reduce-Seite (Stage 1)\n",
    "      .avg(\"salary\")\n",
    "      .orderBy(\"avg(salary)\", ascending=False)\n",
    ")\n",
    "\n",
    "# 4Ô∏è‚É£ Planschilderung anzeigen (Job noch nicht gestartet)\n",
    "result.explain(\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07698204-dece-4752-9c30-470bc3787647",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "| department|      avg(salary)|\n",
      "+-----------+-----------------+\n",
      "|Engineering|75666.66666666667|\n",
      "|  Marketing|          56500.0|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5Ô∏è‚É£ Action ‚Üí start Job ‚Üí √∂ffne Spark-UI unter http://localhost:4040\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d3c16d-d89a-4b35-985e-27c76104db1f",
   "metadata": {},
   "source": [
    "> **Shuffling** -> Spark muss Daten quer √ºber alle Worker ‚Äûumparken‚Äú, damit Zeilen, die zusammengeh√∂ren, auch wirklich zusammenliegen.\n",
    ">\n",
    "> Du gruppierst oder joinst nach einer Spalte ‚Äì Spark braucht dann alle Zeilen desselben Keys in derselben Partition. Daf√ºr werden Bl√∂cke durchs Netzwerk geschickt ‚Üí das nennt man Shuffle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797f89f0-98d6-49b9-9141-dace2f7e627a",
   "metadata": {},
   "source": [
    "## üìä 7. Daten erkunden und transformieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2810e5f-32bc-4ff8-9385-f0cae680b71d",
   "metadata": {},
   "source": [
    "### Dataframe erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76cfe34b-9476-45bf-8c7a-4d75186d0d32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------+------+\n",
      "|   Name|Alter|  Stadt|Gehalt|\n",
      "+-------+-----+-------+------+\n",
      "| M√ºller|   35| Berlin|  4000|\n",
      "|Schmidt|   28|M√ºnchen|  3200|\n",
      "|Fischer|   42|Hamburg|  5100|\n",
      "+-------+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DeutscheDaten\").getOrCreate()\n",
    "\n",
    "# DataFrame mit deutschen Spaltennamen erstellen\n",
    "data = [\n",
    "    (\"M√ºller\", 35, \"Berlin\", 4000),\n",
    "    (\"Schmidt\", 28, \"M√ºnchen\", 3200),\n",
    "    (\"Fischer\", 42, \"Hamburg\", 5100)\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Alter\", \"Stadt\", \"Gehalt\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a8be6ad-093e-4cb9-a349-9deac6f71d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Alter: integer (nullable = true)\n",
      " |-- Stadt: string (nullable = true)\n",
      " |-- Gehalt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Mit expliziten Datentypen\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Alter\", IntegerType(), True),\n",
    "    StructField(\"Stadt\", StringType(), True),\n",
    "    StructField(\"Gehalt\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_mit_schema = spark.createDataFrame(data, schema)\n",
    "df_mit_schema.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "14d1da24-b9bc-497c-8aa0-70c4a13eb95d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------+------+\n",
      "|   Name|Alter|  Stadt|Gehalt|\n",
      "+-------+-----+-------+------+\n",
      "| M√ºller|   35| Berlin|  4000|\n",
      "|Schmidt|   28|M√ºnchen|  3200|\n",
      "|Fischer|   42|Hamburg|  5100|\n",
      "+-------+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mit_schema.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f4910-58c8-43dc-80a7-e1d438395d88",
   "metadata": {},
   "source": [
    "### √úbung 1: Grundlegende DataFrame-Erstellung\n",
    "\n",
    "Aufgabe: Erstelle ein DataFrame mit Mitarbeiterdaten, das folgende Spalten enth√§lt:\n",
    "\n",
    "    Vorname (z.B. \"Anna\", \"Thomas\")\n",
    "    Abteilung (z.B. \"IT\", \"Vertrieb\")\n",
    "    Eintrittsjahr (z.B. 2015, 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d38b04db-c0fe-4a67-9f10-d815d83b1f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------------+\n",
      "|Vorname|Abteilung|Eintrittsjahr|\n",
      "+-------+---------+-------------+\n",
      "|   Anna|       IT|         2018|\n",
      "| Thomas| Vertrieb|         2015|\n",
      "|  Julia|       HR|         2020|\n",
      "+-------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# L√∂sung\n",
    "mitarbeiter_data = [\n",
    "    (\"Anna\", \"IT\", 2018),\n",
    "    (\"Thomas\", \"Vertrieb\", 2015),\n",
    "    (\"Julia\", \"HR\", 2020)\n",
    "]\n",
    "mitarbeiter_df = spark.createDataFrame(mitarbeiter_data, [\"Vorname\", \"Abteilung\", \"Eintrittsjahr\"])\n",
    "mitarbeiter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f91438-e013-4483-90ab-e886fb10edc4",
   "metadata": {},
   "source": [
    "### CSV Lesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "61d153b7-4ac3-4061-b843-5291d2ea2e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---+----------+--------------------+-------------------+------+----------+----------------+\n",
      "|first_name| last_name|age|      city|               email|          job_title|salary|department|years_experience|\n",
      "+----------+----------+---+----------+--------------------+-------------------+------+----------+----------------+\n",
      "|      Anna|   Mueller| 28|    Berlin|anna.mueller@gmai...|       Data Analyst| 52000|      Tech|               4|\n",
      "|       Ben|   Schmidt| 35|   Hamburg|ben.schmidt@outlo...|  Software Engineer|  NULL|      Tech|              12|\n",
      "|     Clara|     Klein| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|     Other|               1|\n",
      "|     David| Schneider| 40|   Cologne|david.schneider@c...|    Project Manager| 83000|Management|              15|\n",
      "|       Eva|     Huber| 31| Stuttgart|    eva.huber@web.de|     Data Scientist|  NULL|      Tech|               6|\n",
      "|     Felix|    Wagner| 29|    Berlin|felix.wagner@exam...|    DevOps Engineer| 67600|      Tech|               5|\n",
      "|      Gina|   Fischer| 45|   Hamburg|gina.fischer@yaho...|         HR Manager| 69000|Management|              20|\n",
      "|    Hannah|      Koch| 26| Frankfurt|hannah.koch@examp...|        UI Designer|  NULL|  Creative|               3|\n",
      "|      Igor|    Keller| 38|    Munich|  igor.keller@gmx.de|      Sales Manager| 75000|Management|              14|\n",
      "|     Julia|   Schmitt| 24|D√ºsseldorf|julia.schmitt@hot...|    Content Creator| 41000|  Creative|               2|\n",
      "|      Karl|     Bauer| 50|    Berlin|karl.bauer@exampl...|                CTO|120000|     Other|              25|\n",
      "|      Lina|     Maier| 33| Stuttgart|lina.maier@t-onli...|         Accountant|  NULL|   Finance|               8|\n",
      "|       Max|     Frank| 41|   Cologne|max.frank@example...|   Network Engineer| 71000|      Tech|              16|\n",
      "|      Nina|   Lehmann| 30|   Hamburg|nina.lehmann@gmai...|      Product Owner| 66400|     Other|               7|\n",
      "|    Oliver|     Weber| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|     Other|               6|\n",
      "|     Paula|  Hartmann| 36|    Munich|paula.hartmann@ic...|   Product Designer| 65000|  Creative|              11|\n",
      "|   Quentin|    Schulz| 43| Frankfurt|quentin.schulz@ex...|         Consultant|  NULL|Management|              18|\n",
      "|      Rita|      Lang| 25|D√ºsseldorf|rita.lang@protonm...|   Junior Developer| 47000|     Other|               2|\n",
      "|    Stefan|    Becker| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|     Other|              13|\n",
      "|      Tina|     Kraus| 32|   Cologne|  tina.kraus@mail.de|      Data Engineer|  NULL|      Tech|               8|\n",
      "|     Marco|Zimmermann| 21|    Berlin|marco.zimmermann@...|             Intern| 28000|     Other|               0|\n",
      "|      Lisa|  Hoffmann| 23|   Hamburg|lisa.hoffmann@gma...|   Customer Service| 32000|     Other|               1|\n",
      "|       Tom|   Richter| 20|    Munich|tom.richter@examp...|    Working Student| 18000|     Other|               0|\n",
      "|     Sarah|      Wolf| 24| Frankfurt| sarah.wolf@yahoo.de|          Reception| 29000|     Other|               2|\n",
      "|     Kevin|   Neumann| 22|   Cologne|kevin.neumann@gmx.de|Warehouse Assistant| 31000|     Other|               1|\n",
      "+----------+----------+---+----------+--------------------+-------------------+------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Korrekte RAW-URL\n",
    "url = \"https://raw.githubusercontent.com/gadanes/spark_kurs/main/notebooks/data.csv\"\n",
    "\n",
    "# Lade die CSV-Datei herunter\n",
    "response = requests.get(url)\n",
    "with open(\"/tmp/data.csv\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Lese die CSV-Datei mit Spark ein\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"/tmp/data.csv\")\n",
    "df.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817713d1-ed22-4581-862f-11242502f14d",
   "metadata": {},
   "source": [
    "### Spalten ausw√§hlen (`select`)\n",
    "\n",
    "Mit `.select()` kannst du gezielt bestimmte Spalten aus dem DataFrame ausw√§hlen.\n",
    "\n",
    "**Merke:**  \n",
    "> `.select()` **ver√§ndert** das urspr√ºngliche DataFrame **nicht**.  \n",
    "> Es erzeugt eine **neue** Version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af63e456-558b-4fc4-a1a0-2770f586e7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+-------------------+\n",
      "|first_name|last_name|      city|          job_title|\n",
      "+----------+---------+----------+-------------------+\n",
      "|      Anna|  Mueller|    Berlin|       Data Analyst|\n",
      "|       Ben|  Schmidt|   Hamburg|  Software Engineer|\n",
      "|     Clara|    Klein|    Munich|Marketing Assistant|\n",
      "|     David|Schneider|   Cologne|    Project Manager|\n",
      "|       Eva|    Huber| Stuttgart|     Data Scientist|\n",
      "|     Felix|   Wagner|    Berlin|    DevOps Engineer|\n",
      "|      Gina|  Fischer|   Hamburg|         HR Manager|\n",
      "|    Hannah|     Koch| Frankfurt|        UI Designer|\n",
      "|      Igor|   Keller|    Munich|      Sales Manager|\n",
      "|     Julia|  Schmitt|D√ºsseldorf|    Content Creator|\n",
      "|      Karl|    Bauer|    Berlin|                CTO|\n",
      "|      Lina|    Maier| Stuttgart|         Accountant|\n",
      "|       Max|    Frank|   Cologne|   Network Engineer|\n",
      "|      Nina|  Lehmann|   Hamburg|      Product Owner|\n",
      "|    Oliver|    Weber|    Berlin|  Backend Developer|\n",
      "|     Paula| Hartmann|    Munich|   Product Designer|\n",
      "|   Quentin|   Schulz| Frankfurt|         Consultant|\n",
      "|      Rita|     Lang|D√ºsseldorf|   Junior Developer|\n",
      "|    Stefan|   Becker| Stuttgart|         IT Support|\n",
      "|      Tina|    Kraus|   Cologne|      Data Engineer|\n",
      "+----------+---------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"first_name\", \"last_name\", \"city\", \"job_title\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e28c33e5-15d2-4f85-b389-04bb6c2dd97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "68577022-cddf-4a01-8017-89396f2812b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+----------+-------------------+\n",
      "|concat(first_name,  , last_name)|      city|          job_title|\n",
      "+--------------------------------+----------+-------------------+\n",
      "|                    Anna Mueller|    Berlin|       Data Analyst|\n",
      "|                     Ben Schmidt|   Hamburg|  Software Engineer|\n",
      "|                     Clara Klein|    Munich|Marketing Assistant|\n",
      "|                 David Schneider|   Cologne|    Project Manager|\n",
      "|                       Eva Huber| Stuttgart|     Data Scientist|\n",
      "|                    Felix Wagner|    Berlin|    DevOps Engineer|\n",
      "|                    Gina Fischer|   Hamburg|         HR Manager|\n",
      "|                     Hannah Koch| Frankfurt|        UI Designer|\n",
      "|                     Igor Keller|    Munich|      Sales Manager|\n",
      "|                   Julia Schmitt|D√ºsseldorf|    Content Creator|\n",
      "|                      Karl Bauer|    Berlin|                CTO|\n",
      "|                      Lina Maier| Stuttgart|         Accountant|\n",
      "|                       Max Frank|   Cologne|   Network Engineer|\n",
      "|                    Nina Lehmann|   Hamburg|      Product Owner|\n",
      "|                    Oliver Weber|    Berlin|  Backend Developer|\n",
      "|                  Paula Hartmann|    Munich|   Product Designer|\n",
      "|                  Quentin Schulz| Frankfurt|         Consultant|\n",
      "|                       Rita Lang|D√ºsseldorf|   Junior Developer|\n",
      "|                   Stefan Becker| Stuttgart|         IT Support|\n",
      "|                      Tina Kraus|   Cologne|      Data Engineer|\n",
      "+--------------------------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.concat(F.col(\"first_name\") , F.lit(\" \") , F.col(\"last_name\")), \"city\", \"job_title\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "80d40a12-5b40-4522-b53f-dbae74059978",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+----------+-------------------+\n",
      "|concat(first_name,  , last_name)|      city|          job_title|\n",
      "+--------------------------------+----------+-------------------+\n",
      "|                    Anna Mueller|    Berlin|       Data Analyst|\n",
      "|                     Ben Schmidt|   Hamburg|  Software Engineer|\n",
      "|                     Clara Klein|    Munich|Marketing Assistant|\n",
      "|                 David Schneider|   Cologne|    Project Manager|\n",
      "|                       Eva Huber| Stuttgart|     Data Scientist|\n",
      "|                    Felix Wagner|    Berlin|    DevOps Engineer|\n",
      "|                    Gina Fischer|   Hamburg|         HR Manager|\n",
      "|                     Hannah Koch| Frankfurt|        UI Designer|\n",
      "|                     Igor Keller|    Munich|      Sales Manager|\n",
      "|                   Julia Schmitt|D√ºsseldorf|    Content Creator|\n",
      "|                      Karl Bauer|    Berlin|                CTO|\n",
      "|                      Lina Maier| Stuttgart|         Accountant|\n",
      "|                       Max Frank|   Cologne|   Network Engineer|\n",
      "|                    Nina Lehmann|   Hamburg|      Product Owner|\n",
      "|                    Oliver Weber|    Berlin|  Backend Developer|\n",
      "|                  Paula Hartmann|    Munich|   Product Designer|\n",
      "|                  Quentin Schulz| Frankfurt|         Consultant|\n",
      "|                       Rita Lang|D√ºsseldorf|   Junior Developer|\n",
      "|                   Stefan Becker| Stuttgart|         IT Support|\n",
      "|                      Tina Kraus|   Cologne|      Data Engineer|\n",
      "+--------------------------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.concat(\"first_name\", lit(\" \") , \"last_name\"), \"city\", \"job_title\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "90e02b71-4bb4-4f04-9c11-9d9eae066470",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+-------------------+\n",
      "|      full_name|      city|          job_title|\n",
      "+---------------+----------+-------------------+\n",
      "|   Anna Mueller|    Berlin|       Data Analyst|\n",
      "|    Ben Schmidt|   Hamburg|  Software Engineer|\n",
      "|    Clara Klein|    Munich|Marketing Assistant|\n",
      "|David Schneider|   Cologne|    Project Manager|\n",
      "|      Eva Huber| Stuttgart|     Data Scientist|\n",
      "|   Felix Wagner|    Berlin|    DevOps Engineer|\n",
      "|   Gina Fischer|   Hamburg|         HR Manager|\n",
      "|    Hannah Koch| Frankfurt|        UI Designer|\n",
      "|    Igor Keller|    Munich|      Sales Manager|\n",
      "|  Julia Schmitt|D√ºsseldorf|    Content Creator|\n",
      "|     Karl Bauer|    Berlin|                CTO|\n",
      "|     Lina Maier| Stuttgart|         Accountant|\n",
      "|      Max Frank|   Cologne|   Network Engineer|\n",
      "|   Nina Lehmann|   Hamburg|      Product Owner|\n",
      "|   Oliver Weber|    Berlin|  Backend Developer|\n",
      "| Paula Hartmann|    Munich|   Product Designer|\n",
      "| Quentin Schulz| Frankfurt|         Consultant|\n",
      "|      Rita Lang|D√ºsseldorf|   Junior Developer|\n",
      "|  Stefan Becker| Stuttgart|         IT Support|\n",
      "|     Tina Kraus|   Cologne|      Data Engineer|\n",
      "+---------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.concat_ws(\" \", \"first_name\", \"last_name\").alias(\"full_name\"), \"city\", \"job_title\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47cc3a2-52ca-4019-91ff-bf8de011b2d1",
   "metadata": {},
   "source": [
    "### √úbung 2: Grundlegende DataFrame-Erstellung\n",
    "\n",
    "Aufgabe: Zeige vollst√§ndigen Namen, Berufsbezeichnung und Monatsgehalt an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "54e7a6c4-fa89-4864-a6c7-fa749b9e91de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+------------+\n",
      "|      full_name|          job_title|monatsgehalt|\n",
      "+---------------+-------------------+------------+\n",
      "|   Anna Mueller|       Data Analyst|     4333.33|\n",
      "|    Ben Schmidt|  Software Engineer|        NULL|\n",
      "|    Clara Klein|Marketing Assistant|      3500.0|\n",
      "|David Schneider|    Project Manager|     6916.67|\n",
      "|      Eva Huber|     Data Scientist|        NULL|\n",
      "|   Felix Wagner|    DevOps Engineer|     5633.33|\n",
      "|   Gina Fischer|         HR Manager|      5750.0|\n",
      "|    Hannah Koch|        UI Designer|        NULL|\n",
      "|    Igor Keller|      Sales Manager|      6250.0|\n",
      "|  Julia Schmitt|    Content Creator|     3416.67|\n",
      "|     Karl Bauer|                CTO|     10000.0|\n",
      "|     Lina Maier|         Accountant|        NULL|\n",
      "|      Max Frank|   Network Engineer|     5916.67|\n",
      "|   Nina Lehmann|      Product Owner|     5533.33|\n",
      "|   Oliver Weber|  Backend Developer|     5583.33|\n",
      "| Paula Hartmann|   Product Designer|     5416.67|\n",
      "| Quentin Schulz|         Consultant|        NULL|\n",
      "|      Rita Lang|   Junior Developer|     3916.67|\n",
      "|  Stefan Becker|         IT Support|     4666.67|\n",
      "|     Tina Kraus|      Data Engineer|        NULL|\n",
      "+---------------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zeige vollst√§ndigen Namen, Berufsbezeichnung und Monatsgehalt an\n",
    "df.select(\n",
    "    F.concat_ws(\" \", \"first_name\", \"last_name\").alias(\"full_name\"),\n",
    "    \"job_title\",\n",
    "    F.round(F.col(\"salary\") / 12, 2).alias(\"monatsgehalt\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf1b4e3-ba14-4f38-95e3-8a855a1da02b",
   "metadata": {},
   "source": [
    "### Zeilen filtern (`filter`, `where`)\n",
    "\n",
    "Mit `.filter()` oder `.where()` kannst du Zeilen nach Bedingungen ausw√§hlen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0d5a474d-0d16-4a79-8f59-1e924e8223f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+---------+--------------------+-----------------+------+----------+----------------+\n",
      "|first_name|last_name|age|     city|               email|        job_title|salary|department|years_experience|\n",
      "+----------+---------+---+---------+--------------------+-----------------+------+----------+----------------+\n",
      "|       Ben|  Schmidt| 35|  Hamburg|ben.schmidt@outlo...|Software Engineer|  NULL|      Tech|              12|\n",
      "|     David|Schneider| 40|  Cologne|david.schneider@c...|  Project Manager| 83000|Management|              15|\n",
      "|       Eva|    Huber| 31|Stuttgart|    eva.huber@web.de|   Data Scientist|  NULL|      Tech|               6|\n",
      "|      Gina|  Fischer| 45|  Hamburg|gina.fischer@yaho...|       HR Manager| 69000|Management|              20|\n",
      "|      Igor|   Keller| 38|   Munich|  igor.keller@gmx.de|    Sales Manager| 75000|Management|              14|\n",
      "|      Karl|    Bauer| 50|   Berlin|karl.bauer@exampl...|              CTO|120000|     Other|              25|\n",
      "|      Lina|    Maier| 33|Stuttgart|lina.maier@t-onli...|       Accountant|  NULL|   Finance|               8|\n",
      "|       Max|    Frank| 41|  Cologne|max.frank@example...| Network Engineer| 71000|      Tech|              16|\n",
      "|      Nina|  Lehmann| 30|  Hamburg|nina.lehmann@gmai...|    Product Owner| 66400|     Other|               7|\n",
      "|     Paula| Hartmann| 36|   Munich|paula.hartmann@ic...| Product Designer| 65000|  Creative|              11|\n",
      "|   Quentin|   Schulz| 43|Frankfurt|quentin.schulz@ex...|       Consultant|  NULL|Management|              18|\n",
      "|    Stefan|   Becker| 39|Stuttgart|stefan.becker@exa...|       IT Support| 56000|     Other|              13|\n",
      "|      Tina|    Kraus| 32|  Cologne|  tina.kraus@mail.de|    Data Engineer|  NULL|      Tech|               8|\n",
      "+----------+---------+---+---------+--------------------+-----------------+------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.age >= 30).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "38213a20-b01d-4c6b-ba69-0f6f37cebab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+-------+--------------------+---------------+------+----------+----------------+\n",
      "|first_name|last_name|age|   city|               email|      job_title|salary|department|years_experience|\n",
      "+----------+---------+---+-------+--------------------+---------------+------+----------+----------------+\n",
      "|     David|Schneider| 40|Cologne|david.schneider@c...|Project Manager| 83000|Management|              15|\n",
      "|      Karl|    Bauer| 50| Berlin|karl.bauer@exampl...|            CTO|120000|     Other|              25|\n",
      "+----------+---------+---+-------+--------------------+---------------+------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.salary > 80000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3d50acc1-d00d-453f-b7ae-90a0ca015ff3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+---------+--------------------+-----------------+------+----------+----------------+\n",
      "|first_name|last_name|age|     city|               email|        job_title|salary|department|years_experience|\n",
      "+----------+---------+---+---------+--------------------+-----------------+------+----------+----------------+\n",
      "|       Ben|  Schmidt| 35|  Hamburg|ben.schmidt@outlo...|Software Engineer|  NULL|      Tech|              12|\n",
      "|     David|Schneider| 40|  Cologne|david.schneider@c...|  Project Manager| 83000|Management|              15|\n",
      "|       Eva|    Huber| 31|Stuttgart|    eva.huber@web.de|   Data Scientist|  NULL|      Tech|               6|\n",
      "|      Gina|  Fischer| 45|  Hamburg|gina.fischer@yaho...|       HR Manager| 69000|Management|              20|\n",
      "|      Igor|   Keller| 38|   Munich|  igor.keller@gmx.de|    Sales Manager| 75000|Management|              14|\n",
      "|      Karl|    Bauer| 50|   Berlin|karl.bauer@exampl...|              CTO|120000|     Other|              25|\n",
      "|      Lina|    Maier| 33|Stuttgart|lina.maier@t-onli...|       Accountant|  NULL|   Finance|               8|\n",
      "|       Max|    Frank| 41|  Cologne|max.frank@example...| Network Engineer| 71000|      Tech|              16|\n",
      "|     Paula| Hartmann| 36|   Munich|paula.hartmann@ic...| Product Designer| 65000|  Creative|              11|\n",
      "|   Quentin|   Schulz| 43|Frankfurt|quentin.schulz@ex...|       Consultant|  NULL|Management|              18|\n",
      "|    Stefan|   Becker| 39|Stuttgart|stefan.becker@exa...|       IT Support| 56000|     Other|              13|\n",
      "|      Tina|    Kraus| 32|  Cologne|  tina.kraus@mail.de|    Data Engineer|  NULL|      Tech|               8|\n",
      "+----------+---------+---+---------+--------------------+-----------------+------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.age > 30).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7064104c-a192-43c5-a4ec-be71c50ef411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+\n",
      "|first_name|last_name|age|\n",
      "+----------+---------+---+\n",
      "|       Ben|  Schmidt| 35|\n",
      "|     David|Schneider| 40|\n",
      "|       Eva|    Huber| 31|\n",
      "|      Gina|  Fischer| 45|\n",
      "|      Igor|   Keller| 38|\n",
      "|      Karl|    Bauer| 50|\n",
      "|      Lina|    Maier| 33|\n",
      "|       Max|    Frank| 41|\n",
      "|     Paula| Hartmann| 36|\n",
      "|   Quentin|   Schulz| 43|\n",
      "|    Stefan|   Becker| 39|\n",
      "|      Tina|    Kraus| 32|\n",
      "+----------+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.age > 30).select(\"first_name\", \"last_name\", \"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09904709-5ee5-4fe2-aefd-5ec1a7349d02",
   "metadata": {},
   "source": [
    "**Hinweis:**  \n",
    "> **`filter`** und **`where`** sind **identisch** ‚Äì es ist reine Geschmackssache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9d3aa4-da27-4056-8b76-7a2057514d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f55b893-2bb5-4e06-a16c-8c689c9615ae",
   "metadata": {},
   "source": [
    "### √úbung 3: selektieren\n",
    "1. Welche Mitarbeiter sind 30 Jahre oder √§lter und arbeiten in Tech Department?\n",
    "2. Zeige alle Mitarbeiter aus Berlin oder M√ºnchen mit ihrem Vor- und Nachnamen.\n",
    "3. Welche Mitarbeiter haben kein Gehalt (NULL)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "96adea1b-564a-400b-b8f5-7d5939cdc339",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+-----------------+\n",
      "|first_name|last_name|age|        job_title|\n",
      "+----------+---------+---+-----------------+\n",
      "|       Ben|  Schmidt| 35|Software Engineer|\n",
      "|       Eva|    Huber| 31|   Data Scientist|\n",
      "|       Max|    Frank| 41| Network Engineer|\n",
      "|      Tina|    Kraus| 32|    Data Engineer|\n",
      "+----------+---------+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "df.filter(\n",
    "    (F.col(\"age\") >= 30) & \n",
    "    (F.col(\"department\") == \"Tech\")\n",
    ").select(\"first_name\", \"last_name\", \"age\", \"job_title\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1809fbf8-5432-4ecb-97d5-a38fb7e7b718",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(F.col(\"city\").)\n",
    "\n",
    "df.with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "102dbeb5-a86d-4279-938b-e2eac6838355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+\n",
      "|first_name|last_name|  city|\n",
      "+----------+---------+------+\n",
      "|      Anna|  Mueller|Berlin|\n",
      "|     Clara|    Klein|Munich|\n",
      "|     Felix|   Wagner|Berlin|\n",
      "|      Igor|   Keller|Munich|\n",
      "|      Karl|    Bauer|Berlin|\n",
      "|    Oliver|    Weber|Berlin|\n",
      "|     Paula| Hartmann|Munich|\n",
      "+----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "df.filter(\n",
    "    (F.col(\"city\") == \"Berlin\") | (F.col(\"city\") == \"Munich\")\n",
    ").select(\"first_name\", \"last_name\", \"city\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "971b0785-0a42-4684-b14c-5719530f65ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+\n",
      "|first_name|last_name|  city|\n",
      "+----------+---------+------+\n",
      "|      Anna|  Mueller|Berlin|\n",
      "|     Clara|    Klein|Munich|\n",
      "|     Felix|   Wagner|Berlin|\n",
      "|      Igor|   Keller|Munich|\n",
      "|      Karl|    Bauer|Berlin|\n",
      "|    Oliver|    Weber|Berlin|\n",
      "|     Paula| Hartmann|Munich|\n",
      "+----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "df.filter(\n",
    "    F.col(\"city\").isin([\"Berlin\", \"Munich\"])\n",
    ").select(\"first_name\", \"last_name\", \"city\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c6d712e1-a46e-4b26-9ec6-430c76bb3876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------------+\n",
      "|first_name|last_name|        job_title|\n",
      "+----------+---------+-----------------+\n",
      "|       Ben|  Schmidt|Software Engineer|\n",
      "|       Eva|    Huber|   Data Scientist|\n",
      "|    Hannah|     Koch|      UI Designer|\n",
      "|      Lina|    Maier|       Accountant|\n",
      "|   Quentin|   Schulz|       Consultant|\n",
      "|      Tina|    Kraus|    Data Engineer|\n",
      "+----------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "df.filter(\n",
    "    F.col(\"salary\").isNull()\n",
    ").select(\"first_name\", \"last_name\", \"job_title\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e9e0ac-b6a5-4265-92a6-ec34dddd9da7",
   "metadata": {},
   "source": [
    "### withColumn():\n",
    "    - Eine neue Spalte zu einem DataFrame hinzuf√ºgen\n",
    "    ODER\n",
    "    - Eine bestehende Spalte modifizieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0259e2-e095-461e-ad9a-a8e85242e731",
   "metadata": {},
   "source": [
    "### Neue Spalten erstellen (`withColumn`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c7e63c4-d344-473b-86d2-2f63c34be827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+--------------------+-------------------+------+----------+\n",
      "|   name|age|      city|               email|          job_title|salary|age_plus_5|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+\n",
      "|   Anna| 28|    Berlin|anna.mueller@exam...|       Data Analyst| 52000|      33.0|\n",
      "|    Ben| 35|   Hamburg|ben.schmidt@examp...|  Software Engineer| 74000|      40.0|\n",
      "|  Clara| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|      27.0|\n",
      "|  David| 40|   Cologne|david.schneider@e...|    Project Manager| 83000|      45.0|\n",
      "|    Eva| 31| Stuttgart|eva.huber@example...|     Data Scientist| 68000|      36.0|\n",
      "|  Felix| 29|    Berlin|felix.wagner@exam...|    DevOps Engineer|  NULL|      34.0|\n",
      "|   Gina| 45|   Hamburg|gina.fischer@exam...|         HR Manager| 69000|      50.0|\n",
      "| Hannah| 26| Frankfurt|hannah.koch@examp...|        UI Designer| 54000|      31.0|\n",
      "|   Igor| 38|    Munich|igor.keller@examp...|      Sales Manager| 75000|      43.0|\n",
      "|  Julia| 24|D√ºsseldorf|julia.schmitt@exa...|    Content Creator| 41000|      29.0|\n",
      "|   Karl| 50|    Berlin|karl.bauer@exampl...|                CTO|120000|      55.0|\n",
      "|   Lina| 33| Stuttgart|lina.maier@exampl...|         Accountant| 58000|      38.0|\n",
      "|    Max| 41|   Cologne|max.frank@example...|   Network Engineer| 71000|      46.0|\n",
      "|   Nina| 30|   Hamburg|nina.lehmann@exam...|      Product Owner|  NULL|      35.0|\n",
      "| Oliver| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|      32.0|\n",
      "|  Paula| 36|    Munich|paula.hartmann@ex...|   Product Designer| 65000|      41.0|\n",
      "|Quentin| 43| Frankfurt|quentin.schulz@ex...|         Consultant| 80000|      48.0|\n",
      "|   Rita| 25|D√ºsseldorf|rita.lang@example...|   Junior Developer| 47000|      30.0|\n",
      "| Stefan| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|      44.0|\n",
      "|   Tina| 32|   Cologne|tina.kraus@exampl...|      Data Engineer| 73000|      37.0|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"age_plus_5\", col(\"age\") + 5)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4a077b12-286b-4793-ac38-c49d11b90dd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+----------+--------------------+-------------------+------+----------+----------------+-------+\n",
      "|first_name|last_name|age|      city|               email|          job_title|salary|department|years_experience|country|\n",
      "+----------+---------+---+----------+--------------------+-------------------+------+----------+----------------+-------+\n",
      "|      Anna|  Mueller| 28|    Berlin|anna.mueller@gmai...|       Data Analyst| 52000|      Tech|               4|Germany|\n",
      "|       Ben|  Schmidt| 35|   Hamburg|ben.schmidt@outlo...|  Software Engineer|  NULL|      Tech|              12|Germany|\n",
      "|     Clara|    Klein| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|     Other|               1|Germany|\n",
      "|     David|Schneider| 40|   Cologne|david.schneider@c...|    Project Manager| 83000|Management|              15|Germany|\n",
      "|       Eva|    Huber| 31| Stuttgart|    eva.huber@web.de|     Data Scientist|  NULL|      Tech|               6|Germany|\n",
      "|     Felix|   Wagner| 29|    Berlin|felix.wagner@exam...|    DevOps Engineer| 67600|      Tech|               5|Germany|\n",
      "|      Gina|  Fischer| 45|   Hamburg|gina.fischer@yaho...|         HR Manager| 69000|Management|              20|Germany|\n",
      "|    Hannah|     Koch| 26| Frankfurt|hannah.koch@examp...|        UI Designer|  NULL|  Creative|               3|Germany|\n",
      "|      Igor|   Keller| 38|    Munich|  igor.keller@gmx.de|      Sales Manager| 75000|Management|              14|Germany|\n",
      "|     Julia|  Schmitt| 24|D√ºsseldorf|julia.schmitt@hot...|    Content Creator| 41000|  Creative|               2|Germany|\n",
      "|      Karl|    Bauer| 50|    Berlin|karl.bauer@exampl...|                CTO|120000|     Other|              25|Germany|\n",
      "|      Lina|    Maier| 33| Stuttgart|lina.maier@t-onli...|         Accountant|  NULL|   Finance|               8|Germany|\n",
      "|       Max|    Frank| 41|   Cologne|max.frank@example...|   Network Engineer| 71000|      Tech|              16|Germany|\n",
      "|      Nina|  Lehmann| 30|   Hamburg|nina.lehmann@gmai...|      Product Owner| 66400|     Other|               7|Germany|\n",
      "|    Oliver|    Weber| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|     Other|               4|Germany|\n",
      "|     Paula| Hartmann| 36|    Munich|paula.hartmann@ic...|   Product Designer| 65000|  Creative|              11|Germany|\n",
      "|   Quentin|   Schulz| 43| Frankfurt|quentin.schulz@ex...|         Consultant|  NULL|Management|              18|Germany|\n",
      "|      Rita|     Lang| 25|D√ºsseldorf|rita.lang@protonm...|   Junior Developer| 47000|     Other|               2|Germany|\n",
      "|    Stefan|   Becker| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|     Other|              13|Germany|\n",
      "|      Tina|    Kraus| 32|   Cologne|  tina.kraus@mail.de|      Data Engineer|  NULL|      Tech|               8|Germany|\n",
      "+----------+---------+---+----------+--------------------+-------------------+------+----------+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mit_land = df.withColumn(\"country\", F.lit(\"Germany\"))\n",
    "df_mit_land.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80911eda-3969-4116-b31b-c4716628e7a8",
   "metadata": {},
   "source": [
    "### √úbung 3: Column hinzuf√ºgen\n",
    "1. Geburtsjahrspalte hinzuf√ºgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a519ebd1-14b9-41a2-9205-7f766dfaa01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+----------+--------------------+-------------------+------+----------+----------------+-----------+\n",
      "|first_name|last_name|age|      city|               email|          job_title|salary|department|years_experience|Geburtsjahr|\n",
      "+----------+---------+---+----------+--------------------+-------------------+------+----------+----------------+-----------+\n",
      "|      Anna|  Mueller| 28|    Berlin|anna.mueller@gmai...|       Data Analyst| 52000|      Tech|               4|       1997|\n",
      "|       Ben|  Schmidt| 35|   Hamburg|ben.schmidt@outlo...|  Software Engineer|  NULL|      Tech|              12|       1990|\n",
      "|     Clara|    Klein| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|     Other|               1|       2003|\n",
      "|     David|Schneider| 40|   Cologne|david.schneider@c...|    Project Manager| 83000|Management|              15|       1985|\n",
      "|       Eva|    Huber| 31| Stuttgart|    eva.huber@web.de|     Data Scientist|  NULL|      Tech|               6|       1994|\n",
      "|     Felix|   Wagner| 29|    Berlin|felix.wagner@exam...|    DevOps Engineer| 67600|      Tech|               5|       1996|\n",
      "|      Gina|  Fischer| 45|   Hamburg|gina.fischer@yaho...|         HR Manager| 69000|Management|              20|       1980|\n",
      "|    Hannah|     Koch| 26| Frankfurt|hannah.koch@examp...|        UI Designer|  NULL|  Creative|               3|       1999|\n",
      "|      Igor|   Keller| 38|    Munich|  igor.keller@gmx.de|      Sales Manager| 75000|Management|              14|       1987|\n",
      "|     Julia|  Schmitt| 24|D√ºsseldorf|julia.schmitt@hot...|    Content Creator| 41000|  Creative|               2|       2001|\n",
      "|      Karl|    Bauer| 50|    Berlin|karl.bauer@exampl...|                CTO|120000|     Other|              25|       1975|\n",
      "|      Lina|    Maier| 33| Stuttgart|lina.maier@t-onli...|         Accountant|  NULL|   Finance|               8|       1992|\n",
      "|       Max|    Frank| 41|   Cologne|max.frank@example...|   Network Engineer| 71000|      Tech|              16|       1984|\n",
      "|      Nina|  Lehmann| 30|   Hamburg|nina.lehmann@gmai...|      Product Owner| 66400|     Other|               7|       1995|\n",
      "|    Oliver|    Weber| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|     Other|               4|       1998|\n",
      "|     Paula| Hartmann| 36|    Munich|paula.hartmann@ic...|   Product Designer| 65000|  Creative|              11|       1989|\n",
      "|   Quentin|   Schulz| 43| Frankfurt|quentin.schulz@ex...|         Consultant|  NULL|Management|              18|       1982|\n",
      "|      Rita|     Lang| 25|D√ºsseldorf|rita.lang@protonm...|   Junior Developer| 47000|     Other|               2|       2000|\n",
      "|    Stefan|   Becker| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|     Other|              13|       1986|\n",
      "|      Tina|    Kraus| 32|   Cologne|  tina.kraus@mail.de|      Data Engineer|  NULL|      Tech|               8|       1993|\n",
      "+----------+---------+---+----------+--------------------+-------------------+------+----------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# F√ºge \"Geburtsjahr\" hinzu\n",
    "df_mit_geburtsjahr = df.withColumn(\"Geburtsjahr\", (F.lit(2025) - F.col(\"age\")).cast(\"integer\"))\n",
    "df_mit_geburtsjahr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0529f6cd-049d-49ac-b7c6-5cfbcd3bc5a5",
   "metadata": {},
   "source": [
    "**Wichtig:**  \n",
    "> Jede `.withColumn()`-Operation erstellt **intern ein neues DataFrame** ‚Äî Spark ver√§ndert nie das Originalobjekt direkt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d5ce0-73c3-41f4-894f-649d02c3ccd1",
   "metadata": {},
   "source": [
    "### Bedingte Logik (`when`, `otherwise`)\n",
    "\n",
    "Mit `when` und `otherwise` kannst du **Bedingungen** einbauen, √§hnlich wie `if-else`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "81e150e6-f8cf-4d1f-a02f-3320d4626eb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---+----------+--------------------+-------------------+------+----------+----------------+---------------+\n",
      "|first_name| last_name|age|      city|               email|          job_title|salary|department|years_experience|income_category|\n",
      "+----------+----------+---+----------+--------------------+-------------------+------+----------+----------------+---------------+\n",
      "|      Anna|   Mueller| 28|    Berlin|anna.mueller@gmai...|       Data Analyst| 52000|      Tech|               4|Normalverdiener|\n",
      "|       Ben|   Schmidt| 35|   Hamburg|ben.schmidt@outlo...|  Software Engineer|  NULL|      Tech|              12|Geringverdiener|\n",
      "|     Clara|     Klein| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|     Other|               1|Normalverdiener|\n",
      "|     David| Schneider| 40|   Cologne|david.schneider@c...|    Project Manager| 83000|Management|              15|  Hochverdiener|\n",
      "|       Eva|     Huber| 31| Stuttgart|    eva.huber@web.de|     Data Scientist|  NULL|      Tech|               6|Geringverdiener|\n",
      "|     Felix|    Wagner| 29|    Berlin|felix.wagner@exam...|    DevOps Engineer| 67600|      Tech|               5|Normalverdiener|\n",
      "|      Gina|   Fischer| 45|   Hamburg|gina.fischer@yaho...|         HR Manager| 69000|Management|              20|Normalverdiener|\n",
      "|    Hannah|      Koch| 26| Frankfurt|hannah.koch@examp...|        UI Designer|  NULL|  Creative|               3|Geringverdiener|\n",
      "|      Igor|    Keller| 38|    Munich|  igor.keller@gmx.de|      Sales Manager| 75000|Management|              14|  Hochverdiener|\n",
      "|     Julia|   Schmitt| 24|D√ºsseldorf|julia.schmitt@hot...|    Content Creator| 41000|  Creative|               2|Normalverdiener|\n",
      "|      Karl|     Bauer| 50|    Berlin|karl.bauer@exampl...|                CTO|120000|     Other|              25|  Hochverdiener|\n",
      "|      Lina|     Maier| 33| Stuttgart|lina.maier@t-onli...|         Accountant|  NULL|   Finance|               8|Geringverdiener|\n",
      "|       Max|     Frank| 41|   Cologne|max.frank@example...|   Network Engineer| 71000|      Tech|              16|  Hochverdiener|\n",
      "|      Nina|   Lehmann| 30|   Hamburg|nina.lehmann@gmai...|      Product Owner| 66400|     Other|               7|Normalverdiener|\n",
      "|    Oliver|     Weber| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|     Other|               6|Normalverdiener|\n",
      "|     Paula|  Hartmann| 36|    Munich|paula.hartmann@ic...|   Product Designer| 65000|  Creative|              11|Normalverdiener|\n",
      "|   Quentin|    Schulz| 43| Frankfurt|quentin.schulz@ex...|         Consultant|  NULL|Management|              18|Geringverdiener|\n",
      "|      Rita|      Lang| 25|D√ºsseldorf|rita.lang@protonm...|   Junior Developer| 47000|     Other|               2|Normalverdiener|\n",
      "|    Stefan|    Becker| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|     Other|              13|Normalverdiener|\n",
      "|      Tina|     Kraus| 32|   Cologne|  tina.kraus@mail.de|      Data Engineer|  NULL|      Tech|               8|Geringverdiener|\n",
      "|     Marco|Zimmermann| 21|    Berlin|marco.zimmermann@...|             Intern| 28000|     Other|               0|Normalverdiener|\n",
      "|      Lisa|  Hoffmann| 23|   Hamburg|lisa.hoffmann@gma...|   Customer Service| 32000|     Other|               1|Normalverdiener|\n",
      "|       Tom|   Richter| 20|    Munich|tom.richter@examp...|    Working Student| 18000|     Other|               0|Geringverdiener|\n",
      "|     Sarah|      Wolf| 24| Frankfurt| sarah.wolf@yahoo.de|          Reception| 29000|     Other|               2|Normalverdiener|\n",
      "|     Kevin|   Neumann| 22|   Cologne|kevin.neumann@gmx.de|Warehouse Assistant| 31000|     Other|               1|Normalverdiener|\n",
      "+----------+----------+---+----------+--------------------+-------------------+------+----------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"income_category\",\n",
    "    when(F.col(\"salary\") >= 70000, \"Hochverdiener\")\n",
    "    .when((F.col(\"salary\") >= 28000) & (F.col(\"salary\") < 70000), \"Normalverdiener\")\n",
    "    .otherwise(\"Geringverdiener\")\n",
    ")\n",
    "df.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2133258a-426f-49dc-9c55-64642dd9581f",
   "metadata": {},
   "source": [
    "#### √úbung 4: Kategorisiere Mitarbeiter nach Berufserfahrung\n",
    "\n",
    "Aufgabe: Erstelle eine neue Spalte \"Erfahrungsstufe\", die Mitarbeiter basierend auf ihren Berufsjahren kategorisiert:\n",
    "\n",
    "    \"Senior\": 15+ Jahre Erfahrung\n",
    "\n",
    "    \"Mid-Level\": 5-14 Jahre Erfahrung\n",
    "\n",
    "    \"Junior\": Weniger als 5 Jahre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2f891b22-029c-4cf2-a76a-4b5eb26d3bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------------+---------------+\n",
      "|first_name|last_name|years_experience|Erfahrungsstufe|\n",
      "+----------+---------+----------------+---------------+\n",
      "|      Anna|  Mueller|               4|         Junior|\n",
      "|       Ben|  Schmidt|              12|      Mid-Level|\n",
      "|     Clara|    Klein|               1|         Junior|\n",
      "|     David|Schneider|              15|         Senior|\n",
      "|       Eva|    Huber|               6|      Mid-Level|\n",
      "|     Felix|   Wagner|               5|      Mid-Level|\n",
      "|      Gina|  Fischer|              20|         Senior|\n",
      "|    Hannah|     Koch|               3|         Junior|\n",
      "|      Igor|   Keller|              14|      Mid-Level|\n",
      "|     Julia|  Schmitt|               2|         Junior|\n",
      "|      Karl|    Bauer|              25|         Senior|\n",
      "|      Lina|    Maier|               8|      Mid-Level|\n",
      "|       Max|    Frank|              16|         Senior|\n",
      "|      Nina|  Lehmann|               7|      Mid-Level|\n",
      "|    Oliver|    Weber|               6|      Mid-Level|\n",
      "|     Paula| Hartmann|              11|      Mid-Level|\n",
      "|   Quentin|   Schulz|              18|         Senior|\n",
      "|      Rita|     Lang|               2|         Junior|\n",
      "|    Stefan|   Becker|              13|      Mid-Level|\n",
      "|      Tina|    Kraus|               8|      Mid-Level|\n",
      "+----------+---------+----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\n",
    "    \"Erfahrungsstufe\",\n",
    "    F.when(F.col(\"years_experience\") >= 15, \"Senior\")\n",
    "     .when(F.col(\"years_experience\") >= 5, \"Mid-Level\")\n",
    "     .otherwise(\"Junior\")\n",
    ")\n",
    "df.select(\"first_name\", \"last_name\", \"years_experience\", \"Erfahrungsstufe\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af09c43-3bca-46ca-b6bd-bdfa3903bffc",
   "metadata": {},
   "source": [
    "**Merke:**  \n",
    "> Viele verschachtelte `when`-Bedingungen k√∂nnen un√ºbersichtlich werden ‚Üí sauber strukturieren!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2954d6-368d-436f-bd07-8a7530190d8f",
   "metadata": {},
   "source": [
    "### Umgang mit fehlenden Werten (`fillna`, `dropna`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58cd589f-cb93-4841-b3d2-25994c39567b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "|   name|age|      city|               email|          job_title|salary|age_plus_5|income_category|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "|   Anna| 28|    Berlin|anna.mueller@exam...|       Data Analyst| 52000|      33.0|Normalverdiener|\n",
      "|    Ben| 35|   Hamburg|ben.schmidt@examp...|  Software Engineer| 74000|      40.0|Normalverdiener|\n",
      "|  Clara| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|      27.0|Geringverdiener|\n",
      "|  David| 40|   Cologne|david.schneider@e...|    Project Manager| 83000|      45.0|  Hochverdiener|\n",
      "|    Eva| 31| Stuttgart|eva.huber@example...|     Data Scientist| 68000|      36.0|Normalverdiener|\n",
      "|  Felix| 29|    Berlin|felix.wagner@exam...|    DevOps Engineer| 50000|      34.0|Geringverdiener|\n",
      "|   Gina| 45|   Hamburg|gina.fischer@exam...|         HR Manager| 69000|      50.0|Normalverdiener|\n",
      "| Hannah| 26| Frankfurt|hannah.koch@examp...|        UI Designer| 54000|      31.0|Normalverdiener|\n",
      "|   Igor| 38|    Munich|igor.keller@examp...|      Sales Manager| 75000|      43.0|Normalverdiener|\n",
      "|  Julia| 24|D√ºsseldorf|julia.schmitt@exa...|    Content Creator| 41000|      29.0|Geringverdiener|\n",
      "|   Karl| 50|    Berlin|karl.bauer@exampl...|                CTO|120000|      55.0|  Hochverdiener|\n",
      "|   Lina| 33| Stuttgart|lina.maier@exampl...|         Accountant| 58000|      38.0|Normalverdiener|\n",
      "|    Max| 41|   Cologne|max.frank@example...|   Network Engineer| 71000|      46.0|Normalverdiener|\n",
      "|   Nina| 30|   Hamburg|nina.lehmann@exam...|      Product Owner| 50000|      35.0|Geringverdiener|\n",
      "| Oliver| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|      32.0|Normalverdiener|\n",
      "|  Paula| 36|    Munich|paula.hartmann@ex...|   Product Designer| 65000|      41.0|Normalverdiener|\n",
      "|Quentin| 43| Frankfurt|quentin.schulz@ex...|         Consultant| 80000|      48.0|  Hochverdiener|\n",
      "|   Rita| 25|D√ºsseldorf|rita.lang@example...|   Junior Developer| 47000|      30.0|Geringverdiener|\n",
      "| Stefan| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|      44.0|Normalverdiener|\n",
      "|   Tina| 32|   Cologne|tina.kraus@exampl...|      Data Engineer| 73000|      37.0|Normalverdiener|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fehlende Werte f√ºllen (`fillna`):\n",
    "# Ersetzt fehlende Geh√§lter durch 50000\n",
    "df_filled = df.fillna({\"salary\": 50000})\n",
    "df_filled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b47df62-00cc-4dac-9afc-d5da14cf63d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "|   name|age|      city|               email|          job_title|salary|age_plus_5|income_category|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "|   Anna| 28|    Berlin|anna.mueller@exam...|       Data Analyst| 52000|      33.0|Normalverdiener|\n",
      "|    Ben| 35|   Hamburg|ben.schmidt@examp...|  Software Engineer| 74000|      40.0|Normalverdiener|\n",
      "|  Clara| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|      27.0|Geringverdiener|\n",
      "|  David| 40|   Cologne|david.schneider@e...|    Project Manager| 83000|      45.0|  Hochverdiener|\n",
      "|    Eva| 31| Stuttgart|eva.huber@example...|     Data Scientist| 68000|      36.0|Normalverdiener|\n",
      "|   Gina| 45|   Hamburg|gina.fischer@exam...|         HR Manager| 69000|      50.0|Normalverdiener|\n",
      "| Hannah| 26| Frankfurt|hannah.koch@examp...|        UI Designer| 54000|      31.0|Normalverdiener|\n",
      "|   Igor| 38|    Munich|igor.keller@examp...|      Sales Manager| 75000|      43.0|Normalverdiener|\n",
      "|  Julia| 24|D√ºsseldorf|julia.schmitt@exa...|    Content Creator| 41000|      29.0|Geringverdiener|\n",
      "|   Karl| 50|    Berlin|karl.bauer@exampl...|                CTO|120000|      55.0|  Hochverdiener|\n",
      "|   Lina| 33| Stuttgart|lina.maier@exampl...|         Accountant| 58000|      38.0|Normalverdiener|\n",
      "|    Max| 41|   Cologne|max.frank@example...|   Network Engineer| 71000|      46.0|Normalverdiener|\n",
      "| Oliver| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|      32.0|Normalverdiener|\n",
      "|  Paula| 36|    Munich|paula.hartmann@ex...|   Product Designer| 65000|      41.0|Normalverdiener|\n",
      "|Quentin| 43| Frankfurt|quentin.schulz@ex...|         Consultant| 80000|      48.0|  Hochverdiener|\n",
      "|   Rita| 25|D√ºsseldorf|rita.lang@example...|   Junior Developer| 47000|      30.0|Geringverdiener|\n",
      "| Stefan| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|      44.0|Normalverdiener|\n",
      "|   Tina| 32|   Cologne|tina.kraus@exampl...|      Data Engineer| 73000|      37.0|Normalverdiener|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zeilen mit fehlenden Werten l√∂schen (`dropna`):\n",
    "# Entfernt alle Zeilen, die mindestens einen `null`-Wert enthalten.\n",
    "df_clean = df.dropna()\n",
    "df_clean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e5eaa6-75f7-4694-8a6a-340bf1cd39ba",
   "metadata": {},
   "source": [
    "### Gruppieren und Aggregieren (`groupBy` + `agg`)\n",
    "\n",
    "Mit `.groupBy()` kannst du dein DataFrame nach einer oder mehreren Spalten **gruppieren**.  \n",
    "Mit `.agg()` kannst du dann **Aggregationfunktionen** auf jede Gruppe anwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1789598f-3d5f-4cb0-bc2f-2ecc17891ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|department|Anzahl_Mitarbeiter|\n",
      "+----------+------------------+\n",
      "|Management|                 4|\n",
      "|   Finance|                 1|\n",
      "|     Other|                11|\n",
      "|      Tech|                 6|\n",
      "|  Creative|                 3|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Gruppierung nach Abteilung (department)\n",
    "# Frage: \"Wie viele Mitarbeiter gibt es pro Abteilung?\"\n",
    "\n",
    "df.groupBy(\"department\") \\\n",
    "  .agg(F.count(\"*\").alias(\"Anzahl_Mitarbeiter\")) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ab15b085-6fd2-405a-b8ea-d4b70ec9c4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      city|Durchschnittsalter|\n",
      "+----------+------------------+\n",
      "| Stuttgart|34.333333333333336|\n",
      "|   Cologne|             33.75|\n",
      "|   Hamburg|             33.25|\n",
      "| Frankfurt|              31.0|\n",
      "|    Berlin|              31.0|\n",
      "|    Munich|              29.0|\n",
      "|D√ºsseldorf|              24.5|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Durchschnittsalter pro Stadt\n",
    "# Frage: \"Wie alt sind Mitarbeiter im Durchschnitt pro Stadt?\"\n",
    "\n",
    "df.groupBy(\"city\") \\\n",
    "  .agg(F.avg(\"age\").alias(\"Durchschnittsalter\")) \\\n",
    "  .orderBy(\"Durchschnittsalter\", ascending=False) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7477e3d2-ac65-4da1-9fc3-09c76bc26673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|department|Maximalgehalt|\n",
      "+----------+-------------+\n",
      "|  Creative|        65000|\n",
      "|Management|        83000|\n",
      "|     Other|        67000|\n",
      "|      Tech|        71000|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. H√∂chstgehalt pro Abteilung (ohne NULL-Werte)\n",
    "# Frage: \"Was ist das maximale Gehalt in jeder Abteilung?\"\n",
    "\n",
    "df.filter(F.col(\"salary\").isNotNull()) \\\n",
    "  .groupBy(\"department\") \\\n",
    "  .agg(F.max(\"salary\").alias(\"Maximalgehalt\")) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0b3bb1a5-663f-4e32-9532-a7585b8b8fc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------------------+---------------+\n",
      "|department|Anzahl|Durchschnittsgehalt|Gesamterfahrung|\n",
      "+----------+------+-------------------+---------------+\n",
      "|Management|     4|  75666.66666666667|           67.0|\n",
      "|   Finance|     1|               NULL|            8.0|\n",
      "|     Other|    11|  48763.63636363636|           58.0|\n",
      "|      Tech|     6| 63533.333333333336|           51.0|\n",
      "|  Creative|     3|            53000.0|           16.0|\n",
      "+----------+------+-------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Kombinierte Aggregationen\n",
    "# Frage: \"Zeige pro Abteilung: Anzahl, Durchschnittsgehalt und Erfahrungssumme\"\n",
    "\n",
    "df.groupBy(\"department\") \\\n",
    "  .agg(\n",
    "      F.count(\"*\").alias(\"Anzahl\"),\n",
    "      F.avg(\"salary\").alias(\"Durchschnittsgehalt\"),\n",
    "      F.sum(\"years_experience\").alias(\"Gesamterfahrung\")\n",
    "  ) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e29f1f-b4d9-4643-9c70-d3a8adfe3648",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### √úbung 4: Aggregation\n",
    "1. Frage: \"Wie viele Mitarbeiter arbeiten in jeder Stadt?\"\n",
    "1. Frage: \"Welche Abteilungen haben mehr als 5 Mitarbeiter?\"\n",
    "2. Frage: \"Wie viele Normalverdiener (20.000-70.000‚Ç¨) und Hochverdiener (>70.000‚Ç¨) gibt es pro Stadt?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4ef4e59c-7af8-435e-b293-ecb9fb40042d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      city|Anzahl_Mitarbeiter|\n",
      "+----------+------------------+\n",
      "| Frankfurt|                 3|\n",
      "|    Berlin|                 5|\n",
      "|D√ºsseldorf|                 2|\n",
      "|   Hamburg|                 4|\n",
      "| Stuttgart|                 3|\n",
      "|   Cologne|                 4|\n",
      "|    Munich|                 4|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "df.groupBy(\"city\") \\\n",
    "  .agg(F.count(\"*\").alias(\"Anzahl_Mitarbeiter\")) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a38d9a81-1bbe-447d-b2c0-4dff13561efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|department|Anzahl|\n",
      "+----------+------+\n",
      "|     Other|    11|\n",
      "|      Tech|     6|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "df.groupBy(\"department\") \\\n",
    "  .agg(F.count(\"*\").alias(\"Anzahl\")) \\\n",
    "  .filter(F.col(\"Anzahl\") > 5) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fc2363be-0620-4d5d-8113-82afa7004f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------+\n",
      "|      city|  Gehaltsklasse|Anzahl|\n",
      "+----------+---------------+------+\n",
      "|    Berlin|  Hochverdiener|     1|\n",
      "|    Berlin|Normalverdiener|     4|\n",
      "|   Cologne|Normalverdiener|     1|\n",
      "|   Cologne|  Hochverdiener|     2|\n",
      "|D√ºsseldorf|Normalverdiener|     2|\n",
      "| Frankfurt|Normalverdiener|     1|\n",
      "|   Hamburg|Normalverdiener|     3|\n",
      "|    Munich|  Hochverdiener|     1|\n",
      "|    Munich|Normalverdiener|     2|\n",
      "| Stuttgart|Normalverdiener|     1|\n",
      "+----------+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Zuerst Gehaltskategorie-Spalte erstellen\n",
    "df_mit_kategorie = df.withColumn(\n",
    "    \"Gehaltsklasse\",\n",
    "    F.when(F.col(\"salary\") >= 70000, \"Hochverdiener\")\n",
    "     .when(F.col(\"salary\") >= 20000, \"Normalverdiener\")\n",
    "     .otherwise(\"Geringverdiener\")\n",
    ")\n",
    "\n",
    "# Dann gruppieren\n",
    "df_mit_kategorie.filter(F.col(\"Gehaltsklasse\").isin([\"Hochverdiener\", \"Normalverdiener\"])) \\\n",
    "  .groupBy(\"city\", \"Gehaltsklasse\") \\\n",
    "  .agg(F.count(\"*\").alias(\"Anzahl\")) \\\n",
    "  .orderBy(\"city\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434926aa-1dc1-486d-8e0b-bfd06df133aa",
   "metadata": {},
   "source": [
    "**Wichtig zu wissen:**  \n",
    "> `.groupBy()` alleine macht noch nichts. Erst `.agg()`, `.count()`, `.sum()` oder `.avg()` l√∂sen die echte Berechnung aus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f41c4d-506e-47fc-b1c9-f3246af5c8f9",
   "metadata": {},
   "source": [
    "### Sortieren und Reihenfolge √§ndern (`orderBy`, `sort`)\n",
    "\n",
    "Mit `.orderBy()` oder `.sort()` kannst du dein DataFrame sortieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "df4fe227-0d66-4e5f-96a0-ffb02165658d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e461222-3472-4bdd-9a6a-3767908642df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.orderBy(F.col(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "abc688e1-a2c2-4549-82d6-b22b74a6ee82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('first_name', 'string'),\n",
       " ('last_name', 'string'),\n",
       " ('age', 'string'),\n",
       " ('city', 'string'),\n",
       " ('email', 'string'),\n",
       " ('job_title', 'string'),\n",
       " ('salary', 'int'),\n",
       " ('department', 'string'),\n",
       " ('years_experience', 'string'),\n",
       " ('income_category', 'string'),\n",
       " ('Erfahrungsstufe', 'string')]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "5a905c80-ee74-4d72-90a3-8861ec215020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---+----------+--------------------+-------------------+------+----------+----------------+---------------+---------------+\n",
      "|first_name| last_name|age|      city|               email|          job_title|salary|department|years_experience|income_category|Erfahrungsstufe|\n",
      "+----------+----------+---+----------+--------------------+-------------------+------+----------+----------------+---------------+---------------+\n",
      "|       Ben|   Schmidt| 35|   Hamburg|ben.schmidt@outlo...|  Software Engineer|  NULL|      Tech|              12|Geringverdiener|      Mid-Level|\n",
      "|       Eva|     Huber| 31| Stuttgart|    eva.huber@web.de|     Data Scientist|  NULL|      Tech|               6|Geringverdiener|      Mid-Level|\n",
      "|    Hannah|      Koch| 26| Frankfurt|hannah.koch@examp...|        UI Designer|  NULL|  Creative|               3|Geringverdiener|         Junior|\n",
      "|      Lina|     Maier| 33| Stuttgart|lina.maier@t-onli...|         Accountant|  NULL|   Finance|               8|Geringverdiener|      Mid-Level|\n",
      "|   Quentin|    Schulz| 43| Frankfurt|quentin.schulz@ex...|         Consultant|  NULL|Management|              18|Geringverdiener|         Senior|\n",
      "|      Tina|     Kraus| 32|   Cologne|  tina.kraus@mail.de|      Data Engineer|  NULL|      Tech|               8|Geringverdiener|      Mid-Level|\n",
      "|       Tom|   Richter| 20|    Munich|tom.richter@examp...|    Working Student| 18000|     Other|               0|Geringverdiener|         Junior|\n",
      "|     Marco|Zimmermann| 21|    Berlin|marco.zimmermann@...|             Intern| 28000|     Other|               0|Normalverdiener|         Junior|\n",
      "|     Sarah|      Wolf| 24| Frankfurt| sarah.wolf@yahoo.de|          Reception| 29000|     Other|               2|Normalverdiener|         Junior|\n",
      "|     Kevin|   Neumann| 22|   Cologne|kevin.neumann@gmx.de|Warehouse Assistant| 31000|     Other|               1|Normalverdiener|         Junior|\n",
      "|      Lisa|  Hoffmann| 23|   Hamburg|lisa.hoffmann@gma...|   Customer Service| 32000|     Other|               1|Normalverdiener|         Junior|\n",
      "|     Julia|   Schmitt| 24|D√ºsseldorf|julia.schmitt@hot...|    Content Creator| 41000|  Creative|               2|Normalverdiener|         Junior|\n",
      "|     Clara|     Klein| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|     Other|               1|Normalverdiener|         Junior|\n",
      "|      Rita|      Lang| 25|D√ºsseldorf|rita.lang@protonm...|   Junior Developer| 47000|     Other|               2|Normalverdiener|         Junior|\n",
      "|      Anna|   Mueller| 28|    Berlin|anna.mueller@gmai...|       Data Analyst| 52000|      Tech|               4|Normalverdiener|         Junior|\n",
      "|    Stefan|    Becker| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|     Other|              13|Normalverdiener|      Mid-Level|\n",
      "|     Paula|  Hartmann| 36|    Munich|paula.hartmann@ic...|   Product Designer| 65000|  Creative|              11|Normalverdiener|      Mid-Level|\n",
      "|      Nina|   Lehmann| 30|   Hamburg|nina.lehmann@gmai...|      Product Owner| 66400|     Other|               7|Normalverdiener|      Mid-Level|\n",
      "|    Oliver|     Weber| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|     Other|               6|Normalverdiener|      Mid-Level|\n",
      "|     Felix|    Wagner| 29|    Berlin|felix.wagner@exam...|    DevOps Engineer| 67600|      Tech|               5|Normalverdiener|      Mid-Level|\n",
      "+----------+----------+---+----------+--------------------+-------------------+------+----------+----------------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"salary\", F.col(\"salary\").cast(IntegerType()))\n",
    "df.orderBy(F.col(\"salary\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9a1a8e7c-97e8-49fe-b129-300eca90e148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[first_name: string, last_name: string, age: string, city: string, email: string, job_title: string, salary: string, department: string, years_experience: string, income_category: string, Erfahrungsstufe: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "25eb8450-e829-4e4e-b3fa-abf6e34b0f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---+----------+--------------------+-------------------+------+----------+----------------+---------------+---------------+\n",
      "|first_name| last_name|age|      city|               email|          job_title|salary|department|years_experience|income_category|Erfahrungsstufe|\n",
      "+----------+----------+---+----------+--------------------+-------------------+------+----------+----------------+---------------+---------------+\n",
      "|       Tom|   Richter| 20|    Munich|tom.richter@examp...|    Working Student| 18000|     Other|               0|Geringverdiener|         Junior|\n",
      "|     Marco|Zimmermann| 21|    Berlin|marco.zimmermann@...|             Intern| 28000|     Other|               0|Normalverdiener|         Junior|\n",
      "|     Clara|     Klein| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|     Other|               1|Normalverdiener|         Junior|\n",
      "|     Kevin|   Neumann| 22|   Cologne|kevin.neumann@gmx.de|Warehouse Assistant| 31000|     Other|               1|Normalverdiener|         Junior|\n",
      "|      Lisa|  Hoffmann| 23|   Hamburg|lisa.hoffmann@gma...|   Customer Service| 32000|     Other|               1|Normalverdiener|         Junior|\n",
      "|     Julia|   Schmitt| 24|D√ºsseldorf|julia.schmitt@hot...|    Content Creator| 41000|  Creative|               2|Normalverdiener|         Junior|\n",
      "|     Sarah|      Wolf| 24| Frankfurt| sarah.wolf@yahoo.de|          Reception| 29000|     Other|               2|Normalverdiener|         Junior|\n",
      "|      Rita|      Lang| 25|D√ºsseldorf|rita.lang@protonm...|   Junior Developer| 47000|     Other|               2|Normalverdiener|         Junior|\n",
      "|    Hannah|      Koch| 26| Frankfurt|hannah.koch@examp...|        UI Designer|  NULL|  Creative|               3|Geringverdiener|         Junior|\n",
      "|    Oliver|     Weber| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|     Other|               6|Normalverdiener|      Mid-Level|\n",
      "|      Anna|   Mueller| 28|    Berlin|anna.mueller@gmai...|       Data Analyst| 52000|      Tech|               4|Normalverdiener|         Junior|\n",
      "|     Felix|    Wagner| 29|    Berlin|felix.wagner@exam...|    DevOps Engineer| 67600|      Tech|               5|Normalverdiener|      Mid-Level|\n",
      "|      Nina|   Lehmann| 30|   Hamburg|nina.lehmann@gmai...|      Product Owner| 66400|     Other|               7|Normalverdiener|      Mid-Level|\n",
      "|       Eva|     Huber| 31| Stuttgart|    eva.huber@web.de|     Data Scientist|  NULL|      Tech|               6|Geringverdiener|      Mid-Level|\n",
      "|      Tina|     Kraus| 32|   Cologne|  tina.kraus@mail.de|      Data Engineer|  NULL|      Tech|               8|Geringverdiener|      Mid-Level|\n",
      "|      Lina|     Maier| 33| Stuttgart|lina.maier@t-onli...|         Accountant|  NULL|   Finance|               8|Geringverdiener|      Mid-Level|\n",
      "|       Ben|   Schmidt| 35|   Hamburg|ben.schmidt@outlo...|  Software Engineer|  NULL|      Tech|              12|Geringverdiener|      Mid-Level|\n",
      "|     Paula|  Hartmann| 36|    Munich|paula.hartmann@ic...|   Product Designer| 65000|  Creative|              11|Normalverdiener|      Mid-Level|\n",
      "|      Igor|    Keller| 38|    Munich|  igor.keller@gmx.de|      Sales Manager| 75000|Management|              14|  Hochverdiener|      Mid-Level|\n",
      "|    Stefan|    Becker| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|     Other|              13|Normalverdiener|      Mid-Level|\n",
      "|     David| Schneider| 40|   Cologne|david.schneider@c...|    Project Manager| 83000|Management|              15|  Hochverdiener|         Senior|\n",
      "|       Max|     Frank| 41|   Cologne|max.frank@example...|   Network Engineer| 71000|      Tech|              16|  Hochverdiener|         Senior|\n",
      "|   Quentin|    Schulz| 43| Frankfurt|quentin.schulz@ex...|         Consultant|  NULL|Management|              18|Geringverdiener|         Senior|\n",
      "|      Gina|   Fischer| 45|   Hamburg|gina.fischer@yaho...|         HR Manager| 69000|Management|              20|Normalverdiener|         Senior|\n",
      "|      Karl|     Bauer| 50|    Berlin|karl.bauer@exampl...|                CTO|120000|     Other|              25|  Hochverdiener|         Senior|\n",
      "+----------+----------+---+----------+--------------------+-------------------+------+----------+----------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Oder aufsteigend (Standard)\n",
    "df.orderBy(\"age\").show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daa97ee-28a9-4233-986c-29f4a94efd60",
   "metadata": {},
   "source": [
    "## üîÉ Sortieren und Reihenfolge √§ndern (`orderBy`, `sort`)\n",
    "\n",
    "Mit `.orderBy()` oder `.sort()` kannst du dein DataFrame sortieren.\n",
    "\n",
    "**Beispiel: Nach Gehalt absteigend sortieren:**\n",
    "\n",
    "```python\n",
    "df.orderBy(F.desc(\"salary\")).show()\n",
    "```\n",
    "\n",
    "**Oder aufsteigend (Standard):**\n",
    "\n",
    "```python\n",
    "df.orderBy(\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23c4293-1fc5-4d04-9474-60b5d87686c3",
   "metadata": {},
   "source": [
    "**Hinweis:**  \n",
    "> `orderBy` und `sort` sind **gleichwertig**.  \n",
    "> Bei riesigen DataFrames kann Sortieren teuer werden ‚Üí vorsichtig einsetzen!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7f8a27-725b-43a3-94d2-6d63acb35f39",
   "metadata": {},
   "source": [
    "## ‚úÖ Daten von Azure einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1ef2dcb-6e82-4e8f-81d3-c2b309be7ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab3c9935-2a73-48bf-8403-399b50e9cd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Localspark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\",\n",
    "        \"com.microsoft.sqlserver:mssql-jdbc:12.6.1.jre11\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd8d78fa-f121-4dfa-aa00-0f9318223868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load variables from .env into os.environ\n",
    "load_dotenv(\"env\")\n",
    "server_name = os.environ.get(\"SERVERNAME\")\n",
    "database_name = os.environ.get(\"DATABASENAME\")\n",
    "username = os.environ.get(\"USERNAME\")\n",
    "password = os.environ.get(\"PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6bfa320-7c42-4aee-b4fd-e1206662edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard JDBC URL format for SQL Server\n",
    "jdbc_url = f\"jdbc:sqlserver://{server_name}:1433;databaseName={database_name}\"\n",
    "\n",
    "#table name to read\n",
    "table_name = \"BIB.Autor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8eab2f5-063c-49a7-93d4-47ca6e55176b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID_Autor: integer (nullable = true)\n",
      " |-- Vorname_1: string (nullable = true)\n",
      " |-- Vorname_2: string (nullable = true)\n",
      " |-- Nachname: string (nullable = true)\n",
      "\n",
      "+--------+---------+---------+--------+\n",
      "|ID_Autor|Vorname_1|Vorname_2|Nachname|\n",
      "+--------+---------+---------+--------+\n",
      "|       1|      Abe|     NULL|    Kobo|\n",
      "|       2|   Chinua|     NULL|  Achebe|\n",
      "|       3|  Theodor|       W.|  Adorno|\n",
      "|       4|     NULL|     NULL|    √Ñsop|\n",
      "|       5|   Samuel|     NULL|   Agnon|\n",
      "+--------+---------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    jdbcDF = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"user\", username) \\\n",
    "        .option(\"password\", password) \\\n",
    "        .load()\n",
    "\n",
    "    # Display schema and a few rows if successful\n",
    "    jdbcDF.printSchema()\n",
    "    jdbcDF.show(5)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"JDBC read failed!\")\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd9f574-fe61-4a32-a8d8-f4b6f547992f",
   "metadata": {},
   "source": [
    "### üß© Beispiel: join() in PySpark\n",
    "üéØ Ziel: Zwei DataFrames anhand einer Spalte verbinden (JOIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa58e84d-2236-4b9d-855e-8c3585df1416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Erweiterte Joins\").getOrCreate()\n",
    "\n",
    "df_kunden = spark.createDataFrame([\n",
    "    Row(kunden_id=1, name=\"Anna\"),\n",
    "    Row(kunden_id=2, name=\"Ben\"),\n",
    "    Row(kunden_id=3, name=\"Clara\"),\n",
    "    Row(kunden_id=4, name=\"Dieter\"),\n",
    "    Row(kunden_id=5, name=\"Eva\"),\n",
    "    Row(kunden_id=6, name=\"Fritz\"),\n",
    "    Row(kunden_id=7, name=\"Gina\")\n",
    "])\n",
    "\n",
    "df_bestellungen = spark.createDataFrame([\n",
    "    Row(bestell_id=101, kunden_id=1, produkt=\"Buch\"),\n",
    "    Row(bestell_id=102, kunden_id=2, produkt=\"Tasse\"),\n",
    "    Row(bestell_id=103, kunden_id=4, produkt=\"Stift\"),\n",
    "    Row(bestell_id=104, kunden_id=1, produkt=\"Laptop\"),\n",
    "    Row(bestell_id=105, kunden_id=6, produkt=\"Kabel\"),\n",
    "    Row(bestell_id=106, kunden_id=8, produkt=\"Lampe\")  # kein passender Kunde\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4747bcaf-8bed-47f8-ae3f-2191c6a012ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|kunden_id|  name|\n",
      "+---------+------+\n",
      "|        1|  Anna|\n",
      "|        2|   Ben|\n",
      "|        3| Clara|\n",
      "|        4|Dieter|\n",
      "|        5|   Eva|\n",
      "|        6| Fritz|\n",
      "|        7|  Gina|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_kunden.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ac53ff8-a2dd-45c5-bbd3-87d2690a86d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|bestell_id|kunden_id|produkt|\n",
      "+----------+---------+-------+\n",
      "|       101|        1|   Buch|\n",
      "|       102|        2|  Tasse|\n",
      "|       103|        4|  Stift|\n",
      "|       104|        1| Laptop|\n",
      "|       105|        6|  Kabel|\n",
      "|       106|        8|  Lampe|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bestellungen.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af08041-c96b-4c49-96e9-0c61a6280ed3",
   "metadata": {},
   "source": [
    "### Inner join\n",
    "‚úÖ Nur Kunden mit passenden Bestellungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2a82b1d-8b73-437c-afea-c03655260d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+----------+-------+\n",
      "|kunden_id|  name|bestell_id|produkt|\n",
      "+---------+------+----------+-------+\n",
      "|        1|  Anna|       101|   Buch|\n",
      "|        1|  Anna|       104| Laptop|\n",
      "|        2|   Ben|       102|  Tasse|\n",
      "|        4|Dieter|       103|  Stift|\n",
      "|        6| Fritz|       105|  Kabel|\n",
      "+---------+------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_kunden.join(df_bestellungen, on=\"kunden_id\", how=\"inner\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8be01b5-10f0-49e5-8f4d-cfb8168342a7",
   "metadata": {},
   "source": [
    "### left Join\n",
    "‚úÖ Alle Kunden, auch wenn sie keine Bestellungen haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d44b0123-8a2d-4a68-a0b7-70d6cb0c7cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[kunden_id: bigint, name: string, bestell_id: bigint, produkt: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kunden.join(df_bestellungen, on=\"kunden_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e425826c-9fd1-4f15-a909-3de01e868eba",
   "metadata": {},
   "source": [
    "## right Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25572ae8-e287-4f15-bbc2-79c5686fd299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+----------+-------+\n",
      "|kunden_id|  name|bestell_id|produkt|\n",
      "+---------+------+----------+-------+\n",
      "|        1|  Anna|       101|   Buch|\n",
      "|        2|   Ben|       102|  Tasse|\n",
      "|        4|Dieter|       103|  Stift|\n",
      "|        1|  Anna|       104| Laptop|\n",
      "|        6| Fritz|       105|  Kabel|\n",
      "|        8|  NULL|       106|  Lampe|\n",
      "+---------+------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_kunden.join(df_bestellungen, on=\"kunden_id\", how=\"right\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c527a6f9-5e7b-4ea7-8115-a6b83a63f199",
   "metadata": {},
   "source": [
    "## outer Join\n",
    "‚úÖ Alle Daten aus beiden Tabellen, inkl. null, wenn kein Match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1389662b-9704-4bcb-b4a5-a274aee35624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+----------+-------+\n",
      "|kunden_id|  name|bestell_id|produkt|\n",
      "+---------+------+----------+-------+\n",
      "|        1|  Anna|       101|   Buch|\n",
      "|        1|  Anna|       104| Laptop|\n",
      "|        2|   Ben|       102|  Tasse|\n",
      "|        3| Clara|      NULL|   NULL|\n",
      "|        4|Dieter|       103|  Stift|\n",
      "|        5|   Eva|      NULL|   NULL|\n",
      "|        6| Fritz|       105|  Kabel|\n",
      "|        7|  Gina|      NULL|   NULL|\n",
      "|        8|  NULL|       106|  Lampe|\n",
      "+---------+------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_kunden.join(df_bestellungen, on=\"kunden_id\", how=\"outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18e910b-6452-4064-82a5-679e8d3add65",
   "metadata": {},
   "source": [
    "### left_semi Join\n",
    "‚úÖ Nur Kunden, die mindestens eine Bestellung haben (keine Bestelldaten sichtbar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6612933-362e-4762-a917-3531725707ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|kunden_id|  name|\n",
      "+---------+------+\n",
      "|        1|  Anna|\n",
      "|        2|   Ben|\n",
      "|        4|Dieter|\n",
      "|        6| Fritz|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_kunden.join(df_bestellungen, on=\"kunden_id\", how=\"left_semi\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904f818d-1e07-42ed-9f94-fbe10b49e089",
   "metadata": {},
   "source": [
    "#### üß† Wann braucht man einen left_semi Join?\n",
    "\n",
    "Ein left_semi Join wird verwendet, wenn du:\n",
    "\n",
    "    ‚úÖ nur Zeilen aus dem linken DataFrame behalten willst,\n",
    "    ‚úÖ nur dann, wenn es einen passenden Eintrag im rechten DataFrame gibt,\n",
    "    ‚ùå aber keine Spalten aus dem rechten DataFrame brauchst.\n",
    "    \n",
    "<p style=\"text-align: center;\"><b>Vergleich left_semi vs inner </b></p>\n",
    "\n",
    "| Eigenschaft                  | `inner` Join | `left_semi` Join |\n",
    "| ---------------------------- | ------------ | ---------------- |\n",
    "| Gibt linke Zeilen zur√ºck     | ‚úÖ            | ‚úÖ                |\n",
    "| Gibt rechte Spalten zur√ºck   | ‚úÖ            | ‚ùå                |\n",
    "| Nutzt man als Filter         | ‚ùå            | ‚úÖ                |\n",
    "| Performance bei gro√üen Daten | Mittel       | Sehr gut         |\n",
    "\n",
    "#### ‚öôÔ∏è Performance-Optimierung\n",
    "\n",
    "    - Schneller als inner join, weil keine unn√∂tigen Spalten gezogen werden.\n",
    "    - Reduziert Speicherbedarf und Shuffle bei gro√üen Datenmengen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ee71da-e917-4d36-be74-04f1e3185b8f",
   "metadata": {},
   "source": [
    " ### left_anti Join\n",
    "‚úÖ Nur Kunden, die keine Bestellung haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5a0f8d7-154e-4024-94b0-a1b34bfedc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|kunden_id| name|\n",
      "+---------+-----+\n",
      "|        3|Clara|\n",
      "|        5|  Eva|\n",
      "|        7| Gina|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_kunden.join(df_bestellungen, on=\"kunden_id\", how=\"left_anti\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528a2ea7-0e51-4d56-b374-c438570495de",
   "metadata": {},
   "source": [
    "#### üöÄ Was ist ein Broadcast Join?\n",
    "\n",
    "Ein Broadcast Join sendet (broadcastet) eine kleine Tabelle an alle Worker-Nodes, sodass ein teurer Shuffle-Vorgang vermieden wird.\n",
    "\n",
    "Wann?\n",
    "\n",
    "    - Wenn ein DataFrame klein ist (z.‚ÄØB. eine Lookup-Tabelle).\n",
    "    - Ideal bei Skewed Joins oder Star Schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a2a9622-54cb-47a3-8963-797c5e11de88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+\n",
      "|user_id|amount| name|\n",
      "+-------+------+-----+\n",
      "|      1|   200| Anna|\n",
      "|      2|   150|  Ben|\n",
      "|      3|   100|Clara|\n",
      "+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Broadcast Join\").getOrCreate()\n",
    "\n",
    "# Gro√üe Tabelle\n",
    "sales = spark.createDataFrame([\n",
    "    Row(user_id=1, amount=200),\n",
    "    Row(user_id=2, amount=150),\n",
    "    Row(user_id=3, amount=100),\n",
    "    Row(user_id=4, amount=50),\n",
    "    Row(user_id=5, amount=300)\n",
    "])\n",
    "\n",
    "# Kleine Lookup-Tabelle\n",
    "users = spark.createDataFrame([\n",
    "    Row(user_id=1, name=\"Anna\"),\n",
    "    Row(user_id=2, name=\"Ben\"),\n",
    "    Row(user_id=3, name=\"Clara\")\n",
    "])\n",
    "\n",
    "# Broadcast Join\n",
    "joined = sales.join(broadcast(users), on=\"user_id\", how=\"inner\")\n",
    "joined.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf50a08-0db0-4020-979b-8f42360cb2e5",
   "metadata": {},
   "source": [
    "#### üß† Wann braucht man einen Broadcast Join?\n",
    "\n",
    "    - Wenn du eine kleine Tabelle hast, z.‚ÄØB. < 10 MB.\n",
    "    - Die kleine Tabelle wird an alle Worker geschickt.\n",
    "    - Spart Shuffle-Zeit und Speicher ‚Äì ideal f√ºr Star Schema Joins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078a3fb8-1453-4ffe-bdac-9efca73901a4",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"text-align: center;\"><b>üìå Typische Anwendungsf√§lle</b></p>\n",
    "\n",
    "| Anwendung                      | Warum Broadcast?                               |\n",
    "| ------------------------------ | ---------------------------------------------- |\n",
    "| Produkttabelle in E-Commerce   | Produktinfos sind klein, Bestellungen riesig   |\n",
    "| Benutzerrolle-Lookup           | Rollen sind fix, schnell zu broadcasten        |\n",
    "| ISO-L√§ndercodes oder Metadaten | Kleine Referenzdaten, oft in Queries gebraucht |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2875acf7-b74a-4c9b-ae11-5486ece1a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"demo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0487f39a-f456-4fcc-81f1-05428f71cc7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b885569-85b0-427a-8267-495409e91fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f4862c7-1d32-4286-be4b-05ae93688339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#55L, wert1#56, wert2#60]\n",
      "   +- SortMergeJoin [id#55L], [id#59L], Inner\n",
      "      :- Sort [id#55L ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(id#55L, 200), ENSURE_REQUIREMENTS, [plan_id=183]\n",
      "      :     +- Filter isnotnull(id#55L)\n",
      "      :        +- Scan ExistingRDD[id#55L,wert1#56]\n",
      "      +- Sort [id#59L ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(id#59L, 200), ENSURE_REQUIREMENTS, [plan_id=184]\n",
      "            +- Filter isnotnull(id#59L)\n",
      "               +- Scan ExistingRDD[id#59L,wert2#60]\n",
      "\n",
      "\n",
      "+---+-----+-----+\n",
      "| id|wert1|wert2|\n",
      "+---+-----+-----+\n",
      "|  1|    A|    X|\n",
      "|  2|    B|    Y|\n",
      "+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ShuffleBeispiel\").getOrCreate()\n",
    "\n",
    "df1 = spark.createDataFrame([\n",
    "    (1, \"A\"), (2, \"B\"), (3, \"C\")\n",
    "], [\"id\", \"wert1\"])\n",
    "\n",
    "df2 = spark.createDataFrame([\n",
    "    (1, \"X\"), (2, \"Y\"), (4, \"Z\")\n",
    "], [\"id\", \"wert2\"])\n",
    "\n",
    "joined = df1.join(df2, on=\"id\")\n",
    "\n",
    "# Execution Plan anzeigen\n",
    "joined.explain()\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47d32179-cbd0-45e5-b98b-c6a366477ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d6014e-b792-4a69-828d-e2de6f17dbcd",
   "metadata": {},
   "source": [
    "### Spark-SQL API in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61b8594-3557-4973-ab13-6106768df2e2",
   "metadata": {},
   "source": [
    "| Was?                   | Kurz erkl√§rt                                                                                 | Warum das z√§hlt                                                                  |\n",
    "| ---------------------- | -------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |\n",
    "| **Spark SQL**          | SQL-Engine **im** Cluster: DataFrames wirken wie Tabellen ‚Üí du kannst Standard-SQL absetzen. | Analyst\\:innen nutzen vertraute Syntax, Entwickler\\:innen kriegen Cluster-Power. |\n",
    "| **PySpark-Bridge**     | DataFrame ‚Üí `createOrReplaceTempView()` ‚Üí `spark.sql(\"‚Ä¶\")`.                                  | Trennscharf: Python, SQL und Optimierer teilen denselben Plan.                   |\n",
    "| **Vorteile**           | ‚úì Vertraute Syntax <br>‚úì Catalyst-Optimierung gratis <br>‚úì Frei mischbar: SQL ‚Üî DataFrame    | Weniger Code-Duplizierung ‚Äì du nimmst jeweils die ausdrucksst√§rkere API.         |\n",
    "| **Temp Views** | leben nur solange die Session lebt; nach Notebook-Restart sind sie weg.           | Erst sichern / als Tabelle speichern, wenn du sie sp√§ter noch brauchst.                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a544420-370f-409e-9656-0fd5006b0d8c",
   "metadata": {},
   "source": [
    "#### 1. Basis-Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66f7e2a8-4ce3-4de1-8575-0e67c0770dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local[4]\")\n",
    "         .appName(\"sql-api-demo\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# 1. Mini-DataFrame im Arbeitsspeicher erzeugen\n",
    "data = [\n",
    "    (\"Alice\", \"Engineering\", 65000),\n",
    "    (\"Bob\",   \"Marketing\",  58000),\n",
    "    (\"Carol\", \"Engineering\", 72000),\n",
    "    (\"Dave\",  \"Marketing\",  55000),\n",
    "    (\"Eve\",   \"HR\",         50000),\n",
    "]\n",
    "cols = [\"name\", \"dept\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, cols)\n",
    "\n",
    "# 2. DataFrame als tempor√§re Tabelle registrieren\n",
    "df.createOrReplaceTempView(\"employees\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d008c2-6f0f-4474-a2a9-03cd78b8c9d8",
   "metadata": {},
   "source": [
    "#### 2. SQL direkt ausf√ºhren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f16ae8dd-2dc9-46f7-94d6-79aeda281e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+\n",
      "|       dept|avg_salary|headcount|\n",
      "+-----------+----------+---------+\n",
      "|Engineering|   68500.0|        2|\n",
      "|  Marketing|   56500.0|        2|\n",
      "|         HR|   50000.0|        1|\n",
      "+-----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Durchschnittsgehalt pro Abteilung (SQL)\n",
    "avg_salary_sql = spark.sql(\"\"\"\n",
    "    SELECT dept,\n",
    "           ROUND(AVG(salary), 0) AS avg_salary,\n",
    "           COUNT(*)             AS headcount\n",
    "    FROM   employees\n",
    "    GROUP  BY dept\n",
    "    ORDER  BY avg_salary DESC\n",
    "\"\"\")\n",
    "\n",
    "avg_salary_sql.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382b7c5f-f422-401a-8020-342eff1ae39e",
   "metadata": {},
   "source": [
    "#### 3. SQL und DataFrame-API kombinieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee51af6f-2a31-411d-afbe-b41d461201e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+\n",
      "|  abteilung|avg_salary|headcount|\n",
      "+-----------+----------+---------+\n",
      "|Engineering|   68500.0|        2|\n",
      "|  Marketing|   56500.0|        2|\n",
      "+-----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ergebnis in DataFrame-Notation weiterverarbeiten\n",
    "top_dept = (avg_salary_sql\n",
    "            .filter(col(\"headcount\") > 1)\n",
    "            .withColumnRenamed(\"dept\", \"abteilung\"))\n",
    "\n",
    "top_dept.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caa71b0-7583-40ce-9516-0e92092c0c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0844f2f2-228b-478c-a888-5c491ca41296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919a921c-55e7-4a1e-8491-a9049df92b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6086894-be8a-4dd4-bea3-6595d3e654e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b9529da-148a-47b3-ab8b-a207b585e4e4",
   "metadata": {},
   "source": [
    "### Window Functions\n",
    "\n",
    "Window Functions erlauben dir, √ºber eine **Teilmenge** deiner Daten zu arbeiten, **ohne** sie vollst√§ndig zu aggregieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7344af77-fded-48b8-a1a0-56a1222e94f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+------------+\n",
      "|   name|age|      city|               email|          job_title|salary|age_plus_5|income_category|rank_in_city|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+------------+\n",
      "| Oliver| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|      32.0|Normalverdiener|           1|\n",
      "|   Anna| 28|    Berlin|anna.mueller@exam...|       Data Analyst| 52000|      33.0|Normalverdiener|           2|\n",
      "|   Karl| 50|    Berlin|karl.bauer@exampl...|                CTO|120000|      55.0|  Hochverdiener|           3|\n",
      "|  Felix| 29|    Berlin|felix.wagner@exam...|    DevOps Engineer|  NULL|      34.0|Geringverdiener|           4|\n",
      "|  David| 40|   Cologne|david.schneider@e...|    Project Manager| 83000|      45.0|  Hochverdiener|           1|\n",
      "|   Tina| 32|   Cologne|tina.kraus@exampl...|      Data Engineer| 73000|      37.0|Normalverdiener|           2|\n",
      "|    Max| 41|   Cologne|max.frank@example...|   Network Engineer| 71000|      46.0|Normalverdiener|           3|\n",
      "|   Rita| 25|D√ºsseldorf|rita.lang@example...|   Junior Developer| 47000|      30.0|Normalverdiener|           1|\n",
      "|  Julia| 24|D√ºsseldorf|julia.schmitt@exa...|    Content Creator| 41000|      29.0|Geringverdiener|           2|\n",
      "|Quentin| 43| Frankfurt|quentin.schulz@ex...|         Consultant| 80000|      48.0|  Hochverdiener|           1|\n",
      "| Hannah| 26| Frankfurt|hannah.koch@examp...|        UI Designer| 54000|      31.0|Normalverdiener|           2|\n",
      "|    Ben| 35|   Hamburg|ben.schmidt@examp...|  Software Engineer| 74000|      40.0|Normalverdiener|           1|\n",
      "|   Gina| 45|   Hamburg|gina.fischer@exam...|         HR Manager| 69000|      50.0|Normalverdiener|           2|\n",
      "|   Nina| 30|   Hamburg|nina.lehmann@exam...|      Product Owner|  NULL|      35.0|Geringverdiener|           3|\n",
      "|   Igor| 38|    Munich|igor.keller@examp...|      Sales Manager| 75000|      43.0|Normalverdiener|           1|\n",
      "|  Paula| 36|    Munich|paula.hartmann@ex...|   Product Designer| 65000|      41.0|Normalverdiener|           2|\n",
      "|  Clara| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|      27.0|Geringverdiener|           3|\n",
      "|    Eva| 31| Stuttgart|eva.huber@example...|     Data Scientist| 68000|      36.0|Normalverdiener|           1|\n",
      "|   Lina| 33| Stuttgart|lina.maier@exampl...|         Accountant| 58000|      38.0|Normalverdiener|           2|\n",
      "| Stefan| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|      44.0|Normalverdiener|           3|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/29 08:00:42 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@baf44cee0c18:38801\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "25/04/29 08:00:51 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@baf44cee0c18:38801\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"city\").orderBy(F.desc(\"salary\"))\n",
    "\n",
    "df = df.withColumn(\"rank_in_city\", F.rank().over(window_spec))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4faa86-2ba9-4336-ba12-0ece074eb1ee",
   "metadata": {},
   "source": [
    "### Lesen von Azure Datenbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1808200-74c9-4ef7-8acf-8ce882caffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Localspark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\",\n",
    "        \"com.microsoft.sqlserver:mssql-jdbc:12.6.1.jre11\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0348963-f55f-4a87-8fb8-737c634b3ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
