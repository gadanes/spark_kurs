{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17cbce6c-6f5c-4104-aac7-9e4d1a90feb1",
   "metadata": {},
   "source": [
    "## üß† 1. Was ist Spark? Warum PySpark?\n",
    "\n",
    "> \"Alle reden √ºber Spark ‚Äî aber was l√∂st es eigentlich?\"\n",
    "\n",
    "- **Apache Spark** ist eine verteilte Datenverarbeitungs-Engine.\n",
    "  - *√úbersetzt*: Anstatt dass ein Computer alles alleine macht (wie dein Laptop mit Python), verteilt Spark die Arbeit auf *viele* Rechner oder CPU-Kerne.\n",
    "  - *Einsatzgebiet*: Riesige Datenmengen und Machine Learning auf Skalenniveau.\n",
    "\n",
    "- **Warum nicht einfach Pandas oder reines Python?**\n",
    "  - *Kleine Daten passen in den Arbeitsspeicher ‚Üí Pandas reicht locker.*\n",
    "  - *Daten gr√∂√üer als RAM ‚Üí Pandas stirbt.*\n",
    "  - *Man braucht echte Parallelisierung ‚Üí Spark.*\n",
    "\n",
    "- **Warum PySpark?**\n",
    "  - Spark ist in Scala und Java geschrieben ‚Äî aber wer will f√ºr normale Datenverarbeitung in Java rumfrickeln?\n",
    "  - **PySpark** erlaubt dir, Spark mit Python zu steuern. Fast alle Vorteile, aber einfacher zu schreiben.\n",
    " \n",
    "- **Wann Scala/Java doch besser ist?**\n",
    "    - Wenn du maximale Performance brauchst (z. B. komplexe UDFs).\n",
    "    - Bei extrem gro√üen Datasets (Python-Overhead kann sp√ºrbar werden).\n",
    "    - F√ºr Spark-Interna-Entwicklung (z. B. eigene Spark-Erweiterungen).\n",
    "\n",
    "**Ehrliche Einsch√§tzung**:  \n",
    "> Nutze Spark nur, wenn du *musst* (Daten zu gro√ü, Performance wird kritisch).  \n",
    "> Ansonsten: **Pandas > PySpark** bei kleinen bis mittleren Daten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b26ccbf-d965-48a4-a67c-fe33ef45105d",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 2. Cluster Mode vs Local Mode\n",
    "\n",
    "Stell dir vor, du hast eine schwere Datenverarbeitung.\n",
    "\n",
    "| Modus | Was passiert | Wann verwenden | Wichtigster Punkt |\n",
    "|:----|:------------|:----------------|:-----------|\n",
    "| **Local Mode** | Spark l√§uft auf deinem Rechner, nutzt mehrere CPU-Kerne. | Entwicklung, Tests, kleine Datenmengen. | *Trainingsmodus.* |\n",
    "| **Cluster Mode** | Spark l√§uft auf mehreren Servern (Nodes). | Produktion, Big Data. | *Echte Skalierung.* |\n",
    "\n",
    "**Skizze:**\n",
    "\n",
    "```plaintext\n",
    "Local Mode:\n",
    "+-----------------+\n",
    "| Laptop          |\n",
    "| [Core1][Core2]  |\n",
    "| [Core3][Core4]  |\n",
    "+-----------------+\n",
    "\n",
    "Cluster Mode:\n",
    "+--------------------+     +-------------------+     +-------------------+\n",
    "| Worker Node 1      |     | Worker Node 2      |     | Worker Node 3      |\n",
    "| [Task1][Task2]     |     | [Task3][Task4]     |     | [Task5][Task6]     |\n",
    "+--------------------+     +-------------------+     +-------------------+\n",
    "          \\                    |                    /\n",
    "           \\                   |                   /\n",
    "                +----------------------------------+\n",
    "                |        Spark Driver Program     |\n",
    "                +----------------------------------+\n",
    "```\n",
    "\n",
    "> **Wichtig**: *Im Cluster Mode steuert ein Programm viele Maschinen.*  \n",
    "> **Local Mode simuliert nur ein Mini-Cluster auf deinem Laptop.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0562472-92d4-47a1-a47d-0c04ec2cbd33",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 3. Spark Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be1d5537-11bc-4025-b7b7-d61f38bb8a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://spark.apache.org/docs/latest/img/cluster-overview.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(url=\"https://spark.apache.org/docs/latest/img/cluster-overview.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ff1737-7bc6-4145-bb15-1406af065e70",
   "metadata": {},
   "source": [
    "### Spark Architektur: Cluster Overview)\r\n",
    "\r\n",
    "Dieses Diagramm zeigt **wie Spark verteilt arbeitet**:\r\n",
    "\r\n",
    "| Komponente | Beschreibung |\r\n",
    "|:---|:---|\r\n",
    "| **Driver Program** | Dein Hauptprogramm. Es steuert alles: Job-Aufteilung, Kommunikation, Fehlerbehandlung. |\r\n",
    "| **SparkContext** | Die zentrale Verbindung vom Driver zu Spark. Du programmierst damit die Aktionen und Transformationen. |\r\n",
    "| **Cluster Manager** | Verwaltet die Ressourcen im Cluster (CPU, RAM). Beispiele: YARN, Kubernetes, Standalone. |\r\n",
    "| **Worker Nodes** | Die Maschinen, die die eigentliche Datenverarbeitung √ºbernehmen. |\r\n",
    "| **Executor** | Ein Prozess auf einem Worker, der Tasks ausf√ºhrt und Daten im Speicher h√§lt. |\r\n",
    "| **Task** | Die kleinste Recheneinheit. Ein Spark-Job wird in viele Tasks aufgeteilt. |\r\n",
    "| **Cache** | Zwischenspeicher (RAM) f√ºr Daten, die mehrfach gebraucht werden, damit sie nicht immer neu berechnet werden m√ºssen. |\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### üî• Wichtig zu verstehen:\r\n",
    "\r\n",
    "- **Driver** ‚Üí Teilt Jobs auf und schickt sie √ºber den **Cluster Manager** an die **Worker Nodes**.\r\n",
    "- Jeder **Worker** hat mindestens einen **Executor**, der wiederum mehrere **Tasks** parallel ausf√ºhrt.\r\n",
    "- **Caches** sparen Zeit, indem sie Daten im RAM statt auf der Platte halten.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### üß† Skeptische Beobachtungen:\r\n",
    "\r\n",
    "- **Single Point of Failure**: Wenn der Driver abst√ºrzt, ist der ganze Job verloren (au√üer du hast High Availability konfiguriert).\r\n",
    "- **Ressourcenverschwendung m√∂glich**: Executors brauchen RAM ‚Äì schlecht abgestimmt ‚Üí viele Out-of-Memory-Fehler.\r\n",
    "- **Netzwerk Bottleneck**: Schweres Shuffling (viel Datentausch zwischen Workern) kann Spark-Jonehmer nicht einfach abschalten.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc35db4-f9e8-4475-b1fd-fa125d321ade",
   "metadata": {},
   "source": [
    "# ‚ú® 4.`SparkSession` als Einstiegspunkt\n",
    "\n",
    "- In Spark 2.0 und neuer ist **`SparkSession`** der *offizielle Einstiegspunkt* f√ºr jede Spark-Anwendung.\n",
    "- Fr√ºher musste man `SparkContext`, `SQLContext`, `HiveContext` usw. separat erstellen ‚Äî heute b√ºndelt `SparkSession` alles in einem Objekt.\n",
    "\n",
    "**Merksatz**:  \n",
    "> **Ohne SparkSession ‚Üí kein Zugriff auf Spark.**\n",
    "\n",
    "**Beispiel:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "621d7b10-96ea-44d6-b5d9-244a72743d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Localspark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ecc8f28-fcb4-48e5-9cb9-4dc489fe6245",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(\"Alice\", \"Engineering\", 65000, 28),\n",
    "    (\"Bob\", \"Marketing\", 58000, 32),\n",
    "    (\"Carol\", \"Engineering\", 72000, 35)], \n",
    "                           [\"name\", \"department\", \"salary\", \"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2882bafc-1d61-4128-accb-c0028513da45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+------+---+\n",
      "| name| department|salary|age|\n",
      "+-----+-----------+------+---+\n",
      "|Alice|Engineering| 65000| 28|\n",
      "|  Bob|  Marketing| 58000| 32|\n",
      "|Carol|Engineering| 72000| 35|\n",
      "+-----+-----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99335c5c-2926-4eef-a2f4-416e53f865a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3949ec0-20d0-420b-91f5-546fd6319156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fd4217-8fad-43d8-a177-89c26c948176",
   "metadata": {},
   "source": [
    "## üÜö 5. DataFrame vs RDD (nur die Basics)\n",
    "\n",
    "| Feature | DataFrame | RDD |\n",
    "|:--------|:----------|:----|\n",
    "| **Definition** | Tabelle mit Spalten und Datentypen (√§hnlich SQL oder Pandas) | Rohes, unstrukturiertes Datenset (verteilte Liste von Objekten) |\n",
    "| **Benutzerfreundlichkeit** | Hoch ‚Äì SQL-√§hnliche Operationen | Niedrig ‚Äì eigene Map/Reduce-Logik schreiben |\n",
    "| **Performance** | Optimiert durch Catalyst & Tungsten Engine | Weniger optimiert (du musst dich selbst um Performance k√ºmmern) |\n",
    "| **Anwendungsfall** | Klassische Datenverarbeitung, Analytics, Machine Learning Pipelines | Low-Level Transformationen, wenn extreme Flexibilit√§t gebraucht wird |\n",
    "\n",
    "**Skeptische Wahrheit**:  \n",
    "> Wer heute noch direkt mit RDDs arbeitet, hat meist ein sehr spezielles Problem ‚Äî oder kein Vertrauen in Spark-Optimierungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "134ca400-0e6c-4a4c-8c67-61e36451d278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('M√ºller', 35, 'Berlin'), ('Schmidt', 28, 'M√ºnchen')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# RDD aus Liste erstellen (verteilte Rohdaten)\n",
    "rdd = sc.parallelize([\n",
    "    (\"M√ºller\", 35, \"Berlin\"),\n",
    "    (\"Schmidt\", 28, \"M√ºnchen\")\n",
    "])\n",
    "\n",
    "data = rdd.collect()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f318be5a-3aef-42e3-b64d-722f2e00fc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------+\n",
      "|   Name|Alter|  Stadt|\n",
      "+-------+-----+-------+\n",
      "| M√ºller|   35| Berlin|\n",
      "|Schmidt|   28|M√ºnchen|\n",
      "+-------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# DataFrame mit Schema erstellen\n",
    "df = spark.createDataFrame(\n",
    "    [(\"M√ºller\", 35, \"Berlin\"), (\"Schmidt\", 28, \"M√ºnchen\")],\n",
    "    [\"Name\", \"Alter\", \"Stadt\"]\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d5a64cc-644c-45ba-adee-c6fe01949b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('M√ºller', 70), ('Schmidt', 56)]\n"
     ]
    }
   ],
   "source": [
    "# Map-Operation (manuelle Transformation)\n",
    "rdd_mapped = rdd.map(lambda x: (x[0], x[1] * 2))  # Verdopple das Alter\n",
    "print(rdd_mapped.collect()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4fcc058-741b-4744-84f8-493ad452bebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "|   Name|Doppeltes_Alter|\n",
      "+-------+---------------+\n",
      "| M√ºller|             70|\n",
      "|Schmidt|             56|\n",
      "+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL-√§hnliche Operation\n",
    "df_transformed = df.select(\"Name\", (df.Alter * 2).alias(\"Doppeltes_Alter\"))\n",
    "df_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d513907b-b574-4bb5-97bc-c91b8278ad2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63, 2)\n",
      "Durchschnittsalter: 31.5\n"
     ]
    }
   ],
   "source": [
    "# Durchschnittsalter in einem Schritt berechnen\n",
    "durchschnittsalter = rdd.map(lambda x: (x[1], 1)) \\\n",
    "                       .reduce(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "\n",
    "print(durchschnittsalter)\n",
    "durchschnittsalter = durchschnittsalter[0] / durchschnittsalter[1]\n",
    "print(\"Durchschnittsalter:\", durchschnittsalter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879380b-bad4-4756-b64e-c7b2c0923782",
   "metadata": {},
   "source": [
    "Was passiert Schritt f√ºr Schritt:\n",
    "\n",
    "    1. Vorbereitung:\n",
    "\n",
    "        - Ihr RDD enth√§lt nach der map-Operation Paare von (Alter, 1)\n",
    "\n",
    "        - Beispiel: [(35, 1), (28, 1)]\n",
    "\n",
    "    2. Erster Reduzierungsschritt:\n",
    "\n",
    "        - a = (35, 1), b = (28, 1)\n",
    "\n",
    "        - Die Lambda-Funktion berechnet:\n",
    "\n",
    "            a[0] + b[0] ‚Üí 35 + 28 ‚Üí 63 (Summe der Alter)\n",
    "\n",
    "            a[1] + b[1] ‚Üí 1 + 1 ‚Üí 2 (Anzahl der Personen)\n",
    "\n",
    "        - Ergebnis: (63, 2)\n",
    "\n",
    "    3. Endergebnis:\n",
    "\n",
    "        - Da nur zwei Elemente im RDD sind, ist dies das finale Ergebnis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0a1312f-65cf-4b0a-a555-ea33e3c56d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|Durchschnittsalter|\n",
      "+------------------+\n",
      "|              31.5|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.agg(F.avg(\"Alter\").alias(\"Durchschnittsalter\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd0d8d7-1239-45f6-b1d0-2318e5693fbf",
   "metadata": {},
   "source": [
    "## üîÑ 6. Lebenszyklus eines DataFrames in Spark\n",
    "\n",
    "**Was passiert intern?**\n",
    "\n",
    "1. **Definition**:  \n",
    "   Du schreibst Transformationen (`select`, `filter`, `join`), aber Spark f√ºhrt *noch nichts* aus.\n",
    "\n",
    "2. **Logischer Plan**:  \n",
    "   Spark erstellt einen logischen Ablaufplan (nur eine Beschreibung, noch keine Ausf√ºhrung).\n",
    "\n",
    "3. **Physikalischer Plan**:  \n",
    "   Spark optimiert den logischen Plan zu einem ausf√ºhrbaren Programm (z.‚ÄØB. Broadcast Joins, Predicate Pushdown).\n",
    "\n",
    "4. **Ausf√ºhrung (Action!)**:  \n",
    "   Erst wenn eine **Action** kommt (`show()`, `collect()`, `write()`) wird der Plan tats√§chlich ausgef√ºhrt ‚Üí *Lazy Execution Prinzip.*\n",
    "\n",
    "**Grafisch:**\n",
    "\n",
    "```plaintext\n",
    "Transformationen -> Logischer Plan -> Optimierter physikalischer Plan -> Task-Ausf√ºhrung\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797f89f0-98d6-49b9-9141-dace2f7e627a",
   "metadata": {},
   "source": [
    "## üìä 7. Daten erkunden und transformieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2810e5f-32bc-4ff8-9385-f0cae680b71d",
   "metadata": {},
   "source": [
    "### Dataframe erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76cfe34b-9476-45bf-8c7a-4d75186d0d32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------+------+\n",
      "|   Name|Alter|  Stadt|Gehalt|\n",
      "+-------+-----+-------+------+\n",
      "| M√ºller|   35| Berlin|  4000|\n",
      "|Schmidt|   28|M√ºnchen|  3200|\n",
      "|Fischer|   42|Hamburg|  5100|\n",
      "+-------+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DeutscheDaten\").getOrCreate()\n",
    "\n",
    "# DataFrame mit deutschen Spaltennamen erstellen\n",
    "data = [\n",
    "    (\"M√ºller\", 35, \"Berlin\", 4000),\n",
    "    (\"Schmidt\", 28, \"M√ºnchen\", 3200),\n",
    "    (\"Fischer\", 42, \"Hamburg\", 5100)\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Alter\", \"Stadt\", \"Gehalt\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a8be6ad-093e-4cb9-a349-9deac6f71d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Alter: integer (nullable = true)\n",
      " |-- Stadt: string (nullable = true)\n",
      " |-- Gehalt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Mit expliziten Datentypen\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Alter\", IntegerType(), True),\n",
    "    StructField(\"Stadt\", StringType(), True),\n",
    "    StructField(\"Gehalt\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_mit_schema = spark.createDataFrame(data, schema)\n",
    "df_mit_schema.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "14d1da24-b9bc-497c-8aa0-70c4a13eb95d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------+------+\n",
      "|   Name|Alter|  Stadt|Gehalt|\n",
      "+-------+-----+-------+------+\n",
      "| M√ºller|   35| Berlin|  4000|\n",
      "|Schmidt|   28|M√ºnchen|  3200|\n",
      "|Fischer|   42|Hamburg|  5100|\n",
      "+-------+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mit_schema.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f4910-58c8-43dc-80a7-e1d438395d88",
   "metadata": {},
   "source": [
    "### √úbung 1: Grundlegende DataFrame-Erstellung\n",
    "\n",
    "Aufgabe: Erstelle ein DataFrame mit Mitarbeiterdaten, das folgende Spalten enth√§lt:\n",
    "\n",
    "    Vorname (z.B. \"Anna\", \"Thomas\")\n",
    "    Abteilung (z.B. \"IT\", \"Vertrieb\")\n",
    "    Eintrittsjahr (z.B. 2015, 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d38b04db-c0fe-4a67-9f10-d815d83b1f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------------+\n",
      "|Vorname|Abteilung|Eintrittsjahr|\n",
      "+-------+---------+-------------+\n",
      "|   Anna|       IT|         2018|\n",
      "| Thomas| Vertrieb|         2015|\n",
      "|  Julia|       HR|         2020|\n",
      "+-------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# L√∂sung\n",
    "mitarbeiter_data = [\n",
    "    (\"Anna\", \"IT\", 2018),\n",
    "    (\"Thomas\", \"Vertrieb\", 2015),\n",
    "    (\"Julia\", \"HR\", 2020)\n",
    "]\n",
    "mitarbeiter_df = spark.createDataFrame(mitarbeiter_data, [\"Vorname\", \"Abteilung\", \"Eintrittsjahr\"])\n",
    "mitarbeiter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f91438-e013-4483-90ab-e886fb10edc4",
   "metadata": {},
   "source": [
    "### CSV Lesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "61d153b7-4ac3-4061-b843-5291d2ea2e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+--------------------+-------------------+------+\n",
      "|   name|age|      city|               email|          job_title|salary|\n",
      "+-------+---+----------+--------------------+-------------------+------+\n",
      "|   Anna| 28|    Berlin|anna.mueller@exam...|       Data Analyst| 52000|\n",
      "|    Ben| 35|   Hamburg|ben.schmidt@examp...|  Software Engineer| 74000|\n",
      "|  Clara| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|\n",
      "|  David| 40|   Cologne|david.schneider@e...|    Project Manager| 83000|\n",
      "|    Eva| 31| Stuttgart|eva.huber@example...|     Data Scientist| 68000|\n",
      "|  Felix| 29|    Berlin|felix.wagner@exam...|    DevOps Engineer|  NULL|\n",
      "|   Gina| 45|   Hamburg|gina.fischer@exam...|         HR Manager| 69000|\n",
      "| Hannah| 26| Frankfurt|hannah.koch@examp...|        UI Designer| 54000|\n",
      "|   Igor| 38|    Munich|igor.keller@examp...|      Sales Manager| 75000|\n",
      "|  Julia| 24|D√ºsseldorf|julia.schmitt@exa...|    Content Creator| 41000|\n",
      "|   Karl| 50|    Berlin|karl.bauer@exampl...|                CTO|120000|\n",
      "|   Lina| 33| Stuttgart|lina.maier@exampl...|         Accountant| 58000|\n",
      "|    Max| 41|   Cologne|max.frank@example...|   Network Engineer| 71000|\n",
      "|   Nina| 30|   Hamburg|nina.lehmann@exam...|      Product Owner|  NULL|\n",
      "| Oliver| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|\n",
      "|  Paula| 36|    Munich|paula.hartmann@ex...|   Product Designer| 65000|\n",
      "|Quentin| 43| Frankfurt|quentin.schulz@ex...|         Consultant| 80000|\n",
      "|   Rita| 25|D√ºsseldorf|rita.lang@example...|   Junior Developer| 47000|\n",
      "| Stefan| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|\n",
      "|   Tina| 32|   Cologne|tina.kraus@exampl...|      Data Engineer| 73000|\n",
      "+-------+---+----------+--------------------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Korrekte RAW-URL\n",
    "url = \"https://raw.githubusercontent.com/gadanes/spark_kurs/main/notebooks/data.csv\"\n",
    "\n",
    "# Lade die CSV-Datei herunter\n",
    "response = requests.get(url)\n",
    "with open(\"/tmp/data.csv\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Lese die CSV-Datei mit Spark ein\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"/tmp/data.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817713d1-ed22-4581-862f-11242502f14d",
   "metadata": {},
   "source": [
    "### Spalten ausw√§hlen (`select`)\n",
    "\n",
    "Mit `.select()` kannst du gezielt bestimmte Spalten aus dem DataFrame ausw√§hlen.\n",
    "\n",
    "**Merke:**  \n",
    "> `.select()` **ver√§ndert** das urspr√ºngliche DataFrame **nicht**.  \n",
    "> Es erzeugt eine **neue** Version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af63e456-558b-4fc4-a1a0-2770f586e7b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------------+\n",
      "|   name|      city|          job_title|\n",
      "+-------+----------+-------------------+\n",
      "|   Anna|    Berlin|       Data Analyst|\n",
      "|    Ben|   Hamburg|  Software Engineer|\n",
      "|  Clara|    Munich|Marketing Assistant|\n",
      "|  David|   Cologne|    Project Manager|\n",
      "|    Eva| Stuttgart|     Data Scientist|\n",
      "|  Felix|    Berlin|    DevOps Engineer|\n",
      "|   Gina|   Hamburg|         HR Manager|\n",
      "| Hannah| Frankfurt|        UI Designer|\n",
      "|   Igor|    Munich|      Sales Manager|\n",
      "|  Julia|D√ºsseldorf|    Content Creator|\n",
      "|   Karl|    Berlin|                CTO|\n",
      "|   Lina| Stuttgart|         Accountant|\n",
      "|    Max|   Cologne|   Network Engineer|\n",
      "|   Nina|   Hamburg|      Product Owner|\n",
      "| Oliver|    Berlin|  Backend Developer|\n",
      "|  Paula|    Munich|   Product Designer|\n",
      "|Quentin| Frankfurt|         Consultant|\n",
      "|   Rita|D√ºsseldorf|   Junior Developer|\n",
      "| Stefan| Stuttgart|         IT Support|\n",
      "|   Tina|   Cologne|      Data Engineer|\n",
      "+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\", \"city\", \"job_title\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccc9cd2-c41d-4636-a9ec-78d128c18642",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"name\", \"city\", \"job_title\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf1b4e3-ba14-4f38-95e3-8a855a1da02b",
   "metadata": {},
   "source": [
    "### Zeilen filtern (`filter`, `where`)\n",
    "\n",
    "Mit `.filter()` oder `.where()` kannst du Zeilen nach Bedingungen ausw√§hlen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d5a474d-0d16-4a79-8f59-1e924e8223f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+--------------------+-----------------+------+\n",
      "|   name|age|     city|               email|        job_title|salary|\n",
      "+-------+---+---------+--------------------+-----------------+------+\n",
      "|    Ben| 35|  Hamburg|ben.schmidt@examp...|Software Engineer| 74000|\n",
      "|  David| 40|  Cologne|david.schneider@e...|  Project Manager| 83000|\n",
      "|    Eva| 31|Stuttgart|eva.huber@example...|   Data Scientist| 68000|\n",
      "|   Gina| 45|  Hamburg|gina.fischer@exam...|       HR Manager| 69000|\n",
      "|   Igor| 38|   Munich|igor.keller@examp...|    Sales Manager| 75000|\n",
      "|   Karl| 50|   Berlin|karl.bauer@exampl...|              CTO|120000|\n",
      "|   Lina| 33|Stuttgart|lina.maier@exampl...|       Accountant| 58000|\n",
      "|    Max| 41|  Cologne|max.frank@example...| Network Engineer| 71000|\n",
      "|  Paula| 36|   Munich|paula.hartmann@ex...| Product Designer| 65000|\n",
      "|Quentin| 43|Frankfurt|quentin.schulz@ex...|       Consultant| 80000|\n",
      "| Stefan| 39|Stuttgart|stefan.becker@exa...|       IT Support| 56000|\n",
      "|   Tina| 32|  Cologne|tina.kraus@exampl...|    Data Engineer| 73000|\n",
      "+-------+---+---------+--------------------+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.age > 30).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38213a20-b01d-4c6b-ba69-0f6f37cebab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+--------------------+---------------+------+\n",
      "| name|age|   city|               email|      job_title|salary|\n",
      "+-----+---+-------+--------------------+---------------+------+\n",
      "|David| 40|Cologne|david.schneider@e...|Project Manager| 83000|\n",
      "| Karl| 50| Berlin|karl.bauer@exampl...|            CTO|120000|\n",
      "+-----+---+-------+--------------------+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.salary > 80000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d50acc1-d00d-453f-b7ae-90a0ca015ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+--------------------+-----------------+------+\n",
      "|   name|age|     city|               email|        job_title|salary|\n",
      "+-------+---+---------+--------------------+-----------------+------+\n",
      "|    Ben| 35|  Hamburg|ben.schmidt@examp...|Software Engineer| 74000|\n",
      "|  David| 40|  Cologne|david.schneider@e...|  Project Manager| 83000|\n",
      "|    Eva| 31|Stuttgart|eva.huber@example...|   Data Scientist| 68000|\n",
      "|   Gina| 45|  Hamburg|gina.fischer@exam...|       HR Manager| 69000|\n",
      "|   Igor| 38|   Munich|igor.keller@examp...|    Sales Manager| 75000|\n",
      "|   Karl| 50|   Berlin|karl.bauer@exampl...|              CTO|120000|\n",
      "|   Lina| 33|Stuttgart|lina.maier@exampl...|       Accountant| 58000|\n",
      "|    Max| 41|  Cologne|max.frank@example...| Network Engineer| 71000|\n",
      "|  Paula| 36|   Munich|paula.hartmann@ex...| Product Designer| 65000|\n",
      "|Quentin| 43|Frankfurt|quentin.schulz@ex...|       Consultant| 80000|\n",
      "| Stefan| 39|Stuttgart|stefan.becker@exa...|       IT Support| 56000|\n",
      "|   Tina| 32|  Cologne|tina.kraus@exampl...|    Data Engineer| 73000|\n",
      "+-------+---+---------+--------------------+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.age > 30).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09904709-5ee5-4fe2-aefd-5ec1a7349d02",
   "metadata": {},
   "source": [
    "**Hinweis:**  \n",
    "> **`filter`** und **`where`** sind **identisch** ‚Äì es ist reine Geschmackssache."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0259e2-e095-461e-ad9a-a8e85242e731",
   "metadata": {},
   "source": [
    "### Neue Spalten erstellen (`withColumn`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c7e63c4-d344-473b-86d2-2f63c34be827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+--------------------+-------------------+------+----------+\n",
      "|   name|age|      city|               email|          job_title|salary|age_plus_5|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+\n",
      "|   Anna| 28|    Berlin|anna.mueller@exam...|       Data Analyst| 52000|      33.0|\n",
      "|    Ben| 35|   Hamburg|ben.schmidt@examp...|  Software Engineer| 74000|      40.0|\n",
      "|  Clara| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|      27.0|\n",
      "|  David| 40|   Cologne|david.schneider@e...|    Project Manager| 83000|      45.0|\n",
      "|    Eva| 31| Stuttgart|eva.huber@example...|     Data Scientist| 68000|      36.0|\n",
      "|  Felix| 29|    Berlin|felix.wagner@exam...|    DevOps Engineer|  NULL|      34.0|\n",
      "|   Gina| 45|   Hamburg|gina.fischer@exam...|         HR Manager| 69000|      50.0|\n",
      "| Hannah| 26| Frankfurt|hannah.koch@examp...|        UI Designer| 54000|      31.0|\n",
      "|   Igor| 38|    Munich|igor.keller@examp...|      Sales Manager| 75000|      43.0|\n",
      "|  Julia| 24|D√ºsseldorf|julia.schmitt@exa...|    Content Creator| 41000|      29.0|\n",
      "|   Karl| 50|    Berlin|karl.bauer@exampl...|                CTO|120000|      55.0|\n",
      "|   Lina| 33| Stuttgart|lina.maier@exampl...|         Accountant| 58000|      38.0|\n",
      "|    Max| 41|   Cologne|max.frank@example...|   Network Engineer| 71000|      46.0|\n",
      "|   Nina| 30|   Hamburg|nina.lehmann@exam...|      Product Owner|  NULL|      35.0|\n",
      "| Oliver| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|      32.0|\n",
      "|  Paula| 36|    Munich|paula.hartmann@ex...|   Product Designer| 65000|      41.0|\n",
      "|Quentin| 43| Frankfurt|quentin.schulz@ex...|         Consultant| 80000|      48.0|\n",
      "|   Rita| 25|D√ºsseldorf|rita.lang@example...|   Junior Developer| 47000|      30.0|\n",
      "| Stefan| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|      44.0|\n",
      "|   Tina| 32|   Cologne|tina.kraus@exampl...|      Data Engineer| 73000|      37.0|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"age_plus_5\", col(\"age\") + 5)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0529f6cd-049d-49ac-b7c6-5cfbcd3bc5a5",
   "metadata": {},
   "source": [
    "**Wichtig:**  \n",
    "> Jede `.withColumn()`-Operation erstellt **intern ein neues DataFrame** ‚Äî Spark ver√§ndert nie das Originalobjekt direkt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d5ce0-73c3-41f4-894f-649d02c3ccd1",
   "metadata": {},
   "source": [
    "### Bedingte Logik (`when`, `otherwise`)\n",
    "\n",
    "Mit `when` und `otherwise` kannst du **Bedingungen** einbauen, √§hnlich wie `if-else`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81e150e6-f8cf-4d1f-a02f-3320d4626eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "|   name|age|      city|               email|          job_title|salary|age_plus_5|income_category|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "|   Anna| 28|    Berlin|anna.mueller@exam...|       Data Analyst| 52000|      33.0|Normalverdiener|\n",
      "|    Ben| 35|   Hamburg|ben.schmidt@examp...|  Software Engineer| 74000|      40.0|Normalverdiener|\n",
      "|  Clara| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|      27.0|Geringverdiener|\n",
      "|  David| 40|   Cologne|david.schneider@e...|    Project Manager| 83000|      45.0|  Hochverdiener|\n",
      "|    Eva| 31| Stuttgart|eva.huber@example...|     Data Scientist| 68000|      36.0|Normalverdiener|\n",
      "|  Felix| 29|    Berlin|felix.wagner@exam...|    DevOps Engineer|  NULL|      34.0|Geringverdiener|\n",
      "|   Gina| 45|   Hamburg|gina.fischer@exam...|         HR Manager| 69000|      50.0|Normalverdiener|\n",
      "| Hannah| 26| Frankfurt|hannah.koch@examp...|        UI Designer| 54000|      31.0|Normalverdiener|\n",
      "|   Igor| 38|    Munich|igor.keller@examp...|      Sales Manager| 75000|      43.0|Normalverdiener|\n",
      "|  Julia| 24|D√ºsseldorf|julia.schmitt@exa...|    Content Creator| 41000|      29.0|Geringverdiener|\n",
      "|   Karl| 50|    Berlin|karl.bauer@exampl...|                CTO|120000|      55.0|  Hochverdiener|\n",
      "|   Lina| 33| Stuttgart|lina.maier@exampl...|         Accountant| 58000|      38.0|Normalverdiener|\n",
      "|    Max| 41|   Cologne|max.frank@example...|   Network Engineer| 71000|      46.0|Normalverdiener|\n",
      "|   Nina| 30|   Hamburg|nina.lehmann@exam...|      Product Owner|  NULL|      35.0|Geringverdiener|\n",
      "| Oliver| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|      32.0|Normalverdiener|\n",
      "|  Paula| 36|    Munich|paula.hartmann@ex...|   Product Designer| 65000|      41.0|Normalverdiener|\n",
      "|Quentin| 43| Frankfurt|quentin.schulz@ex...|         Consultant| 80000|      48.0|  Hochverdiener|\n",
      "|   Rita| 25|D√ºsseldorf|rita.lang@example...|   Junior Developer| 47000|      30.0|Normalverdiener|\n",
      "| Stefan| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|      44.0|Normalverdiener|\n",
      "|   Tina| 32|   Cologne|tina.kraus@exampl...|      Data Engineer| 73000|      37.0|Normalverdiener|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"income_category\",\n",
    "    when(col(\"salary\") >= 80000, \"Hochverdiener\")\n",
    "    .when((col(\"salary\") >= 43000) & (col(\"salary\") < 80000), \"Normalverdiener\")\n",
    "    .otherwise(\"Geringverdiener\")\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af09c43-3bca-46ca-b6bd-bdfa3903bffc",
   "metadata": {},
   "source": [
    "**Merke:**  \n",
    "> Viele verschachtelte `when`-Bedingungen k√∂nnen un√ºbersichtlich werden ‚Üí sauber strukturieren!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2954d6-368d-436f-bd07-8a7530190d8f",
   "metadata": {},
   "source": [
    "### Umgang mit fehlenden Werten (`fillna`, `dropna`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58cd589f-cb93-4841-b3d2-25994c39567b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "|   name|age|      city|               email|          job_title|salary|age_plus_5|income_category|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "|   Anna| 28|    Berlin|anna.mueller@exam...|       Data Analyst| 52000|      33.0|Normalverdiener|\n",
      "|    Ben| 35|   Hamburg|ben.schmidt@examp...|  Software Engineer| 74000|      40.0|Normalverdiener|\n",
      "|  Clara| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|      27.0|Geringverdiener|\n",
      "|  David| 40|   Cologne|david.schneider@e...|    Project Manager| 83000|      45.0|  Hochverdiener|\n",
      "|    Eva| 31| Stuttgart|eva.huber@example...|     Data Scientist| 68000|      36.0|Normalverdiener|\n",
      "|  Felix| 29|    Berlin|felix.wagner@exam...|    DevOps Engineer| 50000|      34.0|Geringverdiener|\n",
      "|   Gina| 45|   Hamburg|gina.fischer@exam...|         HR Manager| 69000|      50.0|Normalverdiener|\n",
      "| Hannah| 26| Frankfurt|hannah.koch@examp...|        UI Designer| 54000|      31.0|Normalverdiener|\n",
      "|   Igor| 38|    Munich|igor.keller@examp...|      Sales Manager| 75000|      43.0|Normalverdiener|\n",
      "|  Julia| 24|D√ºsseldorf|julia.schmitt@exa...|    Content Creator| 41000|      29.0|Geringverdiener|\n",
      "|   Karl| 50|    Berlin|karl.bauer@exampl...|                CTO|120000|      55.0|  Hochverdiener|\n",
      "|   Lina| 33| Stuttgart|lina.maier@exampl...|         Accountant| 58000|      38.0|Normalverdiener|\n",
      "|    Max| 41|   Cologne|max.frank@example...|   Network Engineer| 71000|      46.0|Normalverdiener|\n",
      "|   Nina| 30|   Hamburg|nina.lehmann@exam...|      Product Owner| 50000|      35.0|Geringverdiener|\n",
      "| Oliver| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|      32.0|Normalverdiener|\n",
      "|  Paula| 36|    Munich|paula.hartmann@ex...|   Product Designer| 65000|      41.0|Normalverdiener|\n",
      "|Quentin| 43| Frankfurt|quentin.schulz@ex...|         Consultant| 80000|      48.0|  Hochverdiener|\n",
      "|   Rita| 25|D√ºsseldorf|rita.lang@example...|   Junior Developer| 47000|      30.0|Geringverdiener|\n",
      "| Stefan| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|      44.0|Normalverdiener|\n",
      "|   Tina| 32|   Cologne|tina.kraus@exampl...|      Data Engineer| 73000|      37.0|Normalverdiener|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fehlende Werte f√ºllen (`fillna`):\n",
    "# Ersetzt fehlende Geh√§lter durch 50000\n",
    "df_filled = df.fillna({\"salary\": 50000})\n",
    "df_filled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b47df62-00cc-4dac-9afc-d5da14cf63d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "|   name|age|      city|               email|          job_title|salary|age_plus_5|income_category|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "|   Anna| 28|    Berlin|anna.mueller@exam...|       Data Analyst| 52000|      33.0|Normalverdiener|\n",
      "|    Ben| 35|   Hamburg|ben.schmidt@examp...|  Software Engineer| 74000|      40.0|Normalverdiener|\n",
      "|  Clara| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|      27.0|Geringverdiener|\n",
      "|  David| 40|   Cologne|david.schneider@e...|    Project Manager| 83000|      45.0|  Hochverdiener|\n",
      "|    Eva| 31| Stuttgart|eva.huber@example...|     Data Scientist| 68000|      36.0|Normalverdiener|\n",
      "|   Gina| 45|   Hamburg|gina.fischer@exam...|         HR Manager| 69000|      50.0|Normalverdiener|\n",
      "| Hannah| 26| Frankfurt|hannah.koch@examp...|        UI Designer| 54000|      31.0|Normalverdiener|\n",
      "|   Igor| 38|    Munich|igor.keller@examp...|      Sales Manager| 75000|      43.0|Normalverdiener|\n",
      "|  Julia| 24|D√ºsseldorf|julia.schmitt@exa...|    Content Creator| 41000|      29.0|Geringverdiener|\n",
      "|   Karl| 50|    Berlin|karl.bauer@exampl...|                CTO|120000|      55.0|  Hochverdiener|\n",
      "|   Lina| 33| Stuttgart|lina.maier@exampl...|         Accountant| 58000|      38.0|Normalverdiener|\n",
      "|    Max| 41|   Cologne|max.frank@example...|   Network Engineer| 71000|      46.0|Normalverdiener|\n",
      "| Oliver| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|      32.0|Normalverdiener|\n",
      "|  Paula| 36|    Munich|paula.hartmann@ex...|   Product Designer| 65000|      41.0|Normalverdiener|\n",
      "|Quentin| 43| Frankfurt|quentin.schulz@ex...|         Consultant| 80000|      48.0|  Hochverdiener|\n",
      "|   Rita| 25|D√ºsseldorf|rita.lang@example...|   Junior Developer| 47000|      30.0|Geringverdiener|\n",
      "| Stefan| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|      44.0|Normalverdiener|\n",
      "|   Tina| 32|   Cologne|tina.kraus@exampl...|      Data Engineer| 73000|      37.0|Normalverdiener|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zeilen mit fehlenden Werten l√∂schen (`dropna`):\n",
    "# Entfernt alle Zeilen, die mindestens einen `null`-Wert enthalten.\n",
    "df_clean = df.dropna()\n",
    "df_clean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e5eaa6-75f7-4694-8a6a-340bf1cd39ba",
   "metadata": {},
   "source": [
    "### Gruppieren und Aggregieren (`groupBy` + `agg`)\n",
    "\n",
    "Mit `.groupBy()` kannst du dein DataFrame nach einer oder mehreren Spalten **gruppieren**.  \n",
    "Mit `.agg()` kannst du dann **Aggregationfunktionen** auf jede Gruppe anwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c443ed8a-8e61-4b41-b701-2d70fb240264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      city|       avg(salary)|\n",
      "+----------+------------------+\n",
      "| Frankfurt|           67000.0|\n",
      "|    Berlin| 79666.66666666667|\n",
      "|D√ºsseldorf|           44000.0|\n",
      "|   Hamburg|           71500.0|\n",
      "| Stuttgart|60666.666666666664|\n",
      "|   Cologne| 75666.66666666667|\n",
      "|    Munich|60666.666666666664|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "#Durchschnitt berechnen\n",
    "df.groupBy(\"city\").agg(F.avg(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "934f27be-0407-4e6c-b991-d142d6f36b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      city|count|\n",
      "+----------+-----+\n",
      "| Frankfurt|    2|\n",
      "|    Berlin|    4|\n",
      "|D√ºsseldorf|    2|\n",
      "|   Hamburg|    3|\n",
      "| Stuttgart|    3|\n",
      "|   Cologne|    3|\n",
      "|    Munich|    3|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Z√§hlt die Anzahl der Eintr√§ge\n",
    "df.groupBy(\"city\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "496db719-ab31-4d74-803e-df98241edeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|      city|sum(salary)|\n",
      "+----------+-----------+\n",
      "| Frankfurt|   134000.0|\n",
      "|    Berlin|   239000.0|\n",
      "|D√ºsseldorf|    88000.0|\n",
      "|   Hamburg|   143000.0|\n",
      "| Stuttgart|   182000.0|\n",
      "|   Cologne|   227000.0|\n",
      "|    Munich|   182000.0|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Summiert Werte einer Spalte\n",
    "df.groupBy(\"city\").agg(F.sum(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434926aa-1dc1-486d-8e0b-bfd06df133aa",
   "metadata": {},
   "source": [
    "**Wichtig zu wissen:**  \n",
    "> `.groupBy()` alleine macht noch nichts. Erst `.agg()`, `.count()`, `.sum()` oder `.avg()` l√∂sen die echte Berechnung aus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f41c4d-506e-47fc-b1c9-f3246af5c8f9",
   "metadata": {},
   "source": [
    "### Sortieren und Reihenfolge √§ndern (`orderBy`, `sort`)\n",
    "\n",
    "Mit `.orderBy()` oder `.sort()` kannst du dein DataFrame sortieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a905c80-ee74-4d72-90a3-8861ec215020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "|   name|age|      city|               email|          job_title|salary|age_plus_5|income_category|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "|  David| 40|   Cologne|david.schneider@e...|    Project Manager| 83000|      45.0|  Hochverdiener|\n",
      "|Quentin| 43| Frankfurt|quentin.schulz@ex...|         Consultant| 80000|      48.0|  Hochverdiener|\n",
      "|   Igor| 38|    Munich|igor.keller@examp...|      Sales Manager| 75000|      43.0|Normalverdiener|\n",
      "|    Ben| 35|   Hamburg|ben.schmidt@examp...|  Software Engineer| 74000|      40.0|Normalverdiener|\n",
      "|   Tina| 32|   Cologne|tina.kraus@exampl...|      Data Engineer| 73000|      37.0|Normalverdiener|\n",
      "|    Max| 41|   Cologne|max.frank@example...|   Network Engineer| 71000|      46.0|Normalverdiener|\n",
      "|   Gina| 45|   Hamburg|gina.fischer@exam...|         HR Manager| 69000|      50.0|Normalverdiener|\n",
      "|    Eva| 31| Stuttgart|eva.huber@example...|     Data Scientist| 68000|      36.0|Normalverdiener|\n",
      "| Oliver| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|      32.0|Normalverdiener|\n",
      "|  Paula| 36|    Munich|paula.hartmann@ex...|   Product Designer| 65000|      41.0|Normalverdiener|\n",
      "|   Lina| 33| Stuttgart|lina.maier@exampl...|         Accountant| 58000|      38.0|Normalverdiener|\n",
      "| Stefan| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|      44.0|Normalverdiener|\n",
      "| Hannah| 26| Frankfurt|hannah.koch@examp...|        UI Designer| 54000|      31.0|Normalverdiener|\n",
      "|   Anna| 28|    Berlin|anna.mueller@exam...|       Data Analyst| 52000|      33.0|Normalverdiener|\n",
      "|   Rita| 25|D√ºsseldorf|rita.lang@example...|   Junior Developer| 47000|      30.0|Normalverdiener|\n",
      "|  Clara| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|      27.0|Geringverdiener|\n",
      "|  Julia| 24|D√ºsseldorf|julia.schmitt@exa...|    Content Creator| 41000|      29.0|Geringverdiener|\n",
      "|   Karl| 50|    Berlin|karl.bauer@exampl...|                CTO|120000|      55.0|  Hochverdiener|\n",
      "|  Felix| 29|    Berlin|felix.wagner@exam...|    DevOps Engineer|  NULL|      34.0|Geringverdiener|\n",
      "|   Nina| 30|   Hamburg|nina.lehmann@exam...|      Product Owner|  NULL|      35.0|Geringverdiener|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Nach Gehalt absteigend sortieren\n",
    "df.orderBy(F.desc(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25eb8450-e829-4e4e-b3fa-abf6e34b0f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "|   name|age|      city|               email|          job_title|salary|age_plus_5|income_category|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "|  Clara| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|      27.0|Geringverdiener|\n",
      "|  Julia| 24|D√ºsseldorf|julia.schmitt@exa...|    Content Creator| 41000|      29.0|Geringverdiener|\n",
      "|   Rita| 25|D√ºsseldorf|rita.lang@example...|   Junior Developer| 47000|      30.0|Normalverdiener|\n",
      "| Hannah| 26| Frankfurt|hannah.koch@examp...|        UI Designer| 54000|      31.0|Normalverdiener|\n",
      "| Oliver| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|      32.0|Normalverdiener|\n",
      "|   Anna| 28|    Berlin|anna.mueller@exam...|       Data Analyst| 52000|      33.0|Normalverdiener|\n",
      "|  Felix| 29|    Berlin|felix.wagner@exam...|    DevOps Engineer|  NULL|      34.0|Geringverdiener|\n",
      "|   Nina| 30|   Hamburg|nina.lehmann@exam...|      Product Owner|  NULL|      35.0|Geringverdiener|\n",
      "|    Eva| 31| Stuttgart|eva.huber@example...|     Data Scientist| 68000|      36.0|Normalverdiener|\n",
      "|   Tina| 32|   Cologne|tina.kraus@exampl...|      Data Engineer| 73000|      37.0|Normalverdiener|\n",
      "|   Lina| 33| Stuttgart|lina.maier@exampl...|         Accountant| 58000|      38.0|Normalverdiener|\n",
      "|    Ben| 35|   Hamburg|ben.schmidt@examp...|  Software Engineer| 74000|      40.0|Normalverdiener|\n",
      "|  Paula| 36|    Munich|paula.hartmann@ex...|   Product Designer| 65000|      41.0|Normalverdiener|\n",
      "|   Igor| 38|    Munich|igor.keller@examp...|      Sales Manager| 75000|      43.0|Normalverdiener|\n",
      "| Stefan| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|      44.0|Normalverdiener|\n",
      "|  David| 40|   Cologne|david.schneider@e...|    Project Manager| 83000|      45.0|  Hochverdiener|\n",
      "|    Max| 41|   Cologne|max.frank@example...|   Network Engineer| 71000|      46.0|Normalverdiener|\n",
      "|Quentin| 43| Frankfurt|quentin.schulz@ex...|         Consultant| 80000|      48.0|  Hochverdiener|\n",
      "|   Gina| 45|   Hamburg|gina.fischer@exam...|         HR Manager| 69000|      50.0|Normalverdiener|\n",
      "|   Karl| 50|    Berlin|karl.bauer@exampl...|                CTO|120000|      55.0|  Hochverdiener|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Oder aufsteigend (Standard)\n",
    "df.orderBy(\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c46d30d-767e-4114-8e25-3f576ee60894",
   "metadata": {},
   "outputs": [],
   "source": [
    "## üîÉ Sortieren und Reihenfolge √§ndern (`orderBy`, `sort`)\n",
    "\n",
    "Mit `.orderBy()` oder `.sort()` kannst du dein DataFrame sortieren.\n",
    "\n",
    "**Beispiel: Nach Gehalt absteigend sortieren:**\n",
    "\n",
    "```python\n",
    "df.orderBy(F.desc(\"salary\")).show()\n",
    "```\n",
    "\n",
    "**Oder aufsteigend (Standard):**\n",
    "\n",
    "```python\n",
    "df.orderBy(\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23c4293-1fc5-4d04-9474-60b5d87686c3",
   "metadata": {},
   "source": [
    "**Hinweis:**  \n",
    "> `orderBy` und `sort` sind **gleichwertig**.  \n",
    "> Bei riesigen DataFrames kann Sortieren teuer werden ‚Üí vorsichtig einsetzen!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9529da-148a-47b3-ab8b-a207b585e4e4",
   "metadata": {},
   "source": [
    "### Window Functions\n",
    "\n",
    "Window Functions erlauben dir, √ºber eine **Teilmenge** deiner Daten zu arbeiten, **ohne** sie vollst√§ndig zu aggregieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7344af77-fded-48b8-a1a0-56a1222e94f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+------------+\n",
      "|   name|age|      city|               email|          job_title|salary|age_plus_5|income_category|rank_in_city|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+------------+\n",
      "| Oliver| 27|    Berlin|oliver.weber@exam...|  Backend Developer| 67000|      32.0|Normalverdiener|           1|\n",
      "|   Anna| 28|    Berlin|anna.mueller@exam...|       Data Analyst| 52000|      33.0|Normalverdiener|           2|\n",
      "|   Karl| 50|    Berlin|karl.bauer@exampl...|                CTO|120000|      55.0|  Hochverdiener|           3|\n",
      "|  Felix| 29|    Berlin|felix.wagner@exam...|    DevOps Engineer|  NULL|      34.0|Geringverdiener|           4|\n",
      "|  David| 40|   Cologne|david.schneider@e...|    Project Manager| 83000|      45.0|  Hochverdiener|           1|\n",
      "|   Tina| 32|   Cologne|tina.kraus@exampl...|      Data Engineer| 73000|      37.0|Normalverdiener|           2|\n",
      "|    Max| 41|   Cologne|max.frank@example...|   Network Engineer| 71000|      46.0|Normalverdiener|           3|\n",
      "|   Rita| 25|D√ºsseldorf|rita.lang@example...|   Junior Developer| 47000|      30.0|Normalverdiener|           1|\n",
      "|  Julia| 24|D√ºsseldorf|julia.schmitt@exa...|    Content Creator| 41000|      29.0|Geringverdiener|           2|\n",
      "|Quentin| 43| Frankfurt|quentin.schulz@ex...|         Consultant| 80000|      48.0|  Hochverdiener|           1|\n",
      "| Hannah| 26| Frankfurt|hannah.koch@examp...|        UI Designer| 54000|      31.0|Normalverdiener|           2|\n",
      "|    Ben| 35|   Hamburg|ben.schmidt@examp...|  Software Engineer| 74000|      40.0|Normalverdiener|           1|\n",
      "|   Gina| 45|   Hamburg|gina.fischer@exam...|         HR Manager| 69000|      50.0|Normalverdiener|           2|\n",
      "|   Nina| 30|   Hamburg|nina.lehmann@exam...|      Product Owner|  NULL|      35.0|Geringverdiener|           3|\n",
      "|   Igor| 38|    Munich|igor.keller@examp...|      Sales Manager| 75000|      43.0|Normalverdiener|           1|\n",
      "|  Paula| 36|    Munich|paula.hartmann@ex...|   Product Designer| 65000|      41.0|Normalverdiener|           2|\n",
      "|  Clara| 22|    Munich|clara.klein@examp...|Marketing Assistant| 42000|      27.0|Geringverdiener|           3|\n",
      "|    Eva| 31| Stuttgart|eva.huber@example...|     Data Scientist| 68000|      36.0|Normalverdiener|           1|\n",
      "|   Lina| 33| Stuttgart|lina.maier@exampl...|         Accountant| 58000|      38.0|Normalverdiener|           2|\n",
      "| Stefan| 39| Stuttgart|stefan.becker@exa...|         IT Support| 56000|      44.0|Normalverdiener|           3|\n",
      "+-------+---+----------+--------------------+-------------------+------+----------+---------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/29 08:00:42 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@baf44cee0c18:38801\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "25/04/29 08:00:51 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@baf44cee0c18:38801\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"city\").orderBy(F.desc(\"salary\"))\n",
    "\n",
    "df = df.withColumn(\"rank_in_city\", F.rank().over(window_spec))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4faa86-2ba9-4336-ba12-0ece074eb1ee",
   "metadata": {},
   "source": [
    "### Lesen von Azure Datenbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1808200-74c9-4ef7-8acf-8ce882caffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Localspark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\",\n",
    "        \"com.microsoft.sqlserver:mssql-jdbc:12.6.1.jre11\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0348963-f55f-4a87-8fb8-737c634b3ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
